<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>AWS Solution Architect Practice Test – Page 10</title>
<link rel="stylesheet" href="style.css">
</head>

<body>
<div class="container">

<!-- ================= Navigation Top ================= -->
<div style="text-align:center; margin: 20px 0;">
  <a href="page9.html" style="
      display:inline-block;
      padding: 12px 28px;
      background:#6b7280;
      color:#fff;
      font-size:15px;
      font-weight:600;
      border-radius:8px;
      text-decoration:none;
      margin-right:10px;
  ">
    ← Previous Page
  </a>
  <a href="page11.html" style="
      display:inline-block;
      padding: 12px 28px;
      background:#6b7280;
      color:#fff;
      font-size:15px;
      font-weight:600;
      border-radius:8px;
      text-decoration:none;
  ">
    Next Page →
  </a>
</div>

<h1>AWS Solution Architect – Practice Test (Page 10)</h1>

<!-- ================= Q1 ================= -->
<div class="question">
<pre>
91) A company consists of two separate business units. Each business unit has its own AWS account within a single organization in AWS Organizations. 
The business units regularly share sensitive documents with each other. To facilitate sharing, the company created an Amazon S3 bucket in each account and 
configured two-way replication between the S3 buckets. The S3 buckets have millions of objects.

Recently, a security audit identified that neither S3 bucket has encryption at rest enabled. Company policy requires that all documents must be stored with 
encryption at rest. The company wants to implement server-side encryption with Amazon S3 managed encryption keys (SSE-S3).

What is the MOST operationally efficient solution that meets these requirements?
</pre>
<div class="options">
<label>
<input type="radio" name="q1">
A. Turn on SSE-S3 on both S3 buckets. Use S3 Batch Operations to copy and encrypt the objects in the same location.
</label>

<label>
<input type="radio" name="q1">
B. Create an AWS Key Management Service (AWS KMS) key in each account. Turn on server-side encryption with AWS KMS keys (SSE-KMS) on each S3 bucket by using the corresponding KMS key in that AWS account. Encrypt the existing objects by using an S3 copy command in the AWS CLI.
</label>

<label>
<input type="radio" name="q1">
C. Turn on SSE-S3 on both S3 buckets. Encrypt the existing objects by using an S3 copy command in the AWS CLI.
</label>

<label>
<input type="radio" name="q1">
D. Create an AWS Key Management Service (AWS KMS) key in each account. Turn on server-side encryption with AWS KMS keys (SSE-KMS) on each S3 bucket by using the corresponding KMS key in that AWS account. Use S3 Batch Operations to copy the objects into the same location.
</label>
</div>

<button onclick="checkAnswer(this,[0])">Check Answer</button>
<button onclick="showAnswer(this,[0])">Show Answer</button>

<div class="explanation">
<button class="close-explanation" onclick="closeExplanation(this)">✕ Close</button>
<b>Correct Answer: A</b><br><br>

This question tests <b>encrypting existing S3 objects at scale with SSE-S3 using S3 Batch Operations</b>.<br><br>

<b>Requirements:</b>
<ul>
<li>Two S3 buckets with millions of objects (no encryption currently)</li>
<li>Two-way replication between buckets</li>
<li>Must enable SSE-S3 encryption at rest</li>
<li>Need MOST operationally efficient solution</li>
</ul><br>

<b>Why Option A (SSE-S3 + S3 Batch Operations) is Correct:</b><br>

<b>S3 Batch Operations Benefits:</b>
<ul>
<li>✓ <b>Built for scale:</b> Designed for millions/billions of objects</li>
<li>✓ <b>Automated:</b> No manual scripting required</li>
<li>✓ <b>Progress tracking:</b> Job status, completion reports</li>
<li>✓ <b>Error handling:</b> Automatic retries, failure reports</li>
<li>✓ <b>SSE-S3 is free:</b> No KMS key costs</li>
<li>✓ <b>Copy-in-place:</b> Objects stay in same location</li>
</ul><br>

<b>How S3 Batch Operations Works:</b>
<pre>
<b>Step 1: Enable SSE-S3 on buckets</b>
- Turn on default encryption (SSE-S3) for both buckets
- New objects automatically encrypted
- Existing objects NOT automatically encrypted

<b>Step 2: Create S3 Batch Operations job</b>
- Operation: Copy (copy objects to themselves)
- Source: S3 inventory or object list
- Destination: Same bucket, same key
- Encryption: SSE-S3

<b>Step 3: AWS processes millions of objects</b>
- Batch Operations copies each object in-place
- Copy operation applies SSE-S3 encryption
- Original unencrypted object replaced
- Replication automatically syncs to other bucket

<b>Result:</b>
✓ All objects encrypted with SSE-S3
✓ Automated, scalable process
✓ Full job tracking and reporting
</pre><br>

<b>Implementation:</b>
<pre>
# 1. Enable default SSE-S3 encryption on both buckets
aws s3api put-bucket-encryption \
  --bucket business-unit-1-bucket \
  --server-side-encryption-configuration '{
    "Rules": [{
      "ApplyServerSideEncryptionByDefault": {
        "SSEAlgorithm": "AES256"
      },
      "BucketKeyEnabled": false
    }]
  }'

aws s3api put-bucket-encryption \
  --bucket business-unit-2-bucket \
  --server-side-encryption-configuration '{
    "Rules": [{
      "ApplyServerSideEncryptionByDefault": {
        "SSEAlgorithm": "AES256"
      },
      "BucketKeyEnabled": false
    }]
  }'

# 2. Create S3 Inventory for the bucket (to list all objects)
aws s3api put-bucket-inventory-configuration \
  --bucket business-unit-1-bucket \
  --id inventory-for-encryption \
  --inventory-configuration '{
    "Id": "inventory-for-encryption",
    "IsEnabled": true,
    "Destination": {
      "S3BucketDestination": {
        "Bucket": "arn:aws:s3:::inventory-bucket",
        "Format": "CSV"
      }
    },
    "Schedule": {
      "Frequency": "Daily"
    },
    "IncludedObjectVersions": "Current"
  }'

# 3. Create IAM role for Batch Operations
{
  "Version": "2012-10-17",
  "Statement": [{
    "Effect": "Allow",
    "Action": [
      "s3:GetObject",
      "s3:PutObject",
      "s3:GetObjectVersion"
    ],
    "Resource": "arn:aws:s3:::business-unit-1-bucket/*"
  }]
}

# 4. Create S3 Batch Operations job
aws s3control create-job \
  --account-id 123456789012 \
  --operation '{
    "S3PutObjectCopy": {
      "TargetResource": "arn:aws:s3:::business-unit-1-bucket",
      "StorageClass": "STANDARD",
      "TargetKeyPrefix": "",
      "MetadataDirective": "COPY",
      "NewObjectMetadata": {
        "SSEAlgorithm": "AES256"
      }
    }
  }' \
  --manifest '{
    "Spec": {
      "Format": "S3InventoryReport_CSV_20230101",
      "Fields": ["Bucket", "Key"]
    },
    "Location": {
      "ObjectArn": "arn:aws:s3:::inventory-bucket/inventory-manifest.json",
      "ETag": "etag-value"
    }
  }' \
  --report '{
    "Enabled": true,
    "Bucket": "arn:aws:s3:::batch-ops-reports",
    "Format": "Report_CSV_20180820",
    "ReportScope": "AllTasks"
  }' \
  --priority 10 \
  --role-arn arn:aws:iam::123456789012:role/BatchOpsRole \
  --region us-east-1

# 5. Monitor job progress
aws s3control describe-job \
  --account-id 123456789012 \
  --job-id job-id-12345

# 6. Repeat for second bucket
</pre><br>

<b>S3 Batch Operations Process:</b>
<pre>
<b>For millions of objects:</b>

Batch Operations Job Created
    ↓
Reads inventory list (all object keys)
    ↓
Creates tasks for each object
    ↓
Processes in parallel (scales automatically)
    ↓
For each object:
  - Read object from source
  - Copy to same location with SSE-S3
  - Replace original
  - Replication syncs encrypted version
    ↓
Generate completion report
    ↓
Success: All objects encrypted!

<b>Performance:</b>
- Can process millions of objects per day
- Automatic parallelization
- No manual scripting or error handling needed
</pre><br>

<b>Why Other Options Are Wrong:</b><br>

<b>Option B - SSE-KMS + CLI Copy:</b>
<ul>
<li>✗ <b>SSE-KMS not required:</b> Question asks for SSE-S3</li>
<li>✗ <b>Additional cost:</b> KMS key usage charges</li>
<li>✗ <b>CLI copy for millions:</b> Manual scripting, error-prone</li>
<li>✗ <b>Cross-account KMS complexity:</b> Need key policies for replication</li>
<li>✗ <b>Less operationally efficient</b></li>
</ul>

KMS cost impact:
<pre>
For 10 million objects:

SSE-S3 (Option A):
- Encryption cost: $0 (free)
- S3 Batch Operations: ~$2.50 (10M objects × $0.25/1000 tasks)
- Total: ~$2.50

SSE-KMS (Option B):
- KMS key: $1/month per key × 2 accounts = $2/month
- KMS requests: 10M requests × $0.03/10,000 = $30
- Ongoing KMS requests for every read: $$$ (expensive!)
- S3 Batch Ops or CLI: $2.50+
- Total: $34.50+ initial, plus ongoing costs

<b>SSE-S3 is free and simpler!</b>
</pre><br>

<b>Option C - SSE-S3 + CLI Copy:</b>
<ul>
<li>✓ SSE-S3 is correct</li>
<li>✗ <b>CLI copy for millions of objects:</b> Not operationally efficient</li>
<li>✗ <b>Manual scripting required:</b> Write loop, handle errors</li>
<li>✗ <b>No built-in retry logic:</b> Must handle failures manually</li>
<li>✗ <b>No progress tracking:</b> Hard to monitor</li>
<li>✗ <b>Takes longer to develop and run</b></li>
</ul>

CLI approach problems:
<pre>
# Manual script needed
aws s3 ls s3://bucket --recursive | while read -r line; do
  key=$(echo $line | awk '{print $4}')
  aws s3 cp "s3://bucket/$key" "s3://bucket/$key" \
    --server-side-encryption AES256 \
    --metadata-directive REPLACE
done

<b>Issues:</b>
- 10 million objects = 10 million CLI commands!
- Takes days/weeks to run
- Script can fail mid-way
- No automatic retry
- Difficult to track progress
- Must handle rate limits
- Error-prone
</pre><br>

<b>Option D - SSE-KMS + S3 Batch Operations:</b>
<ul>
<li>✓ S3 Batch Operations is correct approach</li>
<li>✗ <b>SSE-KMS not required:</b> Question specifically asks for SSE-S3</li>
<li>✗ <b>Additional cost:</b> KMS charges</li>
<li>✗ <b>More complex:</b> KMS key policies for cross-account replication</li>
<li>✗ <b>Overengineered:</b> SSE-S3 meets requirements</li>
</ul><br>

<b>Comparison Table:</b>
<table border="1" cellpadding="5">
<tr><th>Feature</th><th>Option A</th><th>Option B</th><th>Option C</th><th>Option D</th></tr>
<tr><td>Encryption Type</td><td><b>SSE-S3 ✓</b></td><td>SSE-KMS</td><td><b>SSE-S3 ✓</b></td><td>SSE-KMS</td></tr>
<tr><td>Scalability</td><td><b>Batch Ops ✓</b></td><td>CLI (manual)</td><td>CLI (manual)</td><td><b>Batch Ops ✓</b></td></tr>
<tr><td>Operational Effort</td><td><b>Low ✓</b></td><td>High</td><td>Very High</td><td>Medium</td></tr>
<tr><td>Cost</td><td><b>Very Low ✓</b></td><td>High (KMS)</td><td>Low</td><td>High (KMS)</td></tr>
<tr><td>Error Handling</td><td><b>Automatic ✓</b></td><td>Manual</td><td>Manual</td><td><b>Automatic ✓</b></td></tr>
<tr><td>Progress Tracking</td><td><b>Built-in ✓</b></td><td>None</td><td>None</td><td><b>Built-in ✓</b></td></tr>
</table><br>

<b>SSE-S3 vs SSE-KMS:</b>
<pre>
<b>SSE-S3 (Amazon S3 Managed Keys):</b>
- Free encryption
- AWS manages keys
- AES-256 encryption
- Automatic key rotation
- Suitable for most use cases
- No KMS costs

<b>SSE-KMS (AWS KMS Keys):</b>
- Customer-managed keys
- Key usage charges ($0.03/10,000 requests)
- Audit trail in CloudTrail
- Fine-grained access control
- Cross-account complexity
- Use only when required for compliance

<b>Question asks for SSE-S3 → Use SSE-S3!</b>
</pre><br>

<b>S3 Replication with Encryption:</b>
<pre>
<b>How replication works with encryption change:</b>

Bucket 1 (Account A):
- Object encrypted with SSE-S3 via Batch Operations
    ↓
    ↓ Replication rule active
    ↓
Bucket 2 (Account B):
- Replicated object automatically encrypted with SSE-S3
- Destination bucket default encryption applied

<b>Key Point:</b>
When you encrypt objects in one bucket, replication
automatically syncs encrypted versions to the other bucket
(if replication rules are properly configured).

Just need to run Batch Operations on BOTH buckets
to ensure all historical objects are encrypted.
</pre><br>

<b>S3 Batch Operations Job Report:</b>
<pre>
Completion Report shows:
- Total tasks: 10,000,000
- Succeeded: 9,998,500
- Failed: 1,500
- Success rate: 99.985%

Failed tasks report:
- Lists specific objects that failed
- Failure reasons (permissions, not found, etc.)
- Can rerun job for failed objects only

<b>Much better than CLI where you have no idea
what succeeded or failed!</b>
</pre><br>

<b>Best Practices:</b>
<ul>
<li>Use S3 Batch Operations for bulk operations at scale</li>
<li>Use SSE-S3 unless KMS is specifically required</li>
<li>Enable default bucket encryption for new objects</li>
<li>Use S3 Inventory to track object metadata</li>
<li>Monitor Batch Operations job progress</li>
<li>Review completion reports for failures</li>
<li>Test on small subset before full execution</li>
<li>Ensure replication rules include encryption</li>
</ul><br>

<b>Timeline Comparison:</b>
<pre>
<b>Option A - S3 Batch Operations:</b>
1. Enable default encryption: 5 minutes
2. Create inventory: 24 hours (daily job)
3. Create Batch Operations job: 10 minutes
4. Job processes 10M objects: 1-2 days
5. Monitor completion: Automatic
Total: 2-3 days with minimal effort

<b>Option C - CLI Script:</b>
1. Enable default encryption: 5 minutes
2. Write script with error handling: 2-4 hours
3. Test script: 1 hour
4. Run script for 10M objects: 5-10 days
5. Monitor and handle errors: Manual, ongoing
6. Debug failures: Hours/days
Total: 7-14 days with significant manual effort

<b>Winner: Option A (faster + less effort)</b>
</pre><br>

<b>Complete Solution:</b>
<ol>
<li>Enable SSE-S3 default encryption on both S3 buckets</li>
<li>Generate S3 Inventory reports for both buckets</li>
<li>Create S3 Batch Operations jobs to copy objects in-place with SSE-S3</li>
<li>Monitor job progress and review completion reports</li>
<li>Verify replication synced encrypted objects</li>
<li>All existing objects now encrypted, new objects automatically encrypted</li>
</ol>

Most operationally efficient: Minimal effort, scalable, automated!
</div>
</div>

<!-- ================= Q2 ================= -->
<div class="question">
<pre>
92) A company is running an application in the AWS Cloud. 
The application collects and stores a large amount of unstructured data in an Amazon S3 bucket. 
The S3 bucket contains several terabytes of data and uses the S3 Standard storage class. 
The data increases in size by several gigabytes every day.

The company needs to query and analyze the data. The company does not access data that is more than 1 year old. 
However, the company must retain all the data indefinitely for compliance reasons.

Which solution will meet these requirements MOST cost-effectively?
</pre>
<div class="options">
<label>
<input type="radio" name="q2">
A. Use S3 Select to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive.
</label>

<label>
<input type="radio" name="q2">
B. Use Amazon Redshift Spectrum to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive.
</label>

<label>
<input type="radio" name="q2">
C. Use an AWS Glue Data Catalog and Amazon Athena to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive.
</label>

<label>
<input type="radio" name="q2">
D. Use Amazon Redshift Spectrum to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Intelligent-Tiering.
</label>
</div>

<button onclick="checkAnswer(this,[2])">Check Answer</button>
<button onclick="showAnswer(this,[2])">Show Answer</button>

<div class="explanation">
<button class="close-explanation" onclick="closeExplanation(this)">✕ Close</button>
<b>Correct Answer: C</b><br><br>

This question tests <b>cost-effective querying of S3 data with Athena and lifecycle policies for compliance</b>.<br><br>

<b>Requirements:</b>
<ul>
<li>Large unstructured data in S3 (terabytes, growing daily)</li>
<li>Need to query and analyze the data</li>
<li>Data older than 1 year not accessed</li>
<li>Must retain all data indefinitely (compliance)</li>
<li>MOST cost-effective solution</li>
</ul><br>

<b>Why Option C (Glue + Athena + Glacier Deep Archive) is Correct:</b><br>

<b>AWS Glue Data Catalog + Amazon Athena:</b>
<ul>
<li>✓ <b>Serverless:</b> No infrastructure to manage</li>
<li>✓ <b>Pay per query:</b> $5 per TB scanned</li>
<li>✓ <b>Query unstructured data:</b> JSON, CSV, Parquet, etc.</li>
<li>✓ <b>SQL queries:</b> Standard SQL syntax</li>
<li>✓ <b>No data movement:</b> Query data in S3 directly</li>
<li>✓ <b>Glue Crawler:</b> Auto-discovers schema</li>
</ul><br>

<b>S3 Glacier Deep Archive:</b>
<ul>
<li>✓ <b>Lowest cost storage:</b> $0.00099/GB/month ($1/TB/month)</li>
<li>✓ <b>Long-term archival:</b> Perfect for compliance data</li>
<li>✓ <b>Rarely accessed:</b> Data > 1 year old not needed</li>
<li>✓ <b>Retrieval when needed:</b> 12-48 hour retrieval time acceptable</li>
</ul><br>

<b>Architecture:</b>
<pre>
<b>Data Flow:</b>

Application writes data → S3 Standard
    ↓
AWS Glue Crawler scans S3
    ↓
Creates/updates Glue Data Catalog (schema)
    ↓
Amazon Athena queries using catalog
    ↓
Analysts run SQL queries on recent data (< 1 year)
    ↓
S3 Lifecycle Policy (after 365 days)
    ↓
Data transitioned to Glacier Deep Archive
    ↓
Compliance retention met, minimal cost


<b>Storage Cost Optimization:</b>

0-365 days: S3 Standard ($0.023/GB/month)
  - Actively queried with Athena
  - Fast access required
  
365+ days: Glacier Deep Archive ($0.00099/GB/month)
  - Compliance archive only
  - Retrieval: 12-48 hours if ever needed
  - 96% cost reduction!
</pre><br>

<b>Implementation:</b>
<pre>
# 1. Create Glue Database
aws glue create-database \
  --database-input '{
    "Name": "analytics_db",
    "Description": "Database for S3 data analytics"
  }'

# 2. Create Glue Crawler to discover schema
aws glue create-crawler \
  --name s3-data-crawler \
  --role arn:aws:iam::123456789012:role/GlueServiceRole \
  --database-name analytics_db \
  --targets '{
    "S3Targets": [{
      "Path": "s3://my-data-bucket/"
    }]
  }' \
  --schedule "cron(0 2 * * ? *)"  # Daily at 2 AM

# 3. Run crawler to populate catalog
aws glue start-crawler --name s3-data-crawler

# 4. Query with Athena
-- Create table (or use Glue-created table)
CREATE EXTERNAL TABLE IF NOT EXISTS analytics_db.app_logs (
  timestamp STRING,
  user_id STRING,
  event_type STRING,
  data STRUCT<...>
)
STORED AS PARQUET
LOCATION 's3://my-data-bucket/';

-- Query recent data
SELECT event_type, COUNT(*) as count
FROM analytics_db.app_logs
WHERE timestamp >= date_sub(current_date, 365)
GROUP BY event_type
ORDER BY count DESC;

# 5. Create S3 Lifecycle Policy
aws s3api put-bucket-lifecycle-configuration \
  --bucket my-data-bucket \
  --lifecycle-configuration '{
    "Rules": [{
      "Id": "Archive old data",
      "Status": "Enabled",
      "Filter": {
        "Prefix": ""
      },
      "Transitions": [{
        "Days": 365,
        "StorageClass": "DEEP_ARCHIVE"
      }]
    }]
  }'
</pre><br>

<b>Cost Analysis (10 TB data, 10 GB new daily):</b>
<pre>
<b>Storage Costs:</b>

Year 1 (all data in S3 Standard):
- 10 TB × $23/TB/month = $230/month
- Annual: $2,760

Year 2 (with lifecycle policy):
- Recent data (1 year): 3.65 TB × $23/TB = $84/month
- Old data (1 year): 10 TB × $1/TB = $10/month
- Total: $94/month
- Annual: $1,128
- <b>Savings: $1,632/year (59% reduction!)</b>

Year 5:
- Recent: 3.65 TB × $23 = $84/month
- Old: 13.65 TB × $1 = $14/month
- Total: $98/month
- <b>Massive savings as old data accumulates!</b>


<b>Query Costs (Athena):</b>
- 100 GB scanned/month × $5/TB = $0.50/month
- Minimal compared to storage savings

<b>Glue Crawler:</b>
- $0.44/hour (runs ~10 min/day) = $2/month

<b>Total Monthly Cost (Year 2): ~$97/month</b>
vs Year 1: $230/month
<b>Savings: $133/month = $1,596/year</b>
</pre><br>

<b>Why Other Options Are Wrong:</b><br>

<b>Option A - S3 Select + Glacier Deep Archive:</b>
<ul>
<li>✓ Glacier Deep Archive is correct</li>
<li>✗ <b>S3 Select limited:</b> Simple filtering only, not full analytics</li>
<li>✗ <b>No SQL support:</b> Can't do complex queries, joins, aggregations</li>
<li>✗ <b>One file at a time:</b> Not designed for analyzing terabytes</li>
<li>✗ <b>No schema management:</b> No data catalog</li>
</ul>

S3 Select limitations:
<pre>
<b>S3 Select can do:</b>
- SELECT * FROM s3object WHERE column = 'value'
- Simple filtering on single file
- Returns subset of data from one object

<b>S3 Select CANNOT do:</b>
✗ JOIN multiple files
✗ Complex aggregations across TB of data
✗ Query partitioned data efficiently
✗ Create views or reusable queries
✗ Manage schema/metadata

<b>Use case:</b> Extract specific records from a large file
<b>NOT for:</b> Full data analytics across terabytes

Question asks to "query and analyze" → Need Athena!
</pre><br>

<b>Option B - Redshift Spectrum + Glacier Deep Archive:</b>
<ul>
<li>✓ Can query S3 data</li>
<li>✓ Glacier Deep Archive is correct</li>
<li>✗ <b>Requires Redshift cluster:</b> Additional infrastructure cost</li>
<li>✗ <b>Redshift cluster 24/7 cost:</b> Minimum $180/month (dc2.large)</li>
<li>✗ <b>More expensive than Athena</b></li>
<li>✗ <b>Overkill for unstructured S3 data</b></li>
</ul>

Cost comparison:
<pre>
<b>Redshift Spectrum (Option B):</b>
- Redshift cluster: dc2.large = $180/month (minimum)
- Spectrum query: $5/TB scanned (same as Athena)
- Total: $180+ per month

<b>Athena (Option C):</b>
- No cluster: $0
- Query cost: $5/TB scanned
- Glue crawler: $2/month
- Total: $7/month for same query workload

<b>Athena is 96% cheaper for S3 queries!</b>

When to use Redshift Spectrum:
- Already have Redshift cluster for DW
- Need to join S3 data with Redshift tables
- Complex transformations and loads

When to use Athena:
- Query S3 data directly (this question)
- Serverless, pay-per-query
- No existing Redshift investment
</pre><br>

<b>Option D - Redshift Spectrum + S3 Intelligent-Tiering:</b>
<ul>
<li>✗ <b>Redshift cluster cost:</b> Expensive infrastructure</li>
<li>✗ <b>S3 Intelligent-Tiering wrong tier:</b> More expensive than Glacier</li>
<li>✗ <b>Not cost-optimized</b></li>
</ul>

Storage tier comparison:
<pre>
<b>For 10 TB of data >1 year old (not accessed):</b>

S3 Intelligent-Tiering (Option D):
- Frequent Access: $23/TB/month
- Infrequent Access: $12.50/TB/month (after 30 days)
- Archive Access: $4.50/TB/month (after 90 days)
- Deep Archive Access: $1/TB/month (after 180 days)
- Monitoring: $0.0025/1000 objects = $25/month extra
- Best case: $1/TB + $25 monitoring = $35/month

S3 Glacier Deep Archive (Option C):
- $1/TB/month
- No monitoring fees
- Total: $10/month

<b>Glacier Deep Archive saves $25/month!</b>

Use Intelligent-Tiering when:
- Unpredictable access patterns
- Need fast retrieval sometimes

Use Glacier Deep Archive when:
- Predictable: data >1 year not accessed
- Compliance/archive only
- Can wait 12-48 hours for retrieval
</pre><br>

<b>Comparison Table:</b>
<table border="1" cellpadding="5">
<tr><th>Feature</th><th>Option A</th><th>Option B</th><th><b>Option C</b></th><th>Option D</th></tr>
<tr><td>Query Engine</td><td>S3 Select</td><td>Redshift Spectrum</td><td><b>Athena ✓</b></td><td>Redshift Spectrum</td></tr>
<tr><td>Infrastructure</td><td>None</td><td>Redshift cluster</td><td><b>Serverless ✓</b></td><td>Redshift cluster</td></tr>
<tr><td>Query Cost</td><td>Low</td><td>$180+/month</td><td><b>$5/TB ✓</b></td><td>$180+/month</td></tr>
<tr><td>Storage (old)</td><td><b>Deep Archive ✓</b></td><td><b>Deep Archive ✓</b></td><td><b>Deep Archive ✓</b></td><td>Intelligent-Tier</td></tr>
<tr><td>Analytics Capability</td><td>Limited</td><td>Full SQL</td><td><b>Full SQL ✓</b></td><td>Full SQL</td></tr>
<tr><td>Total Monthly Cost</td><td>~$100</td><td>~$290</td><td><b>~$97 ✓</b></td><td>~$215</td></tr>
</table><br>

<b>Athena Best Practices for Cost Optimization:</b>
<pre>
<b>1. Partition data:</b>
s3://bucket/year=2024/month=01/day=15/data.parquet
- Athena only scans relevant partitions
- Reduces data scanned = lower cost

<b>2. Use columnar formats:</b>
- Parquet or ORC instead of JSON/CSV
- 10x less data scanned for column-based queries
- $5/TB → $0.50/TB effective cost

<b>3. Compress data:</b>
- Gzip, Snappy compression
- Smaller files = less data scanned

<b>4. Create views for common queries:</b>
CREATE VIEW recent_events AS
SELECT * FROM app_logs
WHERE timestamp >= date_sub(current_date, 365);

<b>5. Use LIMIT when exploring:</b>
SELECT * FROM app_logs LIMIT 100;
- Don't scan full table for testing
</pre><br>

<b>Glue Crawler Automation:</b>
<pre>
<b>What Glue Crawler does:</b>
1. Scans S3 bucket
2. Infers schema from data
3. Creates/updates tables in Glue Data Catalog
4. Handles partitions automatically
5. Detects new data and schema changes

<b>Benefits:</b>
- No manual schema definition
- Automatic partition discovery
- Keeps catalog up-to-date
- Works with Athena, Redshift Spectrum, EMR

<b>Example:</b>
Data structure:
s3://bucket/logs/2024/01/15/file1.json
s3://bucket/logs/2024/01/16/file2.json

Crawler creates:
- Table: logs
- Partitions: year, month, day
- Schema: auto-detected from JSON

Athena query:
SELECT * FROM logs WHERE year='2024' AND month='01';
-- Only scans January 2024 partition!
</pre><br>

<b>Retrieval from Glacier Deep Archive:</b>
<pre>
<b>If compliance audit requires old data:</b>

1. Restore request:
aws s3api restore-object \
  --bucket my-bucket \
  --key old-data/2022-01-15.json \
  --restore-request '{
    "Days": 7,
    "GlacierJobParameters": {
      "Tier": "Standard"
    }
  }'

2. Wait 12-48 hours for retrieval

3. Data available in S3 for 7 days

4. Query with Athena as normal

<b>Retrieval Costs:</b>
- Standard: $0.02/GB (12-48 hours)
- Bulk: $0.0025/GB (48 hours)

For compliance: Acceptable trade-off
- Save 96% on storage
- Pay retrieval cost only when audited
</pre><br>

<b>Complete Solution Architecture:</b>
<pre>
┌─────────────────────────────────────────────┐
│         Application Data Pipeline           │
└──────────────────┬──────────────────────────┘
                   ↓
        ┌──────────────────────┐
        │   S3 Bucket          │
        │   - Standard class   │← Daily writes
        │   - Unstructured     │
        └──────────┬───────────┘
                   │
        ┌──────────┴───────────┐
        │  AWS Glue Crawler    │← Nightly scan
        │  - Discovers schema  │
        │  - Creates tables    │
        └──────────┬───────────┘
                   ↓
        ┌──────────────────────┐
        │ Glue Data Catalog    │← Metadata store
        │ - Table definitions  │
        │ - Partitions         │
        └──────────┬───────────┘
                   ↓
        ┌──────────────────────┐
        │  Amazon Athena       │← Query engine
        │  - SQL queries       │
        │  - Analytics         │
        └──────────────────────┘
                   
        
        S3 Lifecycle Policy (365 days)
                   ↓
        ┌──────────────────────┐
        │ Glacier Deep Archive │← Long-term archive
        │ - $1/TB/month        │
        │ - Compliance         │
        └──────────────────────┘

<b>Cost-optimized, scalable, serverless!</b>
</pre><br>

<b>Why This is Most Cost-Effective:</b>
<ol>
<li><b>No infrastructure:</b> Athena serverless, no Redshift cluster</li>
<li><b>Pay per query:</b> Only pay when analyzing data</li>
<li><b>Cheapest storage:</b> Glacier Deep Archive for old data</li>
<li><b>Auto-scaling:</b> Handles TBs to PBs automatically</li>
<li><b>Minimal operations:</b> Glue crawler automates schema management</li>
<li><b>Compliance met:</b> Indefinite retention at $1/TB/month</li>
</ol>

Perfect for cost-effective analytics with compliance requirements!
</div>
</div>

<!-- ================= Q3 ================= -->
<div class="question">
<pre>
93) A video processing company wants to build a machine learning (ML) model by using 600 TB of compressed data that is stored as thousands of files in the company's
on-premises network attached storage system. The company does not have the necessary compute resources on premises for ML experiments and wants to use AWS.

The company needs to complete the data transfer to AWS within 3 weeks. The data transfer will be a one-time transfer. The data must be encrypted in transit. 
The measured upload speed of the company's internet connection is 100 Mbps, and multiple departments share the connection.

Which solution will meet these requirements MOST cost-effectively?
</pre>
<div class="options">
<label>
<input type="radio" name="q3">
A. Order several AWS Snowball Edge Storage Optimized devices by using the AWS Management Console. Configure the devices with a destination S3 bucket. Copy the data to the devices. Ship the devices back to AWS.
</label>

<label>
<input type="radio" name="q3">
B. Set up a 10 Gbps AWS Direct Connect connection between the company location and the nearest AWS Region. Transfer the data over a VPN connection into the Region to store the data in Amazon S3.
</label>

<label>
<input type="radio" name="q3">
C. Create a VPN connection between the on-premises network attached storage and the nearest AWS Region. Transfer the data over the VPN connection.
</label>

<label>
<input type="radio" name="q3">
D. Deploy an AWS Storage Gateway file gateway on premises. Configure the file gateway with a destination S3 bucket. Copy the data to the file gateway.
</label>
</div>

<button onclick="checkAnswer(this,[0])">Check Answer</button>
<button onclick="showAnswer(this,[0])">Show Answer</button>

<div class="explanation">
<button class="close-explanation" onclick="closeExplanation(this)">✕ Close</button>
<b>Correct Answer: A</b><br><br>

This question tests <b>large-scale data migration to AWS using AWS Snowball for offline transfer</b>.<br><br>

<b>Requirements:</b>
<ul>
<li>Transfer 600 TB of data from on-premises to AWS</li>
<li>Complete within 3 weeks (21 days)</li>
<li>One-time transfer</li>
<li>Data must be encrypted in transit</li>
<li>Internet: 100 Mbps shared with other departments</li>
<li>MOST cost-effective solution</li>
</ul><br>

<b>Key Calculation - Can 100 Mbps Handle 600 TB in 3 Weeks?</b><br>

<pre>
<b>Bandwidth Analysis:</b>

Available bandwidth: 100 Mbps (shared!)
Realistic usable: ~50 Mbps (shared with departments)

Transfer time for 600 TB:
- 600 TB = 600 × 1024 × 1024 × 1024 GB = 644,245,094 GB
- 644,245,094 GB × 8 bits = 5,153,960,752 Gb (gigabits)
- Time = 5,153,960,752 Gb ÷ 50 Mbps
- Time = 103,079,215 seconds
- Time = 1,717,987 minutes
- Time = 28,633 hours
- Time = <b>1,193 days (3.3 years!)</b>

Even at full 100 Mbps:
- Time = 597 days (1.6 years)

<b>Conclusion: Internet transfer NOT FEASIBLE!</b>
Cannot meet 3-week deadline via internet.
</pre><br>

<b>Why Option A (AWS Snowball Edge) is Correct:</b><br>

<b>AWS Snowball Edge Storage Optimized:</b>
<ul>
<li>✓ <b>Capacity:</b> 80 TB usable per device</li>
<li>✓ <b>Number needed:</b> 8 devices for 600 TB (600÷80=7.5)</li>
<li>✓ <b>Encrypted:</b> 256-bit encryption (hardware encrypted)</li>
<li>✓ <b>Physical transfer:</b> Ship via carrier (days, not years!)</li>
<li>✓ <b>Cost-effective:</b> ~$300/device for 10-day job</li>
<li>✓ <b>Meets 3-week timeline</b></li>
</ul><br>

<b>Snowball Timeline:</b>
<pre>
<b>Day 1-2:</b> Order 8 Snowball devices from AWS Console
    ↓
<b>Day 3-5:</b> AWS ships devices to company (2-3 days)
    ↓
<b>Day 6-12:</b> Copy 600 TB to 8 devices (parallel)
  - 75 TB per device ÷ 7 days = 10.7 TB/day/device
  - Local network speed: 10 Gbps NAS → easily achievable
  - Each device: ~150 hours to fill at 150 MB/s
    ↓
<b>Day 13-14:</b> Ship devices back to AWS
    ↓
<b>Day 15-18:</b> AWS uploads data to S3 (internal high-speed)
    ↓
<b>Day 19:</b> Data available in S3
    ↓
<b>Total: ~19 days ✓ (within 3-week deadline!)</b>
</pre><br>

<b>Snowball Cost Analysis:</b>
<pre>
<b>AWS Snowball Edge Storage Optimized:</b>
- Device fee: $300 per device (10-day job)
- Shipping: Included (to/from AWS)
- Data transfer IN to S3: Free!

8 devices × $300 = $2,400 total

Additional days (if needed):
- $30/day per device after first 10 days

<b>Total cost: ~$2,400</b>
</pre><br>

<b>Implementation:</b>
<pre>
# 1. Create S3 bucket for destination
aws s3 mb s3://ml-training-data-bucket

# 2. Create Snowball job via AWS Console or CLI
aws snowball create-job \
  --job-type IMPORT \
  --resources '{
    "S3Resources": [{
      "BucketArn": "arn:aws:s3:::ml-training-data-bucket"
    }]
  }' \
  --description "ML training data transfer - 600TB" \
  --address-id <address-id> \
  --kms-key-arn arn:aws:kms:us-east-1:123456789012:key/abc123 \
  --role-arn arn:aws:iam::123456789012:role/SnowballRole \
  --snowball-capacity-preference T80

# 3. Order 8 devices (repeat job creation or use clone)

# 4. When devices arrive, connect and unlock
snowballEdge unlock-device \
  --endpoint https://192.168.1.100 \
  --manifest-file path/to/manifest.bin \
  --unlock-code <29-char-code>

# 5. Copy data to Snowball
aws s3 cp /nas/ml-data/ s3://snowball-bucket/ \
  --endpoint http://192.168.1.100:8080 \
  --profile snowball \
  --recursive

# 6. Monitor progress
snowballEdge describe-device

# 7. Power off and ship back to AWS
snowballEdge stop-device

# 8. AWS imports data to S3 (automatic)
</pre><br>

<b>Why Other Options Are Wrong:</b><br>

<b>Option B - 10 Gbps Direct Connect + VPN:</b>
<ul>
<li>✗ <b>Setup time:</b> 2-4 weeks just to provision Direct Connect!</li>
<li>✗ <b>Cannot meet 3-week deadline</b></li>
<li>✗ <b>Expensive:</b> Port hours + data transfer</li>
<li>✗ <b>One-time transfer:</b> Not justified for permanent connection</li>
</ul>

Direct Connect timeline:
<pre>
<b>Direct Connect Provisioning:</b>
Week 1: Submit LOA-CFA (Letter of Authorization)
Week 2-3: Cross-connect at colocation facility
Week 3-4: Testing and BGP configuration
Week 4: Ready to transfer data

<b>Problem: 3-4 weeks just to set up!</b>
Exceeds 3-week deadline before transfer starts.

Transfer time (if ready):
- 600 TB at 10 Gbps (theoretical max)
- 600 × 8 Tb ÷ 10 Gbps = 480,000 seconds = 5.5 days
- Realistic (80% utilization): 7 days

Total: 4 weeks setup + 1 week transfer = 5 weeks
<b>Misses 3-week deadline!</b>
</pre>

Cost comparison:
<pre>
<b>Direct Connect Costs:</b>
- Port fee: $0.30/hour for 10 Gbps = $216/month
- Data transfer out: Free (inbound to AWS)
- Cross-connect: $500-1000 setup
- Installation: $500-2000
- Total first month: ~$3,000-4,000

For one-time transfer: Not justified!
Plus cannot meet timeline.
</pre><br>

<b>Option C - VPN over Internet:</b>
<ul>
<li>✗ <b>Too slow:</b> 100 Mbps takes 1.6 years!</li>
<li>✗ <b>Cannot meet 3-week deadline</b></li>
<li>✗ <b>Bandwidth contention:</b> Shared with other departments</li>
<li>✗ <b>Unreliable:</b> Internet disruptions</li>
</ul>

VPN problems:
<pre>
<b>Transfer time calculation (shown earlier):</b>
- 600 TB at 100 Mbps = 597 days
- At realistic 50 Mbps (shared) = 1,193 days

<b>3 weeks = 21 days</b>
<b>VPN needs: 597-1,193 days</b>

<b>28x-57x longer than deadline!</b>

Additional issues:
- VPN throughput often lower than internet speed
- Encryption overhead reduces effective bandwidth
- Single point of failure
- Would disrupt other departments' internet usage
</pre><br>

<b>Option D - Storage Gateway File Gateway:</b>
<ul>
<li>✗ <b>Still uses internet:</b> Same 100 Mbps limitation</li>
<li>✗ <b>Takes years:</b> Cannot meet 3-week deadline</li>
<li>✗ <b>Designed for hybrid storage:</b> Not bulk migration</li>
<li>✗ <b>Caching delays:</b> Not optimized for one-time transfer</li>
</ul>

Storage Gateway issues:
<pre>
<b>How File Gateway works:</b>
1. Deploy gateway on-prem (VM or hardware)
2. Configure S3 bucket as backend
3. Present NFS/SMB share to applications
4. Data written to local cache
5. Gateway uploads to S3 over internet

<b>Problem: Upload still limited by 100 Mbps internet!</b>

Same transfer time as Option C:
- 600 TB ÷ 100 Mbps = 597 days minimum

Storage Gateway is for:
- Ongoing hybrid cloud storage
- Gradual migration over months
- Low-latency local access with cloud backup

NOT for:
- One-time bulk migration
- Tight deadlines (3 weeks)
- Hundreds of terabytes
</pre><br>

<b>Comparison Table:</b>
<table border="1" cellpadding="5">
<tr><th>Solution</th><th>Transfer Time</th><th>Cost</th><th>Meets Deadline?</th><th>Encrypted?</th></tr>
<tr><td><b>Snowball (A)</b></td><td><b>~19 days</b></td><td><b>~$2,400</b></td><td><b>✓ Yes</b></td><td><b>✓ Yes</b></td></tr>
<tr><td>Direct Connect (B)</td><td>5+ weeks</td><td>~$4,000</td><td>✗ No</td><td>✓ Yes (VPN)</td></tr>
<tr><td>VPN (C)</td><td>597-1193 days</td><td>Low</td><td>✗ No</td><td>✓ Yes</td></tr>
<tr><td>Storage GW (D)</td><td>597+ days</td><td>Low</td><td>✗ No</td><td>✓ Yes</td></tr>
</table><br>

<b>Snowball vs Snowmobile Decision:</b>
<pre>
<b>AWS Data Transfer Service Options:</b>

Snowball Edge (80 TB):
- Use for: 10 TB - 10 PB transfers
- Cost: $300 per device (10 days)
- Parallel: Order multiple devices

Snowmobile (100 PB):
- Use for: >10 PB transfers
- Cost: ~$0.005/GB/month
- Single 45-foot shipping container

<b>For 600 TB:</b>
- Less than 10 PB
- Snowball Edge is appropriate
- 8 devices = $2,400
- Snowmobile would be overkill and more expensive setup

Rule of thumb:
- < 10 PB → Snowball Edge
- > 10 PB → Consider Snowmobile
</pre><br>

<b>When to Use Each Migration Method:</b>
<pre>
<b>AWS Snowball Edge:</b>
✓ Large datasets (TB to PBs)
✓ Tight timelines
✓ Limited/expensive internet bandwidth
✓ One-time or periodic migrations
✓ Data sovereignty (physical control)

<b>AWS Direct Connect:</b>
✓ Ongoing hybrid connectivity
✓ Consistent high throughput needed
✓ Low latency requirements
✓ Long-term investment justified
✓ Time to provision available (weeks)

<b>Internet (VPN/Storage Gateway):</b>
✓ Small datasets (< 1 TB)
✓ No timeline constraints
✓ Good internet bandwidth (1+ Gbps)
✓ Ongoing incremental transfers
✓ Quick setup needed

<b>AWS DataSync:</b>
✓ Ongoing synchronization
✓ Network bandwidth available
✓ Automated scheduling
✓ 10s-100s of TB over weeks/months
</pre><br>

<b>Snowball Security Features:</b>
<pre>
<b>Encryption in Transit (meets requirement):</b>
- All data encrypted with 256-bit encryption
- Keys managed by AWS KMS
- Data never stored unencrypted on device
- Tamper-resistant enclosure
- Trusted Platform Module (TPM)

<b>Data Protection:</b>
- E Ink shipping label (auto-erases after job)
- GPS tracking
- Secure erase after import complete
- Chain of custody logging

<b>Compliance:</b>
- HIPAA compliant
- PCI DSS
- SOC 1, 2, 3
- FIPS 140-2 Level 2
</pre><br>

<b>Snowball Operational Workflow:</b>
<pre>
<b>Phase 1: Order (Day 1-2)</b>
1. Create import job in AWS Console
2. Specify S3 destination bucket
3. Choose KMS key for encryption
4. Provide shipping address
5. Order multiple devices (8 for 600 TB)

<b>Phase 2: Receive (Day 3-5)</b>
1. AWS ships devices
2. Track shipment
3. Receive and inspect devices

<b>Phase 3: Copy Data (Day 6-12)</b>
1. Connect Snowball to local network
2. Power on and obtain IP address
3. Unlock with credentials from Console
4. Install Snowball client
5. Copy data using S3 API or CLI
6. Monitor progress
7. Validate checksums

<b>Phase 4: Return (Day 13-14)</b>
1. Stop all copy operations
2. Power off device
3. E Ink label appears automatically
4. Ship via prepaid carrier label

<b>Phase 5: Import (Day 15-18)</b>
1. AWS receives device at facility
2. Device connected to AWS network
3. Data imported to S3 at high speed (40 Gbps+)
4. Device securely erased
5. Notification sent when complete

<b>Phase 6: Verify (Day 19+)</b>
1. Verify data in S3
2. Check import logs
3. Begin ML model training!
</pre><br>

<b>Parallelization Strategy:</b>
<pre>
<b>8 Snowball devices can process in parallel:</b>

Device 1: /nas/data/set1 (75 TB)
Device 2: /nas/data/set2 (75 TB)
Device 3: /nas/data/set3 (75 TB)
Device 4: /nas/data/set4 (75 TB)
Device 5: /nas/data/set5 (75 TB)
Device 6: /nas/data/set6 (75 TB)
Device 7: /nas/data/set7 (75 TB)
Device 8: /nas/data/set8 (75 TB)

<b>Benefits:</b>
- 8x parallelism
- Faster completion
- Risk distribution (one device failure ≠ complete loss)
- Can ship as completed (don't wait for all)

<b>NAS bandwidth:</b>
If NAS has 10 Gbps network:
- 10 Gbps = 1.25 GB/s = 4.5 TB/hour
- 75 TB per device = ~17 hours per device
- With 8 devices: Still ~17 hours (parallel)
- Very fast on local network!
</pre><br>

<b>Cost-Effectiveness Proof:</b>
<pre>
<b>Total Cost of Ownership:</b>

Option A - Snowball Edge:
- Device fees: 8 × $300 = $2,400
- Shipping: Included
- S3 storage: $600/month (600 TB × $1/TB Glacier)
- One-time cost: $2,400
- Monthly cost: $600

Option B - Direct Connect:
- Setup: $2,000
- Port fees (1 month): $216
- Data transfer: $0 (inbound free)
- One-time cost: $2,216
- But: Cannot meet deadline
- Ongoing cost if kept: $216/month

Option C/D - Internet:
- VPN/Gateway: ~$100 setup
- Internet bandwidth: Existing
- One-time cost: $100
- But: Takes 2-3 years!

<b>Winner: Snowball Edge</b>
- Only option meeting 3-week deadline
- Reasonable one-time cost
- Purpose-built for this scenario
</pre><br>

<b>Post-Migration Optimization:</b>
<pre>
<b>After 600 TB in S3:</b>

1. Use S3 Lifecycle policies:
   - Transition to Glacier for archival
   - Delete temporary files after ML training

2. Enable S3 Intelligent-Tiering:
   - Automatically moves between access tiers
   - Optimizes cost based on access patterns

3. Organize data:
   - Partition by training/validation/test
   - Use S3 prefixes for organization

4. Set up SageMaker:
   - Train ML models directly from S3
   - Use distributed training across GPUs
   - Leverage S3 high throughput (100 Gbps+)

<b>ML training can start immediately after import!</b>
</pre><br>

<b>Alternative: AWS DataSync (Why Not Here?):</b>
<pre>
<b>AWS DataSync</b> - Automated data transfer service

Good for:
- Network-based transfers (up to 10 Gbps per agent)
- Ongoing synchronization
- Incremental updates
- Scheduled transfers

For 600 TB at 10 Gbps:
- Transfer time: 5.5 days (theoretical)
- Realistic: 7-10 days

<b>Why not chosen:</b>
- Requires 10 Gbps network (company has 100 Mbps)
- Still network-based (subject to internet limitations)
- Would take months over 100 Mbps connection
- Snowball is faster for one-time bulk migration

<b>DataSync is great when bandwidth available,
Snowball when bandwidth is limited!</b>
</pre><br>

<b>Complete Solution:</b>
<ol>
<li>Order 8 AWS Snowball Edge Storage Optimized devices</li>
<li>Configure devices with destination S3 bucket and KMS encryption</li>
<li>Receive devices in 2-3 days</li>
<li>Copy 75 TB to each device in parallel (~7 days)</li>
<li>Ship devices back to AWS</li>
<li>AWS imports data to S3 at high speed (3-4 days)</li>
<li>Verify data integrity and begin ML training</li>
<li>Total timeline: ~19 days (within 3-week deadline)</li>
<li>Total cost: ~$2,400 (most cost-effective for timeline)</li>
</ol>

Snowball: The right tool for large-scale, time-sensitive data migration!
</div>
</div>

<!-- ================= Q4 ================= -->
<div class="question">
<pre>
94) A company has migrated its forms-processing application to AWS. When users interact with the application, they upload scanned forms as files through a web 
application. A database stores user metadata and references to files that are stored in Amazon S3. The web application runs on Amazon EC2 instances and 
an Amazon RDS for PostgreSQL database.

When forms are uploaded, the application sends notifications to a team through Amazon Simple Notification Service (Amazon SNS).
A team member then logs in and processes each form. The team member performs data validation on the form and extracts relevant data before
entering the information into another system that uses an API.

A solutions architect needs to automate the manual processing of the forms. 
The solution must provide accurate form extraction, minimize time to market, and minimize long-term operational overhead.

Which solution will meet these requirements?
</pre>
<div class="options">
<label>
<input type="radio" name="q4">
A. Develop custom libraries to perform optical character recognition (OCR) on the forms. Deploy the libraries to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster as an application tier. Use this tier to process the forms when forms are uploaded. Store the output in Amazon S3. Parse this output by extracting the data into an Amazon DynamoDB table. Submit the data to the target system's API. Host the new application tier on EC2 instances.
</label>

<label>
<input type="radio" name="q4">
B. Extend the system with an application tier that uses AWS Step Functions and AWS Lambda. Configure this tier to use artificial intelligence and machine learning (AI/ML) models that are trained and hosted on an EC2 instance to perform optical character recognition (OCR) on the forms when forms are uploaded. Store the output in Amazon S3. Parse this output by extracting the data that is required within the application tier. Submit the data to the target system's API.
</label>

<label>
<input type="radio" name="q4">
C. Host a new application tier on EC2 instances. Use this tier to call endpoints that host artificial intelligence and machine learning (AI/ML) models that are trained and hosted in Amazon SageMaker to perform optical character recognition (OCR) on the forms. Store the output in Amazon ElastiCache. Parse this output by extracting the data that is required within the application tier. Submit the data to the target system's API.
</label>

<label>
<input type="radio" name="q4">
D. Extend the system with an application tier that uses AWS Step Functions and AWS Lambda. Configure this tier to use Amazon Textract and Amazon Comprehend to perform optical character recognition (OCR) on the forms when forms are uploaded. Store the output in Amazon S3. Parse this output by extracting the data that is required within the application tier. Submit the data to the target system's API.
</label>
</div>

<button onclick="checkAnswer(this,[3])">Check Answer</button>
<button onclick="showAnswer(this,[3])">Show Answer</button>

<div class="explanation">
<button class="close-explanation" onclick="closeExplanation(this)">✕ Close</button>
<b>Correct Answer: D</b><br><br>

This question tests <b>serverless document processing automation using Amazon Textract and orchestration with Step Functions</b>.<br><br>

<b>Requirements:</b>
<ul>
<li>Automate manual form processing</li>
<li>Perform OCR and data extraction from scanned forms</li>
<li>Provide accurate form extraction</li>
<li>Minimize time to market</li>
<li>Minimize long-term operational overhead</li>
<li>Extract data and submit to API</li>
</ul><br>

<b>Why Option D (Step Functions + Lambda + Textract + Comprehend) is Correct:</b><br>

<b>Amazon Textract:</b>
<ul>
<li>✓ <b>Fully managed OCR service:</b> No infrastructure to manage</li>
<li>✓ <b>Specialized for forms:</b> Extracts tables, forms, key-value pairs</li>
<li>✓ <b>Pre-trained models:</b> Works immediately, no training required</li>
<li>✓ <b>High accuracy:</b> State-of-the-art ML models</li>
<li>✓ <b>Confidence scores:</b> Quality metrics for validation</li>
<li>✓ <b>Pay per use:</b> No upfront costs</li>
</ul><br>

<b>Amazon Comprehend:</b>
<ul>
<li>✓ <b>Natural language processing:</b> Understands extracted text</li>
<li>✓ <b>Entity extraction:</b> Identifies names, dates, organizations</li>
<li>✓ <b>Sentiment analysis:</b> If needed for content</li>
<li>✓ <b>Key phrase extraction:</b> Summarizes important information</li>
<li>✓ <b>Fully managed:</b> No model training required</li>
</ul><br>

<b>AWS Step Functions:</b>
<ul>
<li>✓ <b>Workflow orchestration:</b> Coordinates multi-step process</li>
<li>✓ <b>Error handling:</b> Built-in retry and error branches</li>
<li>✓ <b>Visual workflow:</b> Easy to understand and modify</li>
<li>✓ <b>Serverless:</b> No servers to manage</li>
<li>✓ <b>State management:</b> Tracks processing status</li>
</ul><br>

<b>AWS Lambda:</b>
<ul>
<li>✓ <b>Serverless compute:</b> No EC2 management</li>
<li>✓ <b>Auto-scaling:</b> Handles variable form upload volume</li>
<li>✓ <b>Pay per execution:</b> Cost-efficient</li>
<li>✓ <b>Fast deployment:</b> Quick time to market</li>
</ul><br>

<b>Architecture:</b>
<pre>
<b>Automated Form Processing Flow:</b>

User uploads form → S3 Bucket
    ↓
S3 Event Notification
    ↓
EventBridge / Lambda Trigger
    ↓
AWS Step Functions State Machine starts
    ↓
┌─────────────────────────────────┐
│   Step 1: Textract Analysis     │
│   Lambda → Textract API          │
│   - Extract text, tables, forms  │
│   - Get key-value pairs          │
│   - Confidence scores            │
└──────────────┬──────────────────┘
               ↓
┌─────────────────────────────────┐
│ Step 2: Store Raw Results       │
│ Save Textract output to S3      │
└──────────────┬──────────────────┘
               ↓
┌─────────────────────────────────┐
│ Step 3: Comprehend Analysis      │
│ Lambda → Comprehend API          │
│ - Extract entities (names, etc.) │
│ - Identify key phrases           │
└──────────────┬──────────────────┘
               ↓
┌─────────────────────────────────┐
│ Step 4: Parse & Transform        │
│ Lambda function:                 │
│ - Combine Textract + Comprehend  │
│ - Validate data                  │
│ - Format for target API          │
└──────────────┬──────────────────┘
               ↓
┌─────────────────────────────────┐
│ Step 5: Submit to Target API     │
│ Lambda → External API            │
│ - HTTP POST with form data       │
│ - Handle response                │
└──────────────┬──────────────────┘
               ↓
┌─────────────────────────────────┐
│ Step 6: Update Database          │
│ Update RDS with processing status│
│ - Mark as processed              │
│ - Store extraction results       │
└──────────────┬──────────────────┘
               ↓
Success Notification (SNS)
    ↓
Process Complete!


<b>Error Handling (Step Functions):</b>
Each step has:
- Retry logic (exponential backoff)
- Catch errors → Error handling state
- Dead letter queue for failures
- Admin notification on persistent errors
</pre><br>

<b>Implementation:</b>
<pre>
# Step Functions State Machine Definition (ASL)
{
  "Comment": "Automated Form Processing Workflow",
  "StartAt": "TextractAnalysis",
  "States": {
    "TextractAnalysis": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:region:account:function:textract-processor",
      "Parameters": {
        "s3Bucket.$": "$.detail.bucket.name",
        "s3Key.$": "$.detail.object.key"
      },
      "Retry": [{
        "ErrorEquals": ["States.ALL"],
        "IntervalSeconds": 2,
        "MaxAttempts": 3,
        "BackoffRate": 2.0
      }],
      "Catch": [{
        "ErrorEquals": ["States.ALL"],
        "Next": "ProcessingFailed"
      }],
      "Next": "StoreRawResults"
    },
    "StoreRawResults": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:region:account:function:store-results",
      "Next": "ComprehendAnalysis"
    },
    "ComprehendAnalysis": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:region:account:function:comprehend-processor",
      "Next": "ParseAndValidate"
    },
    "ParseAndValidate": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:region:account:function:data-parser",
      "Next": "SubmitToAPI"
    },
    "SubmitToAPI": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:region:account:function:api-submitter",
      "Retry": [{
        "ErrorEquals": ["APIError"],
        "IntervalSeconds": 5,
        "MaxAttempts": 5,
        "BackoffRate": 2.0
      }],
      "Next": "UpdateDatabase"
    },
    "UpdateDatabase": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:region:account:function:db-updater",
      "Next": "SuccessNotification"
    },
    "SuccessNotification": {
      "Type": "Task",
      "Resource": "arn:aws:states:::sns:publish",
      "Parameters": {
        "TopicArn": "arn:aws:sns:region:account:form-processing-success",
        "Message.$": "$.result"
      },
      "End": true
    },
    "ProcessingFailed": {
      "Type": "Task",
      "Resource": "arn:aws:states:::sns:publish",
      "Parameters": {
        "TopicArn": "arn:aws:sns:region:account:form-processing-failed",
        "Message.$": "$.error"
      },
      "End": true
    }
  }
}

# Lambda Function: Textract Processor
import boto3
import json

textract = boto3.client('textract')

def lambda_handler(event, context):
    bucket = event['s3Bucket']
    key = event['s3Key']
    
    # Start Textract analysis
    response = textract.analyze_document(
        Document={'S3Object': {'Bucket': bucket, 'Name': key}},
        FeatureTypes=['TABLES', 'FORMS']
    )
    
    # Extract key-value pairs from forms
    key_values = {}
    for block in response['Blocks']:
        if block['BlockType'] == 'KEY_VALUE_SET':
            if 'KEY' in block['EntityTypes']:
                key_text = get_text(block, response['Blocks'])
                value_block = get_value_block(block, response['Blocks'])
                value_text = get_text(value_block, response['Blocks'])
                key_values[key_text] = {
                    'value': value_text,
                    'confidence': block['Confidence']
                }
    
    # Extract tables
    tables = extract_tables(response['Blocks'])
    
    return {
        'statusCode': 200,
        'keyValues': key_values,
        'tables': tables,
        's3Bucket': bucket,
        's3Key': key
    }

# Lambda Function: Comprehend Processor
import boto3

comprehend = boto3.client('comprehend')

def lambda_handler(event, context):
    # Get extracted text from Textract results
    text_to_analyze = compile_text(event['keyValues'])
    
    # Extract entities (names, dates, organizations)
    entities_response = comprehend.detect_entities(
        Text=text_to_analyze,
        LanguageCode='en'
    )
    
    # Extract key phrases
    phrases_response = comprehend.detect_key_phrases(
        Text=text_to_analyze,
        LanguageCode='en'
    )
    
    return {
        **event,
        'entities': entities_response['Entities'],
        'keyPhrases': phrases_response['KeyPhrases']
    }

# Lambda Function: Data Parser
def lambda_handler(event, context):
    # Combine Textract and Comprehend results
    form_data = {}
    
    # Map extracted fields to target API format
    key_values = event['keyValues']
    entities = event['entities']
    
    # Example field mapping
    form_data['customer_name'] = find_value(key_values, 'Customer Name')
    form_data['order_date'] = find_entity(entities, 'DATE')
    form_data['amount'] = find_value(key_values, 'Total Amount')
    
    # Validate data
    if validate_required_fields(form_data):
        return {
            **event,
            'formData': form_data,
            'valid': True
        }
    else:
        raise Exception('Validation failed: Missing required fields')

# Lambda Function: API Submitter
import requests

def lambda_handler(event, context):
    form_data = event['formData']
    
    # Submit to external API
    response = requests.post(
        'https://external-system.example.com/api/forms',
        json=form_data,
        headers={'Authorization': 'Bearer ' + get_api_token()}
    )
    
    if response.status_code == 200:
        return {
            **event,
            'apiResponse': response.json(),
            'submitted': True
        }
    else:
        raise Exception(f'API submission failed: {response.status_code}')
</pre><br>

<b>Why Amazon Textract is Perfect for This:</b>
<pre>
<b>Textract Capabilities:</b>

1. <b>Form Extraction:</b>
   - Automatically identifies form fields
   - Extracts key-value pairs (Field: Value)
   - Example: "Customer Name: John Doe"
   - No template configuration needed!

2. <b>Table Detection:</b>
   - Recognizes tables in documents
   - Preserves row/column structure
   - Returns cell-by-cell data

3. <b>Text Detection:</b>
   - OCR for all text in document
   - Maintains spatial relationships
   - High accuracy even with poor scan quality

4. <b>Confidence Scores:</b>
   - Each extraction has confidence score (0-100%)
   - Can filter low-confidence results
   - Flag for manual review if needed

<b>Example Textract Output:</b>
{
  "DocumentMetadata": {...},
  "Blocks": [
    {
      "BlockType": "KEY_VALUE_SET",
      "EntityTypes": ["KEY"],
      "Confidence": 99.7,
      "Text": "Customer Name:",
      "Relationships": [...]
    },
    {
      "BlockType": "KEY_VALUE_SET",
      "EntityTypes": ["VALUE"],
      "Confidence": 98.5,
      "Text": "John Doe",
      "Relationships": [...]
    }
  ]
}

<b>No training, works immediately!</b>
</pre><br>

<b>Why Other Options Are Wrong:</b><br>

<b>Option A - Custom OCR Libraries + EKS + EC2:</b>
<ul>
<li>✗ <b>High development effort:</b> Build custom OCR from scratch</li>
<li>✗ <b>Long time to market:</b> Months to develop and train</li>
<li>✗ <b>High operational overhead:</b> Manage EKS cluster, EC2 instances</li>
<li>✗ <b>Lower accuracy:</b> Custom solution vs AWS pre-trained models</li>
<li>✗ <b>Ongoing maintenance:</b> Model updates, infrastructure patching</li>
</ul>

Development timeline:
<pre>
<b>Custom OCR Development:</b>
Month 1-2: Research and select OCR libraries (Tesseract, etc.)
Month 2-3: Develop image preprocessing pipelines
Month 3-4: Build form field detection logic
Month 4-5: Train and tune models on sample forms
Month 5-6: Integrate with application, test accuracy
Month 6+: Deploy to EKS, set up monitoring

<b>Time to market: 6+ months</b>

<b>Option D (Textract):</b>
Week 1: Design Step Functions workflow
Week 2: Implement Lambda functions
Week 3: Test and deploy

<b>Time to market: 3 weeks</b>

<b>Textract is 20x faster to market!</b>
</pre>

Operational overhead:
<pre>
<b>Custom OCR (Option A):</b>
- Manage EKS control plane
- Manage worker nodes (EC2)
- Kubernetes version upgrades
- Container image updates
- OCR library patching
- Model retraining
- Scaling configuration
- Cost: ~$500/month minimum

<b>Textract (Option D):</b>
- Fully managed (zero infrastructure)
- Automatic scaling
- No maintenance
- Cost: Pay per page processed (~$0.015/page)

<b>Minimal operational overhead!</b>
</pre><br>

<b>Option B - Step Functions + Lambda + Custom ML on EC2:</b>
<ul>
<li>✓ Step Functions + Lambda is good</li>
<li>✗ <b>Custom ML models on EC2:</b> Must train and manage</li>
<li>✗ <b>EC2 management:</b> Infrastructure overhead</li>
<li>✗ <b>Model training time:</b> Need labeled training data</li>
<li>✗ <b>Lower accuracy:</b> vs Textract's extensive training</li>
<li>✗ <b>Longer time to market</b></li>
</ul>

Model training requirements:
<pre>
<b>Custom ML Model (Option B):</b>
1. Collect training data:
   - Need 1000s of labeled form images
   - Manual labeling: weeks of effort
   
2. Train model:
   - Select architecture (CNN, etc.)
   - Train on GPU instances (expensive)
   - Tune hyperparameters
   - Time: 2-4 weeks

3. Host on EC2:
   - Deploy model server
   - Configure auto-scaling
   - Monitor performance

4. Maintain:
   - Retrain with new form types
   - Update models periodically

<b>Amazon Textract (Option D):</b>
- Pre-trained on millions of documents
- Works immediately
- Continuously improved by AWS
- No training data needed
- No model hosting

<b>Textract wins on accuracy AND speed!</b>
</pre><br>

<b>Option C - EC2 + SageMaker + ElastiCache:</b>
<ul>
<li>✗ <b>SageMaker for OCR:</b> Overkill, must train custom model</li>
<li>✗ <b>EC2 instances:</b> Infrastructure overhead</li>
<li>✗ <b>ElastiCache:</b> Unnecessary for this use case</li>
<li>✗ <b>High operational complexity</b></li>
<li>✗ <b>Longer development time</b></li>
</ul>

SageMaker issues:
<pre>
<b>SageMaker (Option C):</b>
- Designed for custom ML model training
- Need to bring your own model or algorithm
- Must collect and label training data
- Host model endpoints (cost: 24/7 instance)
- Appropriate when:
  * Custom business-specific models needed
  * No pre-built service available
  * Unique requirements

<b>Textract (Option D):</b>
- Purpose-built for document extraction
- Pre-trained, ready to use
- No training data required
- Pay per use (not 24/7 endpoint)
- Appropriate when:
  * Standard document processing
  * Forms, invoices, receipts
  * Fast time to market

<b>Why build custom when pre-built service exists?</b>
</pre>

ElastiCache confusion:
<pre>
<b>Why ElastiCache doesn't make sense:</b>

ElastiCache (Redis/Memcached):
- In-memory cache
- Volatile storage
- Good for: frequently accessed hot data

Form processing output:
- Needs persistent storage
- Infrequent re-access
- Better: S3 for long-term storage

<b>S3 is correct choice for document outputs!</b>
</pre><br>

<b>Comparison Table:</b>
<table border="1" cellpadding="5">
<tr><th>Aspect</th><th>Option A</th><th>Option B</th><th>Option C</th><th><b>Option D</b></th></tr>
<tr><td>OCR Solution</td><td>Custom libraries</td><td>Custom ML</td><td>SageMaker</td><td><b>Textract ✓</b></td></tr>
<tr><td>Compute</td><td>EKS + EC2</td><td>Lambda + EC2</td><td>EC2 + SageMaker</td><td><b>Lambda ✓</b></td></tr>
<tr><td>Orchestration</td><td>Custom</td><td><b>Step Functions ✓</b></td><td>Custom</td><td><b>Step Functions ✓</b></td></tr>
<tr><td>Time to Market</td><td>6+ months</td><td>3-4 months</td><td>4-5 months</td><td><b>3 weeks ✓</b></td></tr>
<tr><td>Operational Overhead</td><td>Very High</td><td>High</td><td>High</td><td><b>Very Low ✓</b></td></tr>
<tr><td>Accuracy</td><td>Medium</td><td>Medium</td><td>Medium-High</td><td><b>High ✓</b></td></tr>
<tr><td>Cost (monthly)</td><td>$500+</td><td>$300+</td><td>$400+</td><td><b>$50-200 ✓</b></td></tr>
</table><br>

<b>Cost Analysis (1000 forms/month):</b>
<pre>
<b>Option A - Custom OCR + EKS:</b>
- EKS cluster: $73/month
- EC2 worker nodes: 2 × t3.medium = $60/month
- Storage: $20/month
- Development cost amortized: High
- Total: ~$153/month + development

<b>Option B - Custom ML + EC2:</b>
- EC2 for ML hosting: $100/month
- Lambda: $10/month
- Step Functions: $2/month
- Development cost: High
- Total: ~$112/month + development

<b>Option C - SageMaker + EC2:</b>
- SageMaker endpoint: ml.m5.large = $140/month
- EC2 instances: $60/month
- ElastiCache: $15/month
- Total: ~$215/month + development

<b>Option D - Textract + Lambda + Step Functions:</b>
- Textract: 1000 pages × $0.015 = $15/month
- Lambda: ~$5/month
- Step Functions: $2/month
- S3 storage: $5/month
- Comprehend: $3/month
- Total: ~$30/month + minimal development

<b>Option D is 5-7x cheaper!</b>
Plus fastest development time.
</pre><br>

<b>Textract vs Comprehend Roles:</b>
<pre>
<b>Amazon Textract:</b>
Purpose: Extract text and structure from documents
Input: Image/PDF of scanned form
Output:
- Raw text
- Key-value pairs (Form Field: Value)
- Tables with cell data
- Bounding boxes, confidence scores

Example:
"Customer Name: John Doe" → 
  {key: "Customer Name", value: "John Doe", confidence: 99.8}

<b>Amazon Comprehend:</b>
Purpose: Understand meaning of extracted text
Input: Plain text (from Textract)
Output:
- Named entities (PERSON, ORGANIZATION, DATE, etc.)
- Key phrases
- Sentiment
- Language detection

Example:
"John Doe placed order on January 15th" → 
  {
    entities: [
      {text: "John Doe", type: "PERSON"},
      {text: "January 15th", type: "DATE"}
    ]
  }

<b>Together:</b>
Textract extracts → Comprehend understands → Complete automation!
</pre><br>

<b>Step Functions Benefits:</b>
<pre>
<b>Why Step Functions for Orchestration:</b>

1. <b>Visual Workflow:</b>
   - See entire process flow
   - Easy to understand and modify
   - Non-technical stakeholders can follow

2. <b>Built-in Error Handling:</b>
   - Automatic retries with exponential backoff
   - Catch specific error types
   - Route to error handling states
   - Dead letter queues

3. <b>State Management:</b>
   - Tracks processing state automatically
   - Can pause and resume
   - Handles long-running processes (up to 1 year!)

4. <b>Service Integrations:</b>
   - Direct integration with 200+ AWS services
   - No glue code needed
   - Example: Call Textract, SNS, DynamoDB directly

5. <b>Monitoring:</b>
   - CloudWatch integration
   - Execution history
   - Debug failed executions easily

6. <b>Cost-Effective:</b>
   - $0.025 per 1000 state transitions
   - 1000 forms = ~6 transitions each = $0.15/month
</pre><br>

<b>Real-World Workflow Example:</b>
<pre>
<b>Processing an Invoice Form:</b>

1. User uploads invoice.pdf to S3
   ↓
2. S3 event → EventBridge → Step Functions

3. Textract extracts:
   - Invoice Number: INV-12345
   - Date: 2024-01-15
   - Vendor: ACME Corp
   - Total: $1,234.56
   - Line items table

4. Comprehend enriches:
   - "ACME Corp" → ORGANIZATION entity
   - "2024-01-15" → DATE entity (normalized)

5. Lambda validates:
   - Invoice number format valid?
   - Date in acceptable range?
   - Amount > 0?
   - Vendor in approved list?

6. Lambda calls Accounting API:
   POST /api/invoices
   {
     "invoiceNumber": "INV-12345",
     "vendor": "ACME Corp",
     "date": "2024-01-15",
     "amount": 1234.56,
     "lineItems": [...]
   }

7. Update RDS:
   UPDATE forms 
   SET processing_status = 'completed',
       extracted_data = '...'
   WHERE form_id = 'abc123';

8. Send notification:
   SNS → "Invoice INV-12345 processed successfully"

<b>Elapsed time: 2-5 seconds (vs hours manually!)</b>
</pre><br>

<b>Accuracy and Validation:</b>
<pre>
<b>Confidence Score Handling:</b>

If Textract confidence < 95%:
  → Flag for manual review
  → Send to human validation queue
  → Team member verifies extraction
  
If confidence >= 95%:
  → Process automatically
  → Submit to API
  → Notify success

<b>Validation Rules:</b>
- Required fields present?
- Data types correct (date, number, etc.)?
- Values in expected ranges?
- Business rule compliance?

<b>Error Recovery:</b>
- API submission fails → Retry with backoff
- Persistent failure → Dead letter queue
- Admin notified → Manual intervention
- Can reprocess from S3 anytime
</pre><br>

<b>Scaling Characteristics:</b>
<pre>
<b>Automatic Scaling (Option D):</b>

10 forms/day:
- Lambda scales to 10 concurrent
- Textract scales automatically
- Cost: ~$5/month

1000 forms/day:
- Lambda scales to 1000 concurrent
- Textract scales automatically
- Cost: ~$450/month

10,000 forms/day:
- Lambda scales to 10,000 concurrent
- Textract scales automatically
- Cost: ~$4,500/month

<b>No infrastructure changes needed!</b>
Serverless auto-scales to demand.

<b>Custom Solutions (A, B, C):</b>
- Must provision for peak capacity
- Over-provision = waste money
- Under-provision = performance issues
- Manual scaling adjustments required
</pre><br>

<b>Best Practices:</b>
<ul>
<li>Use Textract for document OCR (don't build custom!)</li>
<li>Use Step Functions for multi-step workflows</li>
<li>Use Lambda for serverless compute</li>
<li>Store raw Textract output in S3 (reprocessing)</li>
<li>Implement confidence score thresholds</li>
<li>Set up human review queue for low-confidence extractions</li>
<li>Monitor execution metrics in CloudWatch</li>
<li>Use Dead Letter Queues for failed processes</li>
<li>Version your Step Functions state machines</li>
<li>Test with sample forms before full deployment</li>
</ul><br>

<b>Complete Solution:</b>
<ol>
<li>Extend application with AWS Step Functions state machine</li>
<li>Create Lambda functions for each processing step</li>
<li>Use Amazon Textract for OCR and form extraction</li>
<li>Use Amazon Comprehend for entity recognition</li>
<li>Store extracted results in S3</li>
<li>Validate and transform data in Lambda</li>
<li>Submit to target system API via Lambda</li>
<li>Update RDS with processing status</li>
<li>Send success/failure notifications via SNS</li>
<li>Deploy in 3 weeks, minimal operational overhead!</li>
</ol>

Serverless, accurate, fast to market, low overhead - perfect!
</div>
</div>

<!-- ================= Q5 ================= -->
<div class="question">
<pre>
95) A company is refactoring its on-premises order-processing platform in the AWS Cloud. The platform includes a web front end that is hosted on a fleet of VMs,
RabbitMQ to connect the front end to the backend, and a Kubernetes cluster to run a containerized backend system to process the orders. 
The company does not want to make any major changes to the application.

Which solution will meet these requirements with the LEAST operational overhead?
</pre>
<div class="options">
<label>
<input type="radio" name="q5">
A. Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up Amazon MQ to replace the on-premises messaging queue. Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend.
</label>

<label>
<input type="radio" name="q5">
B. Create a custom AWS Lambda runtime to mimic the web server environment. Create an Amazon API Gateway API to replace the front-end web servers. Set up Amazon MQ to replace the on-premises messaging queue. Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend.
</label>

<label>
<input type="radio" name="q5">
C. Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up Amazon MQ to replace the on-premises messaging queue. Install Kubernetes on a fleet of different EC2 instances to host the order-processing backend.
</label>

<label>
<input type="radio" name="q5">
D. Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up an Amazon Simple Queue Service (Amazon SQS) queue to replace the on-premises messaging queue. Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend.
</label>
</div>

<button onclick="checkAnswer(this,[0])">Check Answer</button>
<button onclick="showAnswer(this,[0])">Show Answer</button>

<div class="explanation">
<button class="close-explanation" onclick="closeExplanation(this)">✕ Close</button>
<b>Correct Answer: A</b><br><br>

This question tests <b>lift-and-shift migration with minimal changes using managed services</b>.<br><br>

<b>Requirements:</b>
<ul>
<li>Web front end (VMs) → AWS</li>
<li>RabbitMQ messaging → AWS</li>
<li>Kubernetes backend → AWS</li>
<li>Do NOT want major application changes</li>
<li>LEAST operational overhead</li>
</ul><br>

<b>Why Option A is Correct:</b><br>

<b>For Web Front End - AMI + Auto Scaling + ALB:</b>
<ul>
<li>✓ <b>Minimal changes:</b> Lift-and-shift existing VMs</li>
<li>✓ <b>Create AMI:</b> Snapshot existing VM configuration</li>
<li>✓ <b>Auto Scaling:</b> Automatic scaling, high availability</li>
<li>✓ <b>Application Load Balancer:</b> Distribute traffic</li>
<li>✓ <b>No code changes required</b></li>
</ul><br>

<b>For RabbitMQ - Amazon MQ:</b>
<ul>
<li>✓ <b>Managed RabbitMQ:</b> Drop-in replacement</li>
<li>✓ <b>100% compatible:</b> Supports RabbitMQ protocol</li>
<li>✓ <b>No application changes:</b> Same client libraries work</li>
<li>✓ <b>Automatic backups:</b> Managed patching, HA</li>
<li>✓ <b>Low operational overhead:</b> AWS manages infrastructure</li>
</ul><br>

<b>For Kubernetes - Amazon EKS:</b>
<ul>
<li>✓ <b>Managed Kubernetes:</b> Control plane fully managed</li>
<li>✓ <b>Compatible:</b> Standard Kubernetes APIs</li>
<li>✓ <b>Minimal changes:</b> Reuse existing container images and configs</li>
<li>✓ <b>Automatic updates:</b> AWS manages master nodes</li>
<li>✓ <b>Integrated:</b> Works with AWS services</li>
</ul><br>

<b>Architecture:</b>
<pre>
<b>AWS Cloud Migration:</b>

Internet
    ↓
Application Load Balancer
    ↓
┌─────────────────────────────────────┐
│   Auto Scaling Group (Web Tier)    │
│   ┌────────┐ ┌────────┐ ┌────────┐ │
│   │ EC2    │ │ EC2    │ │ EC2    │ │
│   │ Web    │ │ Web    │ │ Web    │ │
│   │ Server │ │ Server │ │ Server │ │
│   └────────┘ └────────┘ └────────┘ │
└──────────────┬──────────────────────┘
               ↓
┌──────────────────────────────────────┐
│      Amazon MQ (RabbitMQ)            │
│      - Active/Standby deployment     │
│      - Automatic failover            │
│      - Message persistence           │
└──────────────┬───────────────────────┘
               ↓
┌──────────────────────────────────────┐
│   Amazon EKS Cluster                 │
│   ┌────────────────────────────┐     │
│   │    Kubernetes Control      │     │
│   │    Plane (AWS Managed)     │     │
│   └────────────────────────────┘     │
│                                      │
│   ┌────────┐ ┌────────┐ ┌────────┐  │
│   │ Pod    │ │ Pod    │ │ Pod    │  │
│   │ Order  │ │ Order  │ │ Order  │  │
│   │ Proc   │ │ Proc   │ │ Proc   │  │
│   └────────┘ └────────┘ └────────┘  │
│   (Containerized Backend)            │
└──────────────────────────────────────┘

<b>All components use managed services!</b>
Minimal operational overhead.
</pre><br>

<b>Implementation:</b>
<pre>
# 1. Create AMI from existing web server VM
aws ec2 create-image \
  --instance-id i-1234567890abcdef \
  --name "web-server-v1" \
  --description "Order processing web front end"

# 2. Create Launch Template
aws ec2 create-launch-template \
  --launch-template-name web-server-template \
  --version-description "v1" \
  --launch-template-data '{
    "ImageId": "ami-abc123",
    "InstanceType": "t3.medium",
    "SecurityGroupIds": ["sg-123456"],
    "UserData": "..."
  }'

# 3. Create Auto Scaling Group
aws autoscaling create-auto-scaling-group \
  --auto-scaling-group-name web-tier-asg \
  --launch-template LaunchTemplateName=web-server-template \
  --min-size 2 \
  --max-size 10 \
  --desired-capacity 3 \
  --target-group-arns arn:aws:elasticloadbalancing:... \
  --vpc-zone-identifier "subnet-1,subnet-2,subnet-3"

# 4. Create Application Load Balancer
aws elbv2 create-load-balancer \
  --name web-tier-alb \
  --subnets subnet-1 subnet-2 subnet-3 \
  --security-groups sg-alb-123 \
  --scheme internet-facing

# 5. Create Amazon MQ Broker (RabbitMQ)
aws mq create-broker \
  --broker-name order-processing-mq \
  --engine-type RABBITMQ \
  --engine-version 3.11 \
  --deployment-mode ACTIVE_STANDBY_MULTI_AZ \
  --host-instance-type mq.m5.large \
  --publicly-accessible false \
  --subnet-ids subnet-private-1 subnet-private-2 \
  --security-groups sg-mq-123 \
  --users Username=admin,Password=SecurePass123

# 6. Create EKS Cluster
eksctl create cluster \
  --name order-processing-cluster \
  --region us-east-1 \
  --nodegroup-name backend-workers \
  --node-type t3.medium \
  --nodes 3 \
  --nodes-min 2 \
  --nodes-max 10 \
  --managed

# 7. Deploy existing Kubernetes manifests to EKS
kubectl apply -f order-backend-deployment.yaml

# 8. Update web tier environment variables
# Point to Amazon MQ endpoint instead of on-prem RabbitMQ
RABBITMQ_HOST=b-abc123.mq.us-east-1.amazonaws.com
RABBITMQ_PORT=5671
RABBITMQ_USER=admin
RABBITMQ_PASS=SecurePass123
</pre><br>

<b>Why Amazon MQ (Not SQS):</b>
<pre>
<b>Amazon MQ for RabbitMQ:</b>
- Protocol: AMQP 0-9-1 (RabbitMQ standard)
- Drop-in replacement for on-prem RabbitMQ
- No code changes needed!
- Supports:
  * Exchanges (direct, topic, fanout, headers)
  * Queues with routing keys
  * Message TTL, dead-letter queues
  * Publisher confirms
  * Consumer acknowledgments

<b>Application code stays the same:</b>
import pika  # RabbitMQ client library

# On-premises:
connection = pika.BlockingConnection(
    pika.ConnectionParameters('on-prem-rabbitmq-host'))

# AWS (only hostname changes!):
connection = pika.BlockingConnection(
    pika.ConnectionParameters('b-abc123.mq.us-east-1.amazonaws.com'))

# All other code identical!
channel = connection.channel()
channel.queue_declare(queue='orders')
channel.basic_publish(exchange='', routing_key='orders', body='Order data')

<b>Zero application changes required!</b>
</pre><br>

<b>Why Other Options Are Wrong:</b><br>

<b>Option B - Lambda + API Gateway + MQ + EKS:</b>
<ul>
<li>✓ Amazon MQ is correct</li>
<li>✓ EKS is correct</li>
<li>✗ <b>Lambda runtime for web tier:</b> MAJOR code changes required!</li>
<li>✗ <b>Cannot use existing VM code:</b> Must refactor to serverless</li>
<li>✗ <b>Violates "no major changes" requirement</b></li>
</ul>

Lambda migration complexity:
<pre>
<b>Converting VM web app to Lambda:</b>

Existing web server (Flask/Express/etc.):
- Stateful sessions
- Long-running connections
- File system access
- Background threads
- Scheduled jobs

Lambda constraints:
- Stateless (no sessions)
- 15-minute timeout
- Ephemeral file system (/tmp only)
- No background threads
- Cold starts

<b>Required changes:</b>
1. Refactor session management (use DynamoDB/ElastiCache)
2. Break long operations into smaller functions
3. Move file storage to S3
4. Rewrite background jobs (EventBridge/Step Functions)
5. Handle cold starts
6. Repackage dependencies

<b>Weeks/months of development work!</b>
Violates "no major changes" requirement.

<b>Option A (EC2):</b>
- Take AMI snapshot
- Deploy to Auto Scaling group
- Done! (days, not months)
</pre><br>

<b>Option C - AMI + MQ + Self-Managed Kubernetes on EC2:</b>
<ul>
<li>✓ AMI + Auto Scaling is correct</li>
<li>✓ Amazon MQ is correct</li>
<li>✗ <b>Self-managed Kubernetes:</b> HIGH operational overhead!</li>
<li>✗ <b>Manual cluster management:</b> Master nodes, etcd, upgrades</li>
<li>✗ <b>More complex than EKS</b></li>
</ul>

Self-managed K8s operational burden:
<pre>
<b>Self-Managed Kubernetes on EC2 (Option C):</b>

Setup:
- Provision master nodes (HA: 3+ instances)
- Install and configure etcd cluster
- Set up Kubernetes control plane
- Configure networking (CNI plugin)
- Set up worker nodes
- Configure authentication/RBAC
- Time: 1-2 weeks initial setup

Ongoing operations:
- Kubernetes version upgrades (quarterly)
- etcd backups and maintenance
- Certificate rotation
- Security patches for master nodes
- Monitoring control plane health
- Troubleshooting cluster issues
- Scaling master nodes
- Effort: 20-40 hours/month

<b>Amazon EKS (Option A):</b>

Setup:
- Run `eksctl create cluster`
- Time: 15-20 minutes

Ongoing operations:
- AWS manages control plane
- Automatic version upgrades available
- Built-in monitoring
- AWS handles etcd, certificates, patching
- Effort: 5-10 hours/month

<b>EKS saves 30+ hours/month!</b>
Much lower operational overhead.
</pre>

Cost comparison:
<pre>
<b>Self-Managed K8s:</b>
- Master nodes: 3 × t3.medium = $90/month
- Worker nodes: 5 × t3.medium = $150/month
- Admin time: 30 hours × $100/hr = $3,000/month value
- Total: $3,240/month

<b>Amazon EKS:</b>
- Control plane: $73/month (flat fee)
- Worker nodes: 5 × t3.medium = $150/month
- Admin time: 10 hours × $100/hr = $1,000/month value
- Total: $1,223/month

<b>EKS saves $2,017/month!</b>
</pre><br>

<b>Option D - AMI + SQS + EKS:</b>
<ul>
<li>✓ AMI + Auto Scaling is correct</li>
<li>✓ EKS is correct</li>
<li>✗ <b>Amazon SQS instead of MQ:</b> Requires code changes!</li>
<li>✗ <b>Different API:</b> Not compatible with RabbitMQ</li>
<li>✗ <b>Violates "no major changes" requirement</b></li>
</ul>

SQS vs RabbitMQ differences:
<pre>
<b>RabbitMQ (on-premises):</b>
Protocol: AMQP 0-9-1
Features:
- Exchanges (routing logic)
- Message routing by exchange type
- Topic-based routing
- Publisher confirms
- Complex routing patterns

<b>Amazon SQS:</b>
Protocol: AWS SDK (HTTP API)
Features:
- Simple queues only
- No exchanges or routing
- Different client library
- Different API calls

<b>Code changes required for SQS:</b>

RabbitMQ code:
import pika
channel.exchange_declare(exchange='orders', exchange_type='topic')
channel.basic_publish(exchange='orders', routing_key='order.created', body=msg)

SQS code (completely different!):
import boto3
sqs = boto3.client('sqs')
sqs.send_message(QueueUrl='https://sqs...', MessageBody=msg)

<b>Requires rewriting all messaging code!</b>

<b>Amazon MQ (Option A):</b>
Uses same RabbitMQ client library (pika)
Only change: Connection hostname
No code refactoring!

<b>Amazon MQ = zero code changes</b>
<b>SQS = rewrite messaging layer</b>
</pre><br>

<b>Comparison Table:</b>
<table border="1" cellpadding="5">
<tr><th>Component</th><th>Option A</th><th>Option B</th><th>Option C</th><th>Option D</th></tr>
<tr><td>Web Tier</td><td><b>AMI+ASG ✓</b></td><td>Lambda (major changes)</td><td><b>AMI+ASG ✓</b></td><td><b>AMI+ASG ✓</b></td></tr>
<tr><td>Messaging</td><td><b>Amazon MQ ✓</b></td><td><b>Amazon MQ ✓</b></td><td><b>Amazon MQ ✓</b></td><td>SQS (code changes)</td></tr>
<tr><td>Kubernetes</td><td><b>EKS ✓</b></td><td><b>EKS ✓</b></td><td>Self-managed</td><td><b>EKS ✓</b></td></tr>
<tr><td>Code Changes</td><td><b>Minimal ✓</b></td><td>Major refactoring</td><td>Minimal</td><td>Messaging rewrite</td></tr>
<tr><td>Operational Overhead</td><td><b>Low ✓</b></td><td>Low</td><td>High</td><td>Low</td></tr>
<tr><td>Meets Requirements</td><td><b>Yes ✓</b></td><td>No</td><td>No (overhead)</td><td>No (changes)</td></tr>
</table><br>

<b>Amazon MQ Details:</b>
<pre>
<b>Amazon MQ for RabbitMQ:</b>

Deployment modes:
1. <b>Single-instance:</b>
   - One broker in one AZ
   - Lower cost
   - For dev/test

2. <b>Active/Standby Multi-AZ:</b> (Recommended for production)
   - Primary broker in AZ-1
   - Standby broker in AZ-2
   - Automatic failover
   - High availability

Features:
- Automatic backups (point-in-time recovery)
- Automatic patching (maintenance windows)
- CloudWatch monitoring
- Encryption at rest and in transit
- VPC integration (private endpoints)

Configuration:
- Instance types: mq.t3.micro to mq.m5.4xlarge
- Storage: EBS volumes (auto-scaling)
- Network: Deploy in private subnets
- Access: IAM, username/password auth

Connection string example:
amqps://b-abc123-1.mq.us-east-1.amazonaws.com:5671
</pre><br>

<b>Migration Path Summary:</b>
<pre>
<b>Step-by-Step Migration:</b>

Week 1: Prepare
- Create AMI from web server VMs
- Export Kubernetes manifests
- Document RabbitMQ configuration

Week 2: Provision AWS Resources
- Create VPC, subnets, security groups
- Set up Auto Scaling group with AMIs
- Create Application Load Balancer
- Provision Amazon MQ broker
- Create EKS cluster

Week 3: Deploy & Test
- Deploy containers to EKS
- Update web tier config (MQ endpoint)
- Test end-to-end workflow
- Load testing

Week 4: Cutover
- DNS migration
- Monitor application
- Decommission on-prem

<b>Total: 4 weeks migration time</b>
Minimal code changes, all managed services!
</pre><br>

<b>EKS Benefits:</b>
<pre>
<b>Why Amazon EKS over Self-Managed:</b>

1. <b>Managed Control Plane:</b>
   - AWS runs Kubernetes masters
   - High availability across AZs
   - Automatic scaling
   - Security patching

2. <b>Easy Upgrades:</b>
   - Click to upgrade Kubernetes version
   - Rolling upgrades with zero downtime
   - AWS tests compatibility

3. <b>AWS Integration:</b>
   - IAM for authentication
   - VPC networking
   - CloudWatch logs/metrics
   - ALB Ingress Controller
   - EBS/EFS for persistent volumes

4. <b>Compliance:</b>
   - SOC, PCI DSS, HIPAA eligible
   - Regular security updates
   - Vulnerability scanning

5. <b>Cost:</b>
   - $73/month for control plane
   - Only pay for worker nodes
   - No master node costs

<b>Deploy existing Kubernetes workloads with minimal changes!</b>
</pre><br>

<b>Minimal Changes Validation:</b>
<pre>
<b>What changes are needed (Option A):</b>

Web Tier:
- Change RabbitMQ connection string (1 config line)
- Deploy AMI to AWS instead of on-prem hypervisor

Messaging:
- None! Amazon MQ is drop-in replacement

Kubernetes Backend:
- kubectl context change (point to EKS)
- Same deployment manifests
- Same container images
- Same kubectl commands

<b>Total code changes: ~5 lines of configuration</b>

<b>What stays the same:</b>
- Application logic (100%)
- Container images (100%)
- RabbitMQ client libraries (100%)
- Kubernetes manifests (95%+)
- Deployment process (similar)

<b>Truly minimal changes!</b>
</pre><br>

<b>Operational Overhead Comparison:</b>
<pre>
<b>Daily/Weekly Operations:</b>

Self-Managed (Option C):
- Check cluster health
- Monitor etcd
- Review logs
- Plan upgrades
- Patch OS and Kubernetes
- Certificate renewal
- Backup management
- Time: 2-3 hours/day

Managed Services (Option A):
- Review CloudWatch dashboards
- Monitor application metrics
- Occasional scaling adjustments
- Time: 30 minutes/day

<b>Monthly Operations:</b>

Self-Managed:
- Security patches
- Kubernetes version upgrade
- etcd maintenance
- Time: 20-30 hours/month

Managed Services:
- Review AWS service updates
- Optional EKS version upgrade (click button)
- Time: 5-8 hours/month

<b>Managed services save 75% operational time!</b>
</pre><br>

<b>Best Practices:</b>
<ul>
<li>Use managed services when available (lower overhead)</li>
<li>For messaging migration: Amazon MQ for RabbitMQ/ActiveMQ (protocol-compatible)</li>
<li>For Kubernetes: EKS over self-managed (unless specific needs)</li>
<li>For web tiers: Auto Scaling + ALB (proven, scalable)</li>
<li>Minimize code changes during migration (de-risk)</li>
<li>Test in AWS before cutover</li>
<li>Use Multi-AZ for production workloads</li>
<li>Enable CloudWatch monitoring</li>
<li>Plan for gradual traffic migration (Route 53 weighted routing)</li>
</ul><br>

<b>Complete Solution:</b>
<ol>
<li>Create AMI from existing web server VMs</li>
<li>Deploy web tier with Auto Scaling Group + Application Load Balancer</li>
<li>Replace RabbitMQ with Amazon MQ (managed, compatible)</li>
<li>Migrate Kubernetes workloads to Amazon EKS</li>
<li>Update configuration (RabbitMQ endpoint only)</li>
<li>All components use managed services = low operational overhead</li>
<li>Minimal code changes, fast migration!</li>
</ol>

Lift-and-shift with managed services - best of both worlds!
</div>
</div>

<!-- ================= Q6 ================= -->
<div class="question">
<pre>
97) A company has developed a web application. The company is hosting the application on a group of Amazon EC2 instances behind an Application Load Balancer. 
The company wants to improve the security posture of the application and plans to use AWS WAF web ACLs. 
The solution must not adversely affect legitimate traffic to the application.

How should a solutions architect configure the web ACLs to meet these requirements?
</pre>
<div class="options">
<label>
<input type="radio" name="q6">
A. Set the action of the web ACL rules to Count. Enable AWS WAF logging. Analyze the requests for false positives. Modify the rules to avoid any false positive. Over time, change the action of the web ACL rules from Count to Block.
</label>

<label>
<input type="radio" name="q6">
B. Use only rate-based rules in the web ACLs, and set the throttle limit as high as possible. Temporarily block all requests that exceed the limit. Define nested rules to narrow the scope of the rate tracking.
</label>

<label>
<input type="radio" name="q6">
C. Set the action of the web ACL rules to Block. Use only AWS managed rule groups in the web ACLs. Evaluate the rule groups by using Amazon CloudWatch metrics with AWS WAF sampled requests or AWS WAF logs.
</label>

<label>
<input type="radio" name="q6">
D. Use only custom rule groups in the web ACLs, and set the action to Allow. Enable AWS WAF logging. Analyze the requests for false positives. Modify the rules to avoid any false positive. Over time, change the action of the web ACL rules from Allow to Block.
</label>
</div>

<button onclick="checkAnswer(this,[0])">Check Answer</button>
<button onclick="showAnswer(this,[0])">Show Answer</button>

<div class="explanation">
<button class="close-explanation" onclick="closeExplanation(this)">✕ Close</button>
<b>Correct Answer: A</b><br><br>

This question tests <b>AWS WAF deployment best practices using Count mode to test before blocking</b>.<br><br>

<b>Requirements:</b>
<ul>
<li>Implement AWS WAF to improve security</li>
<li>Must NOT adversely affect legitimate traffic</li>
<li>Need to configure web ACLs properly</li>
</ul><br>

<b>Why Option A (Count Mode → Test → Block) is Correct:</b><br>

<b>AWS WAF Count Mode Strategy:</b>
<ul>
<li>✓ <b>Count mode:</b> Rules match but don't block (monitoring only)</li>
<li>✓ <b>Test without impact:</b> See what would be blocked without actually blocking</li>
<li>✓ <b>Identify false positives:</b> Legitimate traffic incorrectly flagged</li>
<li>✓ <b>Tune rules:</b> Adjust before enforcing</li>
<li>✓ <b>Gradual rollout:</b> Move to Block mode after validation</li>
<li>✓ <b>Safe deployment:</b> Zero risk to legitimate users</li>
</ul><br>

<b>Deployment Workflow:</b>
<pre>
<b>Phase 1: Deploy in Count Mode (Week 1-2)</b>

1. Create WAF web ACL with rules in Count mode
2. Associate with Application Load Balancer
3. Enable WAF logging to S3 or CloudWatch Logs
4. All requests pass through (nothing blocked)
5. WAF records what WOULD match rules

<b>Phase 2: Analysis (Week 2-3)</b>

6. Review WAF logs and metrics
7. Identify patterns of matched requests:
   - Are they attacks? (Good match!)
   - Are they legitimate? (False positive!)
8. Analyze CloudWatch metrics:
   - CountedRequests metric per rule
   - Compare with total requests
9. Review sampled requests in WAF console

<b>Phase 3: Tune Rules (Week 3-4)</b>

10. For false positives, modify rules:
    - Add exceptions
    - Adjust regex patterns
    - Exclude specific URIs
    - Refine IP sets
11. Re-deploy with updated rules (still Count mode)
12. Monitor for 1-2 weeks

<b>Phase 4: Enable Blocking (Week 5+)</b>

13. Change rule actions from Count to Block
14. Monitor closely for first 24-48 hours
15. Have rollback plan ready
16. Gradually enable blocking for each rule group

<b>Result: WAF protects app without blocking legitimate traffic!</b>
</pre><br>

<b>Implementation:</b>
<pre>
# 1. Create Web ACL with rules in Count mode
aws wafv2 create-web-acl \
  --name production-web-acl \
  --scope REGIONAL \
  --default-action Allow={} \
  --rules '[
    {
      "Name": "AWS-AWSManagedRulesCommonRuleSet",
      "Priority": 1,
      "OverrideAction": {"Count": {}},
      "Statement": {
        "ManagedRuleGroupStatement": {
          "VendorName": "AWS",
          "Name": "AWSManagedRulesCommonRuleSet"
        }
      },
      "VisibilityConfig": {
        "SampledRequestsEnabled": true,
        "CloudWatchMetricsEnabled": true,
        "MetricName": "CommonRuleSet"
      }
    },
    {
      "Name": "AWS-AWSManagedRulesKnownBadInputsRuleSet",
      "Priority": 2,
      "OverrideAction": {"Count": {}},
      "Statement": {
        "ManagedRuleGroupStatement": {
          "VendorName": "AWS",
          "Name": "AWSManagedRulesKnownBadInputsRuleSet"
        }
      },
      "VisibilityConfig": {
        "SampledRequestsEnabled": true,
        "CloudWatchMetricsEnabled": true,
        "MetricName": "KnownBadInputs"
      }
    }
  ]' \
  --visibility-config SampledRequestsEnabled=true,CloudWatchMetricsEnabled=true,MetricName=ProductionWebACL

# 2. Enable logging to S3
aws wafv2 put-logging-configuration \
  --logging-configuration '{
    "ResourceArn": "arn:aws:wafv2:region:account:regional/webacl/production-web-acl/...",
    "LogDestinationConfigs": ["arn:aws:s3:::aws-waf-logs-bucket"],
    "RedactedFields": []
  }'

# 3. Associate with ALB
aws wafv2 associate-web-acl \
  --web-acl-arn arn:aws:wafv2:region:account:regional/webacl/production-web-acl/... \
  --resource-arn arn:aws:elasticloadbalancing:region:account:loadbalancer/app/my-alb/...

# 4. Monitor CloudWatch metrics (after 1-2 weeks)
aws cloudwatch get-metric-statistics \
  --namespace AWS/WAFV2 \
  --metric-name CountedRequests \
  --dimensions Name=Rule,Value=AWS-AWSManagedRulesCommonRuleSet \
  --start-time 2024-01-01T00:00:00Z \
  --end-time 2024-01-15T00:00:00Z \
  --period 3600 \
  --statistics Sum

# 5. Query WAF logs (identify false positives)
# Using Athena on S3 logs
SELECT 
  action,
  httprequest.uri,
  httprequest.clientip,
  rulegrouplist,
  COUNT(*) as request_count
FROM waf_logs
WHERE terminatingruleid != 'Default_Action'
GROUP BY action, httprequest.uri, httprequest.clientip, rulegrouplist
ORDER BY request_count DESC
LIMIT 100;

# 6. Add exception for false positive (example)
# If /api/upload legitimately uploads files that trigger rule
{
  "Name": "AllowAPIUploads",
  "Priority": 0,
  "Action": {"Allow": {}},
  "Statement": {
    "AndStatement": {
      "Statements": [
        {
          "ByteMatchStatement": {
            "FieldToMatch": {"UriPath": {}},
            "PositionalConstraint": "STARTS_WITH",
            "SearchString": "/api/upload"
          }
        }
      ]
    }
  },
  "VisibilityConfig": {...}
}

# 7. Update Web ACL to change Count to Block (after testing)
aws wafv2 update-web-acl \
  --id abc123 \
  --scope REGIONAL \
  --default-action Allow={} \
  --rules '[
    {
      "Name": "AWS-AWSManagedRulesCommonRuleSet",
      "Priority": 1,
      "OverrideAction": {"None": {}},  # Changed from Count!
      "Statement": {...},
      "VisibilityConfig": {...}
    }
  ]'
</pre><br>

<b>False Positive Analysis Example:</b>
<pre>
<b>WAF Log Entry (Count Mode):</b>

{
  "timestamp": 1704124800000,
  "action": "COUNT",
  "terminatingRuleId": "AWS-AWSManagedRulesCommonRuleSet",
  "terminatingRuleMatchDetails": [
    {
      "conditionType": "XSS",
      "location": "BODY",
      "matchedData": ["&lt;script&gt;"]
    }
  ],
  "httpRequest": {
    "clientIp": "203.0.113.5",
    "uri": "/api/save-html-content",
    "method": "POST",
    "headers": [...]
  }
}

<b>Analysis:</b>
- Rule matched: XSS detection (found &lt;script&gt; tag)
- Endpoint: /api/save-html-content
- Use case: Content management system (users save HTML)
- Verdict: FALSE POSITIVE! (Legitimate feature)

<b>Solution:</b>
Add exception rule to allow HTML content for this specific endpoint:
- Priority: 0 (evaluated first)
- Match: UriPath = "/api/save-html-content"
- Action: Allow
- Bypasses XSS rule for this endpoint

<b>Without Count mode, this would have broken the application!</b>
</pre><br>

<b>Why Other Options Are Wrong:</b><br>

<b>Option B - Rate-Based Rules Only with High Throttle:</b>
<ul>
<li>✗ <b>"Only rate-based rules":</b> Incomplete protection</li>
<li>✗ <b>Misses attack types:</b> No protection against SQLi, XSS, etc.</li>
<li>✗ <b>High throttle limit:</b> Ineffective against slow attacks</li>
<li>✗ <b>Limited security coverage</b></li>
</ul>

Rate-based limitations:
<pre>
<b>Rate-Based Rules:</b>
Purpose: Prevent DDoS and brute force
Protection: Limits requests per 5-minute window per IP

Example: Block IP if >2000 requests in 5 minutes

<b>What rate-based rules DON'T protect against:</b>
✗ SQL injection attacks (1 malicious request is enough!)
✗ Cross-site scripting (XSS)
✗ Path traversal
✗ Remote code execution
✗ Known CVE exploits
✗ Malicious user agents/bots
✗ Distributed slow attacks (under rate limit)

<b>Need comprehensive rule set, not just rate limiting!</b>

AWS Managed Rule Groups cover:
- Core Rule Set (SQLi, XSS, LFI, RFI)
- Known Bad Inputs
- SQL Database protection
- Linux/Windows exploits
- PHP application protection
- And more...

<b>Rate-based rules are ONE component, not complete solution!</b>
</pre><br>

<b>Option C - Block Mode + Managed Rules (No Testing):</b>
<ul>
<li>✓ AWS managed rules are good</li>
<li>✗ <b>Start with Block mode:</b> High risk of false positives!</li>
<li>✗ <b>Can break legitimate traffic immediately</b></li>
<li>✗ <b>No testing phase</b></li>
<li>✗ <b>Reactive instead of proactive</b></li>
</ul>

Immediate blocking risks:
<pre>
<b>Deploying in Block mode without testing:</b>

Day 1: Enable WAF with Block action
    ↓
Minutes later:
- Customer support calls spike
- "Can't submit form!"
- "File upload failing!"
- "Search not working!"
    ↓
Investigation:
- WAF blocking legitimate requests
- XSS rule triggered by HTML content
- SQLi rule triggered by SQL-like search queries
- Common pattern in usernames/emails
    ↓
Emergency Response:
- Disable WAF entirely (no protection!)
- Or: Add exceptions reactively
- Or: Customers experience outage
    ↓
<b>Production incident caused by WAF deployment!</b>

<b>With Count mode first (Option A):</b>
- Week 1-2: Count mode (zero impact)
- Identify these issues in logs
- Add exceptions proactively
- Week 3+: Enable Block mode safely
- No customer impact!

<b>Testing prevents outages!</b>
</pre><br>

<b>Option D - Custom Rules with Allow Action:</b>
<ul>
<li>✗ <b>Custom rules only:</b> Missing AWS managed protections</li>
<li>✗ <b>Allow action:</b> Backwards logic (allows matched traffic)</li>
<li>✗ <b>High development effort:</b> Must create all rules from scratch</li>
<li>✗ <b>Lower quality:</b> vs AWS-maintained rules</li>
</ul>

Custom vs managed rules:
<pre>
<b>Custom Rules (Option D):</b>

Development effort:
- Research OWASP Top 10
- Write regex for SQLi detection
- Create XSS patterns
- Test against attack patterns
- Maintain as new attacks emerge
- Time: Weeks/months

Coverage:
- Limited to what you know
- May miss emerging threats
- Requires security expertise

<b>AWS Managed Rule Groups (Option A):</b>

Development effort:
- Select pre-built rule groups
- Enable with one click
- Time: Minutes

Coverage:
- OWASP Top 10
- Known CVEs
- Emerging threats (AWS updates automatically)
- Security team maintains rules
- Thousands of patterns

<b>Quality:</b>
AWS Managed Rules:
- Built by AWS security team
- Continuously updated
- Tested across millions of requests
- Industry best practices

Custom Rules:
- Only as good as your expertise
- Manual updates required
- Limited testing

<b>Use managed rules + custom exceptions (Option A approach)</b>
<b>Don't reinvent the wheel (Option D)</b>
</pre>

Allow action confusion:
<pre>
<b>Option D suggests "set action to Allow":</b>

This means:
- Create rules that ALLOW matching traffic
- Then change to BLOCK later

<b>Problems:</b>
1. Backwards logic (confusing)
2. Default action must be Block (block everything not allowed)
3. Creates whitelist instead of blacklist
4. Difficult to maintain
5. Easy to misconfigure

<b>Correct approach (Option A):</b>
- Rules identify threats (SQLi, XSS, etc.)
- Action starts as COUNT (monitor)
- Later changes to BLOCK (enforce)
- Default action: ALLOW (normal traffic passes)
- Standard security model
</pre><br>

<b>Comparison Table:</b>
<table border="1" cellpadding="5">
<tr><th>Aspect</th><th><b>Option A</b></th><th>Option B</th><th>Option C</th><th>Option D</th></tr>
<tr><td>Testing Phase</td><td><b>Count mode ✓</b></td><td>No testing</td><td>No testing</td><td>Complex</td></tr>
<tr><td>False Positive Prevention</td><td><b>Yes ✓</b></td><td>Partial</td><td>No</td><td>Partial</td></tr>
<tr><td>Rule Quality</td><td><b>AWS Managed ✓</b></td><td>Limited</td><td><b>AWS Managed ✓</b></td><td>Custom</td></tr>
<tr><td>Risk to Traffic</td><td><b>Zero ✓</b></td><td>Medium</td><td>High</td><td>High</td></tr>
<tr><td>Security Coverage</td><td><b>Comprehensive ✓</b></td><td>Rate limiting only</td><td>Comprehensive</td><td>Limited</td></tr>
<tr><td>Time to Deploy Safely</td><td><b>2-4 weeks ✓</b></td><td>Days</td><td>Hours (risky)</td><td>Months</td></tr>
</table><br>

<b>AWS WAF Logging and Analysis:</b>
<pre>
<b>Enable WAF Logging:</b>

Destinations:
1. <b>Amazon S3:</b>
   - Long-term storage
   - Query with Athena
   - Cost-effective archival

2. <b>CloudWatch Logs:</b>
   - Real-time analysis
   - CloudWatch Insights queries
   - Alarms and dashboards

3. <b>Amazon Kinesis Data Firehose:</b>
   - Stream to analytics
   - S3, Redshift, Elasticsearch
   - Real-time processing

<b>Log Format (JSON):</b>
{
  "timestamp": 1704124800000,
  "formatVersion": 1,
  "webaclId": "arn:aws:wafv2:...",
  "terminatingRuleId": "RuleName",
  "terminatingRuleType": "REGULAR",
  "action": "COUNT",
  "httpSourceName": "ALB",
  "httpSourceId": "alb-id",
  "ruleGroupList": [...],
  "httpRequest": {
    "clientIp": "1.2.3.4",
    "country": "US",
    "headers": [...],
    "uri": "/api/endpoint",
    "method": "POST"
  },
  "labels": [...]
}

<b>Analysis Queries:</b>

# Top blocked/counted URIs
SELECT 
  httprequest.uri,
  action,
  COUNT(*) as count
FROM waf_logs
WHERE action IN ('BLOCK', 'COUNT')
GROUP BY httprequest.uri, action
ORDER BY count DESC
LIMIT 20;

# False positive candidates (legitimate IPs being counted)
SELECT 
  httprequest.clientip,
  httprequest.uri,
  terminatingruleid,
  COUNT(*) as matches
FROM waf_logs
WHERE action = 'COUNT'
  AND httprequest.clientip IN ('known-good-ips')
GROUP BY httprequest.clientip, httprequest.uri, terminatingruleid;
</pre><br>

<b>CloudWatch Metrics:</b>
<pre>
<b>Key WAF Metrics:</b>

1. <b>AllowedRequests:</b>
   - Requests that passed all rules
   - Should be majority of traffic

2. <b>BlockedRequests:</b>
   - Requests blocked by rules
   - Monitor for sudden spikes

3. <b>CountedRequests:</b>
   - Requests matched in Count mode
   - Use during testing phase

4. <b>PassedRequests:</b>
   - Requests evaluated by rule but didn't match

<b>CloudWatch Dashboard:</b>
- Line graph: AllowedRequests vs BlockedRequests
- Pie chart: Requests by action (Allow/Block/Count)
- Bar chart: Counted requests by rule group
- Alarm: BlockedRequests > 100/minute (DDoS alert)

<b>Analysis:</b>
If CountedRequests high for specific rule:
  → Investigate in logs
  → If false positives → Add exception
  → If real attacks → Change to Block mode
</pre><br>

<b>Gradual Rollout Strategy:</b>
<pre>
<b>Best Practice: Enable Blocking Gradually</b>

Week 1-2: All rules in Count mode
    ↓
Week 3: Enable Block for Known Bad Inputs rule
  - Low false positive risk
  - Monitor for 3-4 days
    ↓
Week 4: Enable Block for SQL Database rules
  - Monitor for 3-4 days
    ↓
Week 5: Enable Block for Core Rule Set
  - Higher complexity
  - Monitor closely
    ↓
Week 6+: All rules in Block mode
  - Continuous monitoring
  - Refine exceptions as needed

<b>Benefits:</b>
- Isolate issues to specific rule groups
- Easier troubleshooting
- Lower risk
- Build confidence gradually
</pre><br>

<b>Exception Rules Best Practices:</b>
<pre>
<b>When to Add Exceptions:</b>

1. <b>Confirmed False Positive:</b>
   - Legitimate traffic being blocked
   - Business-critical endpoint
   - No security risk

2. <b>Specific Endpoint:</b>
   - Content management (allows HTML/JS)
   - API that accepts SQL queries
   - File upload (various content types)

<b>Exception Rule Structure:</b>
{
  "Name": "AllowCMSHTMLContent",
  "Priority": 0,  # Evaluated before blocking rules
  "Action": {"Allow": {}},
  "Statement": {
    "AndStatement": {
      "Statements": [
        {
          "ByteMatchStatement": {
            "FieldToMatch": {"UriPath": {}},
            "PositionalConstraint": "STARTS_WITH",
            "SearchString": "/cms/content"
          }
        },
        {
          "ByteMatchStatement": {
            "FieldToMatch": {"Method": {}},
            "PositionalConstraint": "EXACTLY",
            "SearchString": "POST"
          }
        }
      ]
    }
  }
}

<b>Narrow scope:</b>
- Specific URI path only
- Specific HTTP method
- Don't blanket-allow all XSS detection!
</pre><br>

<b>Best Practices:</b>
<ul>
<li>Always start with Count mode (test before enforcing)</li>
<li>Enable WAF logging (S3 or CloudWatch)</li>
<li>Analyze logs for false positives (1-2 weeks)</li>
<li>Use AWS Managed Rule Groups (don't reinvent)</li>
<li>Add targeted exceptions for false positives</li>
<li>Gradually enable Block mode per rule group</li>
<li>Monitor CloudWatch metrics continuously</li>
<li>Set up alarms for unusual blocking patterns</li>
<li>Review and refine rules monthly</li>
<li>Document exceptions and reasoning</li>
</ul><br>

<b>Complete Solution:</b>
<ol>
<li>Create WAF web ACL with AWS Managed Rule Groups</li>
<li>Set all rule actions to Count mode (monitoring only)</li>
<li>Associate WAF with Application Load Balancer</li>
<li>Enable WAF logging to S3 or CloudWatch Logs</li>
<li>Monitor for 1-2 weeks, analyze logs</li>
<li>Identify false positives (legitimate traffic being counted)</li>
<li>Add exception rules for confirmed false positives</li>
<li>Gradually change rule actions from Count to Block</li>
<li>Monitor closely during transition</li>
<li>Continuous refinement and tuning</li>
</ol>

Safe WAF deployment with zero impact to legitimate traffic!
</div>
</div>

</div> <!-- end container -->

<script>
function checkAnswer(btn, correct) {
  const q = btn.parentElement;
  const isMultiSelect = q.querySelector('input[type="checkbox"]') !== null;
  const inputs = q.querySelectorAll(isMultiSelect ? 'input[type="checkbox"]' : 'input[type="radio"]');
  const labels = q.querySelectorAll("label");
  const selected = [];

  inputs.forEach((inp, idx) => {
    if (inp.checked) selected.push(idx);
  });

  labels.forEach((label, idx) => {
    label.classList.remove("user-correct", "user-wrong", "correct");
    if (selected.includes(idx)) {
      if (correct.includes(idx)) {
        label.classList.add("user-correct");
      } else {
        label.classList.add("user-wrong");
      }
    }
  });

  let resultMsg = q.querySelector(".result-message");
  if (!resultMsg) {
    resultMsg = document.createElement("div");
    resultMsg.className = "result-message";
    q.appendChild(resultMsg);
  }

  const isCorrect = JSON.stringify(selected.sort()) === JSON.stringify(correct.sort());
  if (isCorrect) {
    resultMsg.textContent = "✔ Correct!";
    resultMsg.style.color = "#10b981";
    resultMsg.style.fontWeight = "600";
  } else {
    resultMsg.textContent = "✖ Incorrect. Try again or click 'Show Answer'.";
    resultMsg.style.color = "#ef4444";
    resultMsg.style.fontWeight = "600";
  }
  resultMsg.style.display = "block";
}

function showAnswer(btn, correct) {
  const q = btn.parentElement;
  const labels = q.querySelectorAll("label");
  
  labels.forEach(label => {
    label.classList.remove("user-correct", "user-wrong");
  });
  
  correct.forEach(i => labels[i].classList.add("correct"));
  const explanation = q.querySelector(".explanation");
  explanation.style.display = "block";
  
  const resultMsg = q.querySelector(".result-message");
  if (resultMsg) {
    resultMsg.style.display = "none";
  }
}

function closeExplanation(btn) {
  const explanation = btn.parentElement;
  explanation.style.display = "none";
}
</script>

</body>
</html>