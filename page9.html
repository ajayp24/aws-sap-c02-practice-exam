<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>AWS Solution Architect Practice Test – Page 9</title>
<link rel="stylesheet" href="style.css">
</head>

<body>
<div class="container">

<!-- ================= Navigation Top ================= -->
<div style="text-align:center; margin: 20px 0;">
  <a href="page8.html" style="
      display:inline-block;
      padding: 12px 28px;
      background:#6b7280;
      color:#fff;
      font-size:15px;
      font-weight:600;
      border-radius:8px;
      text-decoration:none;
      margin-right:10px;
  ">
    ← Previous Page
  </a>
  <a href="page10.html" style="
      display:inline-block;
      padding: 12px 28px;
      background:#6b7280;
      color:#fff;
      font-size:15px;
      font-weight:600;
      border-radius:8px;
      text-decoration:none;
  ">
    Next Page →
  </a>
</div>

<h1>AWS Solution Architect – Practice Test (Page 9)</h1>

<!-- ================= Q1 ================= -->
<div class="question">
<pre>
81) A company is building an electronic document management system in which users upload their documents. The application stack is entirely serverless and runs on AWS in the eu-central-1 Region. The system includes a web application that uses an Amazon CloudFront distribution for delivery with Amazon S3 as the origin. The web application communicates with Amazon API Gateway Regional endpoints. The API Gateway APIs call AWS Lambda functions that store metadata in an Amazon Aurora Serverless database and put the documents into an S3 bucket.
The company is growing steadily and has completed a proof of concept with its largest customer. The company must improve latency outside of Europe.

Which combination of actions will meet these requirements? (Choose two.)
</pre>
<div class="options">
<label>
<input type="checkbox" name="q1">
A. Enable S3 Transfer Acceleration on the S3 bucket. Ensure that the web application uses the Transfer Acceleration signed URLs.
</label>

<label>
<input type="checkbox" name="q1">
B. Create an accelerator in AWS Global Accelerator. Attach the accelerator to the CloudFront distribution.
</label>

<label>
<input type="checkbox" name="q1">
C. Change the API Gateway Regional endpoints to edge-optimized endpoints.
</label>

<label>
<input type="checkbox" name="q1">
D. Provision the entire stack in two other locations that are spread across the world. Use global databases on the Aurora Serverless cluster.
</label>

<label>
<input type="checkbox" name="q1">
E. Add an Amazon RDS proxy between the Lambda functions and the Aurora Serverless database.
</label>
</div>

<button onclick="checkAnswer(this,[0,2])">Check Answer</button>
<button onclick="showAnswer(this,[0,2])">Show Answer</button>

<div class="explanation">
<button class="close-explanation" onclick="closeExplanation(this)">✕ Close</button>
<b>Correct Answer: A, C</b><br><br>

This question tests <b>optimizing global latency for serverless applications</b>.<br><br>

<b>Current Architecture:</b>
<ul>
<li>CloudFront → S3 (web application delivery) ✓</li>
<li>API Gateway Regional endpoints (eu-central-1)</li>
<li>Lambda → Aurora Serverless (metadata)</li>
<li>Document uploads → S3 bucket</li>
<li><b>Problem:</b> High latency for users outside Europe</li>
</ul><br>

<b>Why Options A + C Are Correct:</b><br>

<b>Option A - S3 Transfer Acceleration:</b>
<ul>
<li>✓ <b>Faster Uploads:</b> Uses CloudFront edge locations for uploads</li>
<li>✓ <b>Global Coverage:</b> 400+ edge locations worldwide</li>
<li>✓ <b>AWS Backbone:</b> Routes via optimized AWS network</li>
<li>✓ <b>Simple Implementation:</b> Just enable and change URL</li>
<li>✓ <b>Significant Improvement:</b> 50-500% faster for distant regions</li>
</ul><br>

<b>Transfer Acceleration Architecture:</b>
<pre>
User in Asia/Americas
    ↓
    ↓ Upload document
    ↓
CloudFront Edge Location (nearest)
    ↓
    ↓ AWS Global Network (optimized)
    ↓
S3 Bucket (eu-central-1)

Without Transfer Acceleration:
User → Public Internet → eu-central-1 (slow)

With Transfer Acceleration:
User → Nearest Edge → AWS Backbone → eu-central-1 (fast)
</pre><br>

<b>Implementation:</b>
<pre>
# Enable S3 Transfer Acceleration
aws s3api put-bucket-accelerate-configuration \
  --bucket my-documents-bucket \
  --accelerate-configuration Status=Enabled

# Update upload URLs
Standard URL:
https://my-documents-bucket.s3.eu-central-1.amazonaws.com/document.pdf

Transfer Acceleration URL:
https://my-documents-bucket.s3-accelerate.amazonaws.com/document.pdf

# Generate pre-signed URL with acceleration
import boto3
s3 = boto3.client('s3', config=Config(s3={'use_accelerate_endpoint': True}))
url = s3.generate_presigned_url(
    'put_object',
    Params={'Bucket': 'my-documents-bucket', 'Key': 'document.pdf'},
    ExpiresIn=3600
)
</pre><br>

<b>Option C - Edge-Optimized API Gateway:</b>
<ul>
<li>✓ <b>CloudFront Integration:</b> API Gateway creates CloudFront distribution</li>
<li>✓ <b>Global Edge Locations:</b> API requests routed to nearest edge</li>
<li>✓ <b>Reduced Latency:</b> Edge locations cache responses</li>
<li>✓ <b>Easy Change:</b> Just update endpoint type</li>
<li>✓ <b>AWS-Managed:</b> No additional infrastructure</li>
</ul><br>

<b>Edge-Optimized API Gateway:</b>
<pre>
Regional Endpoint (Current):
User in Asia → Public Internet → eu-central-1 API Gateway
Latency: 200-400ms

Edge-Optimized Endpoint:
User in Asia → Asia Edge Location → eu-central-1 API Gateway
Latency: 50-150ms

<b>How it works:</b>
1. API Gateway creates CloudFront distribution
2. User requests go to nearest edge location
3. Edge location forwards to regional API Gateway
4. Response can be cached at edge (if configured)
5. Subsequent requests served from cache
</pre><br>

<b>Converting to Edge-Optimized:</b>
<pre>
# Update API Gateway endpoint type
aws apigateway update-rest-api \
  --rest-api-id abc123 \
  --patch-operations \
    op=replace,path=/endpointConfiguration/types/REGIONAL,value=EDGE

# Deploy changes
aws apigateway create-deployment \
  --rest-api-id abc123 \
  --stage-name prod

# New endpoint URL
Old: https://abc123.execute-api.eu-central-1.amazonaws.com/prod
New: https://xyz789.cloudfront.net/prod
</pre><br>

<b>Complete Solution Architecture:</b>
<pre>
<b>Global Users</b>
    ↓
    ↓ Static content (HTML/JS/CSS)
    ↓
CloudFront Distribution
    └─ Origin: S3 (web app)
    
    ↓ API requests
    ↓
Edge-Optimized API Gateway (via CloudFront)
    └─ Regional API in eu-central-1
    
    ↓ Process requests
    ↓
Lambda Functions (eu-central-1)
    ├─ Store metadata → Aurora Serverless
    └─ Upload docs → S3 (via Transfer Acceleration)
    
    ↓ Document uploads
    ↓
S3 Transfer Acceleration
    └─ Nearest edge → AWS network → S3 bucket

<b>Latency Improvements:</b>
- Static content: Already optimized (CloudFront)
- API calls: Optimized with edge-optimized endpoints
- Document uploads: Optimized with Transfer Acceleration
</pre><br>

<b>Why Other Options Are Wrong:</b><br>

<b>Option B - Global Accelerator + CloudFront:</b>
<ul>
<li>✗ <b>Cannot attach to CloudFront:</b> Global Accelerator works with ALB, NLB, EC2, EIP</li>
<li>✗ <b>CloudFront already global:</b> Already uses edge locations</li>
<li>✗ <b>Redundant:</b> CloudFront provides same benefit</li>
<li>✗ <b>Invalid configuration:</b> These services don't integrate this way</li>
</ul>

Clarification:
<pre>
Global Accelerator: For non-HTTP workloads (TCP/UDP)
- Use cases: Gaming, VoIP, static IP requirements
- Targets: ALB, NLB, EC2, Elastic IP

CloudFront: For HTTP/HTTPS content delivery
- Use cases: Websites, APIs, streaming
- Origins: S3, ALB, EC2, custom origins

Cannot combine: Global Accelerator → CloudFront ✗
</pre><br>

<b>Option D - Multi-Region Deployment:</b>
<ul>
<li>✓ Would reduce latency (data closer to users)</li>
<li>✗ <b>Massive complexity:</b> Deploy entire stack in 3 regions</li>
<li>✗ <b>Aurora Global Database:</b> Complex setup and management</li>
<li>✗ <b>High cost:</b> Triple infrastructure costs</li>
<li>✗ <b>Operational overhead:</b> Manage multiple regions</li>
<li>✗ <b>Overkill:</b> Edge-optimized endpoints achieve similar latency reduction</li>
<li>✗ <b>Not required:</b> Current solution can stay in eu-central-1</li>
</ul>

Cost comparison:
<pre>
Options A + C:
- Transfer Acceleration: $0.04/GB
- Edge-optimized API: No extra cost (included)
- Total additional: Minimal

Option D:
- Lambda in 3 regions: 3x cost
- Aurora in 3 regions: 3x cost
- S3 in 3 regions: 3x storage cost
- Cross-region replication: Data transfer costs
- Total: 200-300% cost increase!
</pre><br>

<b>Option E - RDS Proxy:</b>
<ul>
<li>✗ <b>Doesn't improve latency:</b> Only helps connection management</li>
<li>✗ <b>Wrong problem:</b> Issue is geographic latency, not database connections</li>
<li>✗ <b>Aurora Serverless already scales:</b> Auto-manages connections</li>
<li>✗ <b>Adds cost:</b> $0.015/hour per vCPU without benefit</li>
<li>✗ <b>No global benefit:</b> Database still in eu-central-1</li>
</ul><br>

<b>Performance Comparison:</b>
<table border="1" cellpadding="5">
<tr><th>Component</th><th>Before</th><th>After (A+C)</th><th>Improvement</th></tr>
<tr><td>Static Content</td><td>Fast (CloudFront)</td><td>Fast (same)</td><td>0%</td></tr>
<tr><td>API Calls</td><td>200-400ms</td><td>50-150ms</td><td>60-75%</td></tr>
<tr><td>Document Uploads</td><td>Slow (long distance)</td><td>Fast (edge)</td><td>50-500%</td></tr>
<tr><td>Infrastructure Cost</td><td>Baseline</td><td>+5-10%</td><td>Minimal</td></tr>
</table><br>

<b>Latency Breakdown:</b>
<pre>
<b>User in Sydney uploading 100 MB document</b>

Without Transfer Acceleration:
Sydney → Public Internet → Frankfurt
- Latency: ~300ms RTT
- Bandwidth: Variable, packet loss
- Upload time: 5-10 minutes
- Failures: 10-20% failure rate

With Transfer Acceleration:
Sydney → Sydney Edge → AWS Backbone → Frankfurt
- Latency: ~50ms to edge, optimized to Frankfurt
- Bandwidth: AWS network (consistent)
- Upload time: 2-3 minutes
- Failures: <2% failure rate

Improvement: 60-70% faster, more reliable
</pre><br>

<b>API Gateway Endpoint Types:</b>
<table border="1" cellpadding="5">
<tr><th>Type</th><th>Use Case</th><th>Latency</th><th>Cost</th></tr>
<tr><td>Regional</td><td>Same-region clients</td><td>Low (local)</td><td>Standard</td></tr>
<tr><td><b>Edge-Optimized</b></td><td><b>Global clients</b></td><td><b>Low (global)</b></td><td><b>Standard</b></td></tr>
<tr><td>Private</td><td>VPC-only access</td><td>Lowest (VPC)</td><td>Standard</td></tr>
</table><br>

<b>Caching at Edge:</b>
<pre>
# Enable caching on API Gateway edge
{
  "GET /documents": {
    "cacheEnabled": true,
    "cacheTtlInSeconds": 300,
    "cacheKeyParameters": ["id"]
  }
}

Benefits:
- Repeated requests served from edge cache
- No round-trip to eu-central-1
- Sub-10ms response times
- Reduced Lambda invocations (cost savings)
</pre><br>

<b>Transfer Acceleration Use Cases:</b>
<ul>
<li>Large file uploads from distant locations</li>
<li>Users distributed globally</li>
<li>Variable internet quality</li>
<li>Time-sensitive uploads</li>
<li>High-value data (worth the small extra cost)</li>
</ul><br>

<b>Testing Transfer Acceleration:</b>
<pre>
# AWS provides speed comparison tool
http://s3-accelerate-speedtest.s3-accelerate.amazonaws.com/en/accelerate-speed-comparsion.html

# Shows:
- Standard endpoint speed
- Accelerated endpoint speed
- Improvement percentage for your location
</pre><br>

<b>Best Practices:</b>
<ul>
<li>Use CloudFront for static content delivery (already doing)</li>
<li>Use edge-optimized API Gateway for APIs (Option C)</li>
<li>Use S3 Transfer Acceleration for uploads (Option A)</li>
<li>Enable API Gateway caching for cacheable responses</li>
<li>Monitor CloudWatch metrics for latency</li>
<li>Consider Lambda@Edge for custom logic at edge</li>
<li>Keep backend services in single region (simpler)</li>
</ul><br>

<b>Complete Implementation:</b>
<pre>
# 1. Enable S3 Transfer Acceleration
aws s3api put-bucket-accelerate-configuration \
  --bucket docs-bucket \
  --accelerate-configuration Status=Enabled

# 2. Convert API Gateway to edge-optimized
aws apigateway update-rest-api \
  --rest-api-id abc123 \
  --patch-operations op=replace,path=/endpointConfiguration/types/REGIONAL,value=EDGE

# 3. Update client code
// Old
const uploadUrl = 'https://docs-bucket.s3.eu-central-1.amazonaws.com';
const apiUrl = 'https://api123.execute-api.eu-central-1.amazonaws.com';

// New
const uploadUrl = 'https://docs-bucket.s3-accelerate.amazonaws.com';
const apiUrl = 'https://cloudfront-id.cloudfront.net'; // Auto-generated

# 4. Deploy and test
# 5. Monitor improvements in CloudWatch
</pre><br>

<b>Monitoring:</b>
<ul>
<li>CloudWatch Metrics: API Gateway latency by region</li>
<li>S3 Transfer Acceleration metrics: Speed improvements</li>
<li>CloudFront metrics: Cache hit ratio, viewer requests</li>
<li>X-Ray: End-to-end request tracing</li>
</ul>
</div>
</div>

<!-- ================= Q2 ================= -->
<div class="question">
<pre>
82) An adventure company has launched a new feature on its mobile app. Users can use the feature to upload their hiking and rafting photos and videos anytime. The photos and videos are stored in Amazon S3 Standard storage in an S3 bucket and are served through Amazon CloudFront.

The company needs to optimize the cost of the storage. A solutions architect discovers that most of the uploaded photos and videos are accessed infrequently after 30 days. However, some of the uploaded photos and videos are accessed frequently after 30 days. The solutions architect needs to implement a solution that maintains millisecond retrieval availability of the photos and videos at the lowest possible cost.

Which solution will meet these requirements?
</pre>
<div class="options">
<label>
<input type="radio" name="q2">
A. Configure S3 Intelligent-Tiering on the S3 bucket.
</label>

<label>
<input type="radio" name="q2">
B. Configure an S3 Lifecycle policy to transition image objects and video objects from S3 Standard to S3 Glacier Deep Archive after 30 days.
</label>

<label>
<input type="radio" name="q2">
C. Replace Amazon S3 with an Amazon Elastic File System (Amazon EFS) file system that is mounted on Amazon EC2 instances.
</label>

<label>
<input type="radio" name="q2">
D. Add a Cache-Control: max-age header to the S3 image objects and S3 video objects. Set the header to 30 days.
</label>
</div>

<button onclick="checkAnswer(this,[0])">Check Answer</button>
<button onclick="showAnswer(this,[0])">Show Answer</button>

<div class="explanation">
<button class="close-explanation" onclick="closeExplanation(this)">✕ Close</button>
<b>Correct Answer: A</b><br><br>

This question tests <b>S3 storage cost optimization with unpredictable access patterns</b>.<br><br>

<b>Requirements:</b>
<ul>
<li>Most content accessed infrequently after 30 days</li>
<li><b>Some content accessed frequently after 30 days</b> (unpredictable)</li>
<li>Must maintain millisecond retrieval</li>
<li>Lowest possible cost</li>
<li>Cannot predict which objects will be accessed</li>
</ul><br>

<b>Why Option A (S3 Intelligent-Tiering) is Correct:</b><br>

<b>S3 Intelligent-Tiering Features:</b>
<ul>
<li>✓ <b>Automatic Tiering:</b> Moves objects between tiers based on access patterns</li>
<li>✓ <b>No Retrieval Fees:</b> Unlike Glacier</li>
<li>✓ <b>Millisecond Access:</b> All tiers provide low latency</li>
<li>✓ <b>No Lifecycle Policies Needed:</b> Fully automatic</li>
<li>✓ <b>Perfect for Unpredictable Access:</b> Adapts to actual usage</li>
<li>✓ <b>Cost Optimization:</b> 40-60% savings vs Standard</li>
</ul><br>

<b>S3 Intelligent-Tiering Architecture:</b>
<pre>
Object Uploaded
    ↓
Frequent Access Tier (30 days)
    - Same as S3 Standard performance
    - Cost: $0.023/GB/month
    ↓
    ↓ No access for 30 days
    ↓
Infrequent Access Tier
    - Millisecond access maintained
    - Cost: $0.0125/GB/month (46% cheaper)
    ↓
    ↓ No access for 90 days (optional)
    ↓
Archive Instant Access Tier
    - Millisecond access maintained
    - Cost: $0.004/GB/month (83% cheaper)
    ↓
    ↓ No access for 180 days (optional)
    ↓
Archive Access Tier
    - Minutes to hours retrieval
    - Cost: $0.00099/GB/month

<b>If accessed again: Automatically moves back to Frequent Access tier!</b>
</pre><br>

<b>How It Works:</b>
<ol>
<li>Object uploaded to Frequent Access tier</li>
<li>S3 monitors access patterns automatically</li>
<li>After 30 consecutive days without access → Move to Infrequent Access</li>
<li>If accessed → Move back to Frequent Access immediately</li>
<li>No retrieval delay, no retrieval fees</li>
<li>Continuous monitoring and optimization</li>
</ol><br>

<b>Configuration:</b>
<pre>
# Enable Intelligent-Tiering on bucket
aws s3api put-bucket-intelligent-tiering-configuration \
  --bucket adventure-photos \
  --id default-config \
  --intelligent-tiering-configuration '{
    "Id": "default-config",
    "Status": "Enabled",
    "Tierings": [
      {
        "Days": 90,
        "AccessTier": "ARCHIVE_ACCESS"
      },
      {
        "Days": 180,
        "AccessTier": "DEEP_ARCHIVE_ACCESS"
      }
    ]
  }'

# Or set default storage class for new uploads
aws s3api put-bucket \
  --bucket adventure-photos \
  --object-ownership BucketOwnerEnforced \
  --x-amz-storage-class INTELLIGENT_TIERING
</pre><br>

<b>Cost Example (1 TB of data):</b>
<pre>
<b>Scenario: 70% infrequently accessed after 30 days, 30% still frequently accessed</b>

S3 Standard (current):
- 1,000 GB × $0.023/GB = $23/month
- All data in Standard tier

S3 Intelligent-Tiering:
- Frequent (300 GB): 300 × $0.023 = $6.90
- Infrequent (700 GB): 700 × $0.0125 = $8.75
- Monitoring: 1,000 GB × $0.0025 = $2.50
- Total: $18.15/month

<b>Savings: $4.85/month (21% reduction)</b>
For larger datasets or longer retention: Even bigger savings!
</pre><br>

<b>Why It's Perfect for This Use Case:</b>
<ul>
<li><b>Unpredictable access:</b> Can't know which videos will trend</li>
<li><b>Some viral content:</b> Old videos might get accessed frequently</li>
<li><b>No intervention needed:</b> Automatic optimization</li>
<li><b>No user impact:</b> Always millisecond access</li>
<li><b>Cost-effective:</b> Pay less for infrequent, same for frequent</li>
</ul><br>

<b>Why Other Options Are Wrong:</b><br>

<b>Option B - Lifecycle to Glacier Deep Archive:</b>
<ul>
<li>✗ <b>Retrieval time:</b> 12-48 hours (not millisecond!)</li>
<li>✗ <b>Retrieval cost:</b> $0.02/GB to retrieve</li>
<li>✗ <b>Inflexible:</b> ALL objects moved, even frequently accessed ones</li>
<li>✗ <b>User impact:</b> Videos unavailable for hours</li>
<li>✗ <b>Violates requirement:</b> "millisecond retrieval availability"</li>
</ul>

Problem scenario:
<pre>
User uploads video in January
30 days pass → Moved to Glacier Deep Archive
March: Video goes viral, millions want to watch
Problem: Video not available for 12+ hours!
Users see errors, bad experience
Cost to restore: $0.02/GB × size × millions of views

This violates the requirement!
</pre><br>

<b>Option C - Replace S3 with EFS:</b>
<ul>
<li>✗ <b>Much more expensive:</b> EFS $0.30/GB vs S3 $0.023/GB (13x more!)</li>
<li>✗ <b>Requires EC2:</b> Need instances to mount EFS (more cost)</li>
<li>✗ <b>Not designed for this:</b> EFS for file system access, not object storage</li>
<li>✗ <b>No CDN integration:</b> Can't directly integrate with CloudFront</li>
<li>✗ <b>Operational overhead:</b> Manage EC2, mounting, scaling</li>
<li>✗ <b>Wrong tool for the job</b></li>
</ul>

Cost comparison:
<pre>
1 TB storage for 1 month:

S3 Standard: $23
S3 Intelligent-Tiering: ~$18
EFS Standard: $300 (13x more!)
+ EC2 instances: $50-500/month
Total EFS solution: $350-800/month!
</pre><br>

<b>Option D - Cache-Control Headers:</b>
<ul>
<li>✗ <b>Doesn't reduce storage cost:</b> Still paying S3 Standard prices</li>
<li>✗ <b>Only affects CloudFront caching:</b> Not S3 storage</li>
<li>✗ <b>Wrong solution:</b> Headers control CDN, not storage tier</li>
<li>✗ <b>Doesn't address the problem:</b> Question asks about storage optimization</li>
</ul>

What Cache-Control actually does:
<pre>
Cache-Control: max-age=2592000 (30 days)

Effect:
- CloudFront caches object for 30 days
- Reduces S3 GET requests (small cost saving)
- Users get cached version (faster, but same with or without header)

Does NOT:
- Change S3 storage tier
- Reduce S3 storage costs
- Optimize based on access patterns
</pre><br>

<b>S3 Storage Classes Comparison:</b>
<table border="1" cellpadding="5">
<tr><th>Class</th><th>Use Case</th><th>Retrieval</th><th>Cost/GB</th></tr>
<tr><td>Standard</td><td>Frequent access</td><td>ms</td><td>$0.023</td></tr>
<tr><td><b>Intelligent-Tiering</b></td><td><b>Unpredictable</b></td><td><b>ms</b></td><td><b>$0.0125-0.023</b></td></tr>
<tr><td>Standard-IA</td><td>Infrequent (known)</td><td>ms</td><td>$0.0125</td></tr>
<tr><td>One Zone-IA</td><td>Infrequent, non-critical</td><td>ms</td><td>$0.01</td></tr>
<tr><td>Glacier Instant</td><td>Archive, instant access</td><td>ms</td><td>$0.004</td></tr>
<tr><td>Glacier Flexible</td><td>Archive</td><td>min-hrs</td><td>$0.0036</td></tr>
<tr><td>Glacier Deep</td><td>Long-term archive</td><td>12-48 hrs</td><td>$0.00099</td></tr>
</table><br>

<b>When to Use Each Storage Class:</b>
<pre>
<b>S3 Standard:</b>
- Frequently accessed data
- Known access patterns
- Performance-critical

<b>S3 Intelligent-Tiering:</b> ← THIS QUESTION
- Unpredictable access patterns
- Mix of frequent and infrequent access
- Don't want to manage lifecycle policies
- Need millisecond access always

<b>S3 Standard-IA:</b>
- Infrequently accessed (known pattern)
- OK with $0.01/GB retrieval fee
- Backups, disaster recovery

<b>Glacier Classes:</b>
- Long-term archival
- OK with retrieval delays
- Compliance, regulatory retention
</pre><br>

<b>Intelligent-Tiering Monitoring Cost:</b>
<pre>
Monitoring fee: $0.0025 per 1,000 objects

Example:
100,000 photos/videos
Monitoring: 100,000 / 1,000 × $0.0025 = $0.25/month

Usually negligible compared to storage savings!
</pre><br>

<b>Lifecycle Policy vs Intelligent-Tiering:</b>
<table border="1" cellpadding="5">
<tr><th>Feature</th><th>Lifecycle Policy</th><th>Intelligent-Tiering</th></tr>
<tr><td>Access Pattern</td><td>Predictable</td><td>Unpredictable</td></tr>
<tr><td>Management</td><td>Manual rules</td><td>Automatic</td></tr>
<tr><td>Flexibility</td><td>Fixed schedule</td><td>Access-based</td></tr>
<tr><td>Retrieval</td><td>Depends on tier</td><td>Always millisecond</td></tr>
<tr><td>Best For</td><td>Known patterns</td><td>Unknown patterns</td></tr>
</table><br>

<b>Real-World Scenario:</b>
<pre>
Adventure Company uploads:
- 1,000 photos/videos per day
- 70% become inactive after 30 days
- 30% remain active (user favorites, popular content)
- Cannot predict which will be popular

Perfect for Intelligent-Tiering because:
✓ Don't know which content will be accessed
✓ Some old content might suddenly become popular
✓ Need instant access always (user experience)
✓ Want automatic cost optimization
</pre><br>

<b>Integration with CloudFront:</b>
<pre>
Users → CloudFront → S3 Intelligent-Tiering

Benefits:
1. CloudFront caches popular content (reduced S3 requests)
2. S3 Intelligent-Tiering optimizes storage costs
3. Less-accessed content moves to cheaper tier
4. If accessed again, instant retrieval (no user impact)
5. Combined savings from both services
</pre><br>

<b>Best Practices:</b>
<ul>
<li>Use Intelligent-Tiering for unknown access patterns</li>
<li>Enable Archive Access tiers for additional savings</li>
<li>Monitor S3 Storage Lens for tier distribution</li>
<li>Combine with CloudFront for delivery optimization</li>
<li>Set object lifecycle for true archives (e.g., delete after 2 years)</li>
<li>Tag objects for cost allocation tracking</li>
</ul><br>

<b>Monitoring:</b>
<pre>
# CloudWatch Metrics
- StorageType: Distribution across tiers
- NumberOfObjects: Per tier

# S3 Storage Lens
- View tier distribution over time
- Analyze cost savings
- Identify optimization opportunities
</pre>
</div>
</div>

<!-- ================= Q3 ================= -->
<div class="question">
<pre>
83) A company uses Amazon S3 to store files and images in a variety of storage classes. The company's S3 costs have increased substantially during the past year.

A solutions architect needs to review data trends for the past 12 months and identity the appropriate storage class for the objects.

Which solution will meet these requirements?
</pre>
<div class="options">
<label>
<input type="radio" name="q3">
A. Download AWS Cost and Usage Reports for the last 12 months of S3 usage. Review AWS Trusted Advisor recommendations for cost savings.
</label>

<label>
<input type="radio" name="q3">
B. Use S3 storage class analysis. Import data trends into an Amazon QuickSight dashboard to analyze storage trends.
</label>

<label>
<input type="radio" name="q3">
C. Use Amazon S3 Storage Lens. Upgrade the default dashboard to include advanced metrics for storage trends.
</label>

<label>
<input type="radio" name="q3">
D. Use Access Analyzer for S3. Download the Access Analyzer for S3 report for the last 12 months. Import the .csv file to an Amazon QuickSight dashboard.
</label>
</div>

<button onclick="checkAnswer(this,[2])">Check Answer</button>
<button onclick="showAnswer(this,[2])">Show Answer</button>

<div class="explanation">
<button class="close-explanation" onclick="closeExplanation(this)">✕ Close</button>
<b>Correct Answer: C</b><br><br>

This question tests <b>AWS S3 Storage Lens for comprehensive storage analytics</b>.<br><br>

<b>Requirements:</b>
<ul>
<li>Review 12 months of S3 data trends</li>
<li>Identify appropriate storage classes</li>
<li>Analyze cost increases</li>
<li>Make optimization recommendations</li>
</ul><br>

<b>Why Option C (S3 Storage Lens) is Correct:</b><br>

<b>S3 Storage Lens Features:</b>
<ul>
<li>✓ <b>Organization-Wide Visibility:</b> All accounts, buckets, prefixes</li>
<li>✓ <b>Historical Trends:</b> Up to 15 months of data</li>
<li>✓ <b>Storage Class Analysis:</b> Distribution across tiers</li>
<li>✓ <b>Cost Optimization:</b> Identifies opportunities</li>
<li>✓ <b>Advanced Metrics:</b> Detailed analytics with upgrade</li>
<li>✓ <b>Built-in Dashboard:</b> Interactive visualizations</li>
<li>✓ <b>Recommendations:</b> Automatic optimization suggestions</li>
</ul><br>

<b>S3 Storage Lens Architecture:</b>
<pre>
S3 Storage Lens
    ├─ Free Tier (Default Dashboard)
    │   - 14 days of data
    │   - 28 usage metrics
    │   - Account-level aggregation
    │
    └─ Advanced Metrics & Recommendations (Paid)
        - 15 months historical data ← Perfect for 12-month analysis!
        - 35+ additional metrics
        - Prefix-level metrics
        - Activity metrics
        - Detailed cost breakdown
        - CloudWatch publishing
        - Cost: $0.20 per million objects analyzed/month

<b>Key Metrics for Storage Class Analysis:</b>
- Total storage by class
- Storage class distribution
- Access patterns by class
- Cost trends over time
- Optimization opportunities
</pre><br>

<b>What Storage Lens Shows:</b>
<pre>
<b>Storage Class Distribution:</b>
January 2025:
- Standard: 500 TB (70%) - $11,500
- Standard-IA: 150 TB (21%) - $1,875
- Glacier: 50 TB (7%) - $180
- Intelligent-Tiering: 10 TB (1%) - $230

June 2025:
- Standard: 800 TB (80%) - $18,400 ← Increased!
- Standard-IA: 150 TB (15%) - $1,875
- Glacier: 40 TB (4%) - $144
- Intelligent-Tiering: 10 TB (1%) - $230

<b>Insights:</b>
✗ Standard storage grew 60% (major cost driver)
✓ Should transition more to IA/Glacier
✓ Many objects not accessed in months
✓ Opportunity for Intelligent-Tiering
</pre><br>

<b>Storage Lens Recommendations:</b>
<pre>
<b>Automatic Recommendations:</b>

1. Transition Opportunities
   - 300 TB in Standard not accessed in 90+ days
   - Recommendation: Move to Standard-IA
   - Potential savings: $3,450/month

2. Delete Markers
   - 50,000 delete markers without objects
   - Recommendation: Clean up
   - Potential savings: $125/month

3. Incomplete Multipart Uploads
   - 10 TB of incomplete uploads
   - Recommendation: Add lifecycle policy
   - Potential savings: $230/month

4. Versioning Optimization
   - 100 TB of old versions
   - Recommendation: Archive or delete
   - Potential savings: $2,300/month
</pre><br>

<b>Enabling Advanced Metrics:</b>
<pre>
# Enable S3 Storage Lens with advanced metrics
aws s3control put-storage-lens-configuration \
  --account-id 123456789012 \
  --config-id advanced-storage-lens \
  --storage-lens-configuration '{
    "Id": "advanced-storage-lens",
    "AccountLevel": {
      "ActivityMetrics": {
        "IsEnabled": true
      },
      "BucketLevel": {
        "ActivityMetrics": {
          "IsEnabled": true
        },
        "PrefixLevel": {
          "StorageMetrics": {
            "IsEnabled": true,
            "SelectionCriteria": {
              "MaxDepth": 5
            }
          }
        }
      },
      "AdvancedCostOptimizationMetrics": {
        "IsEnabled": true
      },
      "AdvancedDataProtectionMetrics": {
        "IsEnabled": true
      }
    },
    "DataExport": {
      "S3BucketDestination": {
        "OutputSchemaVersion": "V_1",
        "Format": "Parquet",
        "AccountId": "123456789012",
        "Arn": "arn:aws:s3:::storage-lens-exports",
        "Prefix": "lens-data/"
      }
    },
    "IsEnabled": true
  }'
</pre><br>

<b>Dashboard Metrics:</b>
<table border="1" cellpadding="5">
<tr><th>Category</th><th>Metrics</th></tr>
<tr><td>Summary</td><td>Total storage, object count, buckets</td></tr>
<tr><td>Cost Efficiency</td><td>Incomplete uploads, noncurrent versions, delete markers</td></tr>
<tr><td>Data Protection</td><td>Encryption status, replication, versioning</td></tr>
<tr><td>Access Patterns</td><td>GET/PUT requests, download bytes</td></tr>
<tr><td>Performance</td><td>Transfer acceleration usage, request patterns</td></tr>
</table><br>

<b>Why Other Options Are Wrong:</b><br>

<b>Option A - Cost and Usage Reports + Trusted Advisor:</b>
<ul>
<li>✓ CUR shows costs</li>
<li>✗ <b>Doesn't show storage class analytics:</b> Only costs, not usage patterns</li>
<li>✗ <b>Manual analysis required:</b> Need to parse and analyze raw data</li>
<li>✗ <b>Trusted Advisor limited:</b> Generic recommendations, not detailed S3 analysis</li>
<li>✗ <b>No historical trends:</b> Trusted Advisor shows current state only</li>
<li>✗ <b>More work:</b> Need to build custom analytics</li>
</ul>

What you'd get:
<pre>
CUR: "Spent $20,000 on S3 Standard in June"
But NOT: 
- Which objects are infrequently accessed?
- What should move to IA?
- Access pattern trends?
- Specific optimization recommendations?
</pre><br>

<b>Option B - S3 Storage Class Analysis:</b>
<ul>
<li>✓ Analyzes access patterns for specific buckets</li>
<li>✗ <b>Limited scope:</b> Per-bucket only, not organization-wide</li>
<li>✗ <b>30 days minimum:</b> Takes time to collect data, doesn't have 12 months</li>
<li>✗ <b>Manual setup:</b> Must configure for each bucket</li>
<li>✗ <b>No historical data:</b> Starts from when you enable it</li>
<li>✗ <b>Doesn't meet "12 months" requirement:</b> Can't retroactively analyze</li>
</ul>

Comparison:
<pre>
<b>Storage Class Analysis:</b>
- Scope: Single bucket
- History: Starts from enablement (30+ days to collect)
- Recommendations: Age-based transitions
- Use case: Optimize individual bucket

<b>S3 Storage Lens:</b>
- Scope: All buckets, organization-wide
- History: 15 months (historical data available)
- Recommendations: Comprehensive (versions, uploads, classes, etc.)
- Use case: Organization-wide optimization
</pre><br>

<b>Option D - Access Analyzer for S3:</b>
<ul>
<li>✗ <b>Wrong tool:</b> Access Analyzer is for public/cross-account access</li>
<li>✗ <b>Security focus:</b> Not for storage class optimization</li>
<li>✗ <b>Doesn't analyze costs:</b> Only analyzes access policies</li>
<li>✗ <b>No storage class data:</b> Doesn't provide this information</li>
</ul>

What Access Analyzer actually does:
<pre>
Access Analyzer for S3:
- Identifies buckets with public access
- Finds cross-account access
- Security posture analysis

Does NOT:
- Analyze storage classes
- Provide cost trends
- Recommend tier transitions
- Show access patterns for optimization
</pre><br>

<b>Tool Comparison:</b>
<table border="1" cellpadding="5">
<tr><th>Tool</th><th>Purpose</th><th>Historical Data</th><th>Storage Class Analysis</th></tr>
<tr><td><b>S3 Storage Lens</b></td><td><b>Storage optimization</b></td><td><b>15 months</b></td><td><b>Yes (comprehensive)</b></td></tr>
<tr><td>Storage Class Analysis</td><td>Bucket optimization</td><td>30+ days</td><td>Yes (per bucket)</td></tr>
<tr><td>Cost & Usage Reports</td><td>Billing analysis</td><td>13 months</td><td>No (only costs)</td></tr>
<tr><td>Trusted Advisor</td><td>General recommendations</td><td>Current</td><td>Limited</td></tr>
<tr><td>Access Analyzer</td><td>Security</td><td>90 days</td><td>No</td></tr>
</table><br>

<b>Storage Lens Use Cases:</b>
<pre>
<b>Perfect for this question:</b>
✓ Identify cost increase causes
✓ Analyze 12-month trends
✓ Find optimization opportunities
✓ Organization-wide visibility
✓ Automated recommendations

<b>Other use cases:</b>
- Compliance: Find unencrypted buckets
- Data protection: Verify replication status
- Access patterns: Identify hot/cold data
- Multi-account management: Consolidated view
</pre><br>

<b>Advanced Metrics Example:</b>
<pre>
<b>Without Advanced Metrics (Free):</b>
- Total storage: 1 PB
- Total cost: $23,000/month
- Object count: 100M

<b>With Advanced Metrics ($0.20/M objects = $20/month):</b>
- Standard storage not accessed in 90 days: 300 TB
- Potential savings moving to IA: $3,450/month
- Incomplete multipart uploads: 50 GB ($1.15/month wasted)
- Noncurrent versions: 100 TB ($2,300/month)
- Old delete markers: 10,000 (cleanup recommended)

ROI: Spend $20/month, save $5,000+/month
</pre><br>

<b>Implementing Recommendations:</b>
<pre>
# Based on Storage Lens insights

# 1. Transition old Standard objects
{
  "Rules": [{
    "Id": "transition-old-standard",
    "Status": "Enabled",
    "Transitions": [{
      "Days": 90,
      "StorageClass": "STANDARD_IA"
    }, {
      "Days": 180,
      "StorageClass": "GLACIER"
    }]
  }]
}

# 2. Clean up incomplete multipart uploads
{
  "Rules": [{
    "Id": "cleanup-multipart",
    "Status": "Enabled",
    "AbortIncompleteMultipartUpload": {
      "DaysAfterInitiation": 7
    }
  }]
}

# 3. Manage noncurrent versions
{
  "Rules": [{
    "Id": "archive-old-versions",
    "Status": "Enabled",
    "NoncurrentVersionTransitions": [{
      "NoncurrentDays": 30,
      "StorageClass": "GLACIER"
    }],
    "NoncurrentVersionExpiration": {
      "NoncurrentDays": 365
    }
  }]
}
</pre><br>

<b>Storage Lens Dashboard Views:</b>
<pre>
<b>1. Summary Dashboard</b>
- Storage by region
- Storage by bucket
- Storage by class
- Month-over-month trends

<b>2. Cost Optimization Dashboard</b>
- Transition opportunities
- Incomplete uploads
- Delete markers
- Old versions

<b>3. Data Protection Dashboard</b>
- Encryption status
- Replication status
- Versioning enabled

<b>4. Access Patterns Dashboard</b>
- Request trends
- Download patterns
- Hot vs cold data
</pre><br>

<b>Export and Analyze:</b>
<pre>
# Storage Lens can export to S3
# Data format: Parquet (queryable with Athena)

# Example Athena query
SELECT
  storage_class,
  SUM(storage_bytes) / 1024 / 1024 / 1024 as storage_gb,
  SUM(object_count) as objects
FROM storage_lens_export
WHERE report_date = '2025-06-01'
GROUP BY storage_class
ORDER BY storage_gb DESC;

# Results show distribution for optimization decisions
</pre><br>

<b>Best Practices:</b>
<ul>
<li>Enable S3 Storage Lens with advanced metrics</li>
<li>Review dashboard monthly for optimization</li>
<li>Set up CloudWatch alarms for cost thresholds</li>
<li>Export data for custom analysis if needed</li>
<li>Implement lifecycle policies based on recommendations</li>
<li>Monitor trends after implementing changes</li>
<li>Use organization-wide dashboards for multi-account</li>
</ul><br>

<b>Cost of Storage Lens:</b>
<pre>
Free Tier:
- Default dashboard
- 14 days of data
- 28 metrics
- Account-level only

Advanced Tier:
- $0.20 per million objects analyzed/month
- 15 months of data
- 60+ metrics
- Prefix-level granularity

Example: 100M objects = $20/month
Typical savings identified: $1,000-10,000/month
ROI: 50-500x
</pre>
</div>
</div>

<!-- ================= Q4 ================= -->
<div class="question">
<pre>
84) A company has its cloud infrastructure on AWS. A solutions architect needs to define the infrastructure as code. The infrastructure is currently deployed in one AWS Region. The company's business expansion plan includes deployments in multiple Regions across multiple AWS accounts.

What should the solutions architect do to meet these requirements?
</pre>
<div class="options">
<label>
<input type="radio" name="q4">
A. Use AWS CloudFormation templates. Add IAM policies to control the various accounts. Deploy the templates across the multiple Regions.
</label>

<label>
<input type="radio" name="q4">
B. Use AWS Organizations. Deploy AWS CloudFormation templates from the management account. Use AWS Control Tower to manage deployments across accounts.
</label>

<label>
<input type="radio" name="q4">
C. Use AWS Organizations and AWS CloudFormation StackSets. Deploy a CloudFormation template from an account that has the necessary IAM permissions.
</label>

<label>
<input type="radio" name="q4">
D. Use nested stacks with AWS CloudFormation templates. Change the Region by using nested stacks.
</label>
</div>

<button onclick="checkAnswer(this,[2])">Check Answer</button>
<button onclick="showAnswer(this,[2])">Show Answer</button>

<div class="explanation">
<button class="close-explanation" onclick="closeExplanation(this)">✕ Close</button>
<b>Correct Answer: C</b><br><br>

This question tests <b>CloudFormation StackSets for multi-account, multi-region deployments</b>.<br><br>

<b>Requirements:</b>
<ul>
<li>Infrastructure as code (IaC)</li>
<li>Currently in one region</li>
<li>Need: Multiple regions</li>
<li>Need: Multiple AWS accounts</li>
<li>Scalable deployment strategy</li>
</ul><br>

<b>Why Option C (Organizations + StackSets) is Correct:</b><br>

<b>AWS CloudFormation StackSets:</b>
<ul>
<li>✓ <b>Multi-Account Deployment:</b> Deploy to multiple accounts simultaneously</li>
<li>✓ <b>Multi-Region Deployment:</b> Deploy to multiple regions simultaneously</li>
<li>✓ <b>Single Operation:</b> One template, many accounts/regions</li>
<li>✓ <b>Centralized Management:</b> Manage from one location</li>
<li>✓ <b>Automatic Updates:</b> Update all stacks together</li>
<li>✓ <b>Organizations Integration:</b> Works seamlessly with AWS Organizations</li>
</ul><br>

<b>StackSets Architecture:</b>
<pre>
Administrator Account (Management Account)
    │
    ├─ CloudFormation StackSet Definition
    │   - Template: infrastructure.yaml
    │   - Target accounts: [111111, 222222, 333333]
    │   - Target regions: [us-east-1, eu-west-1, ap-southeast-1]
    │
    ↓ Deploy StackSet
    │
    ├─ Account 111111 (Production)
    │   ├─ us-east-1 → Stack Instance
    │   ├─ eu-west-1 → Stack Instance
    │   └─ ap-southeast-1 → Stack Instance
    │
    ├─ Account 222222 (Development)
    │   ├─ us-east-1 → Stack Instance
    │   └─ eu-west-1 → Stack Instance
    │
    └─ Account 333333 (Testing)
        └─ us-east-1 → Stack Instance

<b>Single Template → 7 Stack Instances deployed automatically!</b>
</pre><br>

<b>How StackSets Work:</b>
<ol>
<li>Create StackSet in administrator account</li>
<li>Specify template and parameters</li>
<li>Define target accounts (via OU or account IDs)</li>
<li>Define target regions</li>
<li>StackSets creates stack instances in all targets</li>
<li>Updates propagate to all instances</li>
<li>Centralized management and rollback</li>
</ol><br>

<b>Implementation:</b>
<pre>
# 1. Create StackSet with service-managed permissions (with Organizations)
aws cloudformation create-stack-set \
  --stack-set-name company-infrastructure \
  --template-body file://infrastructure.yaml \
  --permission-model SERVICE_MANAGED \
  --auto-deployment Enabled=true,RetainStacksOnAccountRemoval=false \
  --call-as SELF

# 2. Deploy to organizational units
aws cloudformation create-stack-instances \
  --stack-set-name company-infrastructure \
  --deployment-targets \
    OrganizationalUnitIds=ou-prod-123,ou-dev-456 \
  --regions us-east-1 eu-west-1 ap-southeast-1 \
  --operation-preferences \
    RegionConcurrencyType=PARALLEL \
    MaxConcurrentPercentage=100 \
    FailureTolerancePercentage=50

# 3. Update all instances at once
aws cloudformation update-stack-set \
  --stack-set-name company-infrastructure \
  --template-body file://infrastructure-v2.yaml \
  --regions us-east-1 eu-west-1
</pre><br>

<b>StackSets + Organizations Integration:</b>
<pre>
<b>Service-Managed Permissions:</b>
- No need to set up IAM roles manually
- Organizations automatically grants permissions
- StackSets can deploy to any account in org
- Automatic deployment to new accounts in OU

<b>Organization Structure:</b>
Root
├─ Production OU
│   ├─ Account A (us-east-1, eu-west-1)
│   └─ Account B (us-east-1, ap-southeast-1)
├─ Development OU
│   └─ Account C (us-east-1)
└─ Testing OU
    └─ Account D (us-east-1)

<b>StackSet Configuration:</b>
Target: Production OU
Regions: us-east-1, eu-west-1, ap-southeast-1
Result: All accounts in Production OU get stacks in all regions
</pre><br>

<b>Auto-Deployment Feature:</b>
<pre>
When auto-deployment is enabled:

New account added to target OU
    ↓
    ↓ Automatic trigger
    ↓
StackSet automatically deploys to new account
    ↓
Infrastructure provisioned without manual intervention

Perfect for:
- Scaling organization
- Landing zones
- Compliance requirements
- Standardized infrastructure
</pre><br>

<b>Why Other Options Are Wrong:</b><br>

<b>Option A - CloudFormation + IAM Policies:</b>
<ul>
<li>✓ CloudFormation is correct for IaC</li>
<li>✗ <b>Manual deployment:</b> Must deploy to each account/region separately</li>
<li>✗ <b>No centralized management:</b> Can't update all at once</li>
<li>✗ <b>Complex IAM setup:</b> Need cross-account roles manually</li>
<li>✗ <b>Not scalable:</b> 10 accounts × 3 regions = 30 manual deployments</li>
<li>✗ <b>No Organizations integration:</b> Missing the best tool</li>
</ul>

Manual deployment example:
<pre>
# Must repeat for each account and region!
aws cloudformation create-stack \
  --stack-name infrastructure \
  --template-body file://template.yaml \
  --region us-east-1 \
  --profile account-1

# Then change profile and region
aws cloudformation create-stack \
  --stack-name infrastructure \
  --template-body file://template.yaml \
  --region eu-west-1 \
  --profile account-1

# Repeat 30 times... Not scalable!
</pre><br>

<b>Option B - Organizations + Control Tower:</b>
<ul>
<li>✓ Organizations is part of the solution</li>
<li>✗ <b>Control Tower not mentioned in requirements:</b> Overkill for this</li>
<li>✗ <b>Doesn't specify StackSets:</b> How to deploy to multiple accounts?</li>
<li>✗ <b>Control Tower is for landing zones:</b> Not general IaC deployment</li>
<li>✗ <b>Less flexible:</b> Control Tower has specific use case</li>
</ul>

Control Tower vs StackSets:
<pre>
<b>Control Tower:</b>
- Purpose: Set up multi-account landing zone
- Use case: Initial AWS environment setup
- Scope: Account creation, baseline controls
- Flexibility: Pre-defined patterns

<b>StackSets:</b>
- Purpose: Deploy any infrastructure
- Use case: Custom IaC across accounts/regions
- Scope: Any CloudFormation template
- Flexibility: Fully customizable
</pre><br>

<b>Option D - Nested Stacks:</b>
<ul>
<li>✗ <b>Nested stacks for modularity:</b> Not for multi-account/region</li>
<li>✗ <b>Single account/region:</b> Nested stacks don't span accounts</li>
<li>✗ <b>Wrong use case:</b> Nested stacks organize complex templates</li>
<li>✗ <b>"Change region with nested stacks":</b> Not how it works</li>
</ul>

What nested stacks actually do:
<pre>
Parent Stack (single account/region)
├─ Nested Stack: VPC
├─ Nested Stack: Security Groups
├─ Nested Stack: EC2 Instances
└─ Nested Stack: RDS Database

Purpose:
- Organize complex templates into modules
- Reuse common patterns
- Manage dependencies

Does NOT:
- Deploy across accounts
- Deploy across regions
- Solve multi-account problem
</pre><br>

<b>Comparison Table:</b>
<table border="1" cellpadding="5">
<tr><th>Feature</th><th>Option A</th><th>Option B</th><th>Option C (StackSets)</th><th>Option D</th></tr>
<tr><td>Multi-Account</td><td>Manual</td><td>Partial</td><td><b>Automatic</b></td><td>No</td></tr>
<tr><td>Multi-Region</td><td>Manual</td><td>Partial</td><td><b>Automatic</b></td><td>No</td></tr>
<tr><td>Centralized</td><td>No</td><td>Yes</td><td><b>Yes</b></td><td>No</td></tr>
<tr><td>Scalability</td><td>Low</td><td>Medium</td><td><b>High</b></td><td>Low</td></tr>
<tr><td>Ease of Use</td><td>Complex</td><td>Medium</td><td><b>Simple</b></td><td>N/A</td></tr>
</table><br>

<b>StackSets Use Cases:</b>
<pre>
<b>Perfect for this question:</b>
✓ Deploy VPC infrastructure across regions
✓ Deploy security baselines to all accounts
✓ Deploy monitoring stack organization-wide
✓ Deploy IAM roles to multiple accounts
✓ Deploy compliance controls everywhere

<b>Real-world examples:</b>
- Multi-region DR setup
- Organization-wide CloudTrail
- Centralized logging infrastructure
- Security hub deployment
- Config rules across organization
</pre><br>

<b>StackSet Deployment Strategies:</b>
<pre>
<b>Sequential Deployment:</b>
RegionConcurrencyType: SEQUENTIAL
- Deploy to one region at a time
- Safer for testing
- Slower overall

<b>Parallel Deployment:</b>
RegionConcurrencyType: PARALLEL
- Deploy to all regions simultaneously
- Faster deployment
- Higher risk if template has issues

<b>Failure Tolerance:</b>
FailureTolerancePercentage: 25
- Allow 25% of deployments to fail
- Useful for partial rollouts
- Can continue despite some failures
</pre><br>

<b>Permission Models:</b>
<pre>
<b>Service-Managed (with Organizations):</b> ← Recommended for this question
- AWS handles permissions automatically
- No manual IAM role creation
- Auto-deployment to new accounts
- Requires Organizations

<b>Self-Managed:</b>
- Manual IAM role setup in each account
- More control over permissions
- More complex to set up
- Use without Organizations
</pre><br>

<b>Template Example:</b>
<pre>
# infrastructure.yaml - deployed to all accounts/regions
AWSTemplateFormatVersion: '2010-09-09'
Description: 'Company Standard Infrastructure'

Parameters:
  Environment:
    Type: String
    AllowedValues: [prod, dev, test]

Resources:
  VPC:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: !Sub '10.${Environment}.0.0/16'
      EnableDnsHostnames: true
      Tags:
        - Key: Name
          Value: !Sub '${Environment}-vpc-${AWS::Region}'

  # More resources...

# Deploy with different parameters per account/OU
</pre><br>

<b>Monitoring StackSets:</b>
<pre>
# List all stack instances
aws cloudformation list-stack-instances \
  --stack-set-name company-infrastructure

# Check operation status
aws cloudformation describe-stack-set-operation \
  --stack-set-name company-infrastructure \
  --operation-id 12345

# View drift detection
aws cloudformation detect-stack-set-drift \
  --stack-set-name company-infrastructure
</pre><br>

<b>Best Practices:</b>
<ul>
<li>Use service-managed permissions with Organizations</li>
<li>Enable auto-deployment for new accounts</li>
<li>Use parallel deployment for speed</li>
<li>Set appropriate failure tolerance</li>
<li>Test template in single region first</li>
<li>Use drift detection regularly</li>
<li>Implement CI/CD for StackSet updates</li>
<li>Tag resources for cost allocation</li>
<li>Document template parameters</li>
</ul><br>

<b>Update Strategy:</b>
<pre>
# Update all stack instances
1. Update StackSet template
2. StackSets propagates to all instances
3. Can specify subset of accounts/regions if needed

# Selective update example
aws cloudformation update-stack-instances \
  --stack-set-name company-infrastructure \
  --accounts 111111 222222 \
  --regions us-east-1 \
  --parameter-overrides ParameterKey=Environment,ParameterValue=prod
</pre>
</div>
</div>

<!-- ================= Q5 ================= -->
<div class="question">
<pre>
86) A company plans to refactor a monolithic application into a modern application design deployed on AWS. The CI/CD pipeline needs to be upgraded to support the modern design for the application with the following requirements:

• It should allow changes to be released several times every hour.
• It should be able to roll back the changes as quickly as possible.

Which design will meet these requirements?
</pre>
<div class="options">
<label>
<input type="radio" name="q5">
A. Deploy a CI/CD pipeline that incorporates AMIs to contain the application and their configurations. Deploy the application by replacing Amazon EC2 instances.
</label>

<label>
<input type="radio" name="q5">
B. Specify AWS Elastic Beanstalk to stage in a secondary environment as the deployment target for the CI/CD pipeline of the application. To deploy, swap the staging and production environment URLs.
</label>

<label>
<input type="radio" name="q5">
C. Use AWS Systems Manager to re-provision the infrastructure for each deployment. Update the Amazon EC2 user data to pull the latest code artifact from Amazon S3 and use Amazon Route 53 weighted routing to point to the new environment.
</label>

<label>
<input type="radio" name="q5">
D. Roll out the application updates as part of an Auto Scaling event using prebuilt AMIs. Use new versions of the AMIs to add instances, and phase out all instances that use the previous AMI version with the configured termination policy during a deployment event.
</label>
</div>

<button onclick="checkAnswer(this,[1])">Check Answer</button>
<button onclick="showAnswer(this,[1])">Show Answer</button>

<div class="explanation">
<button class="close-explanation" onclick="closeExplanation(this)">✕ Close</button>
<b>Correct Answer: B</b><br><br>

This question tests <b>rapid deployment and rollback strategies with AWS Elastic Beanstalk</b>.<br><br>

<b>Requirements:</b>
<ul>
<li>Release changes several times per hour (high frequency)</li>
<li>Quick rollback capability (seconds, not minutes)</li>
<li>Modern application design (microservices/cloud-native)</li>
<li>Minimal downtime</li>
</ul><br>

<b>Why Option B (Elastic Beanstalk URL Swap) is Correct:</b><br>

<b>Elastic Beanstalk Blue/Green Deployment:</b>
<ul>
<li>✓ <b>Instant URL Swap:</b> DNS switch in seconds</li>
<li>✓ <b>Instant Rollback:</b> Swap back immediately if issues</li>
<li>✓ <b>Zero Downtime:</b> Both environments running during swap</li>
<li>✓ <b>Quick Deployments:</b> Deploy to staging while prod runs</li>
<li>✓ <b>Validation:</b> Test staging before swapping</li>
<li>✓ <b>Managed Platform:</b> Elastic Beanstalk handles infrastructure</li>
</ul><br>

<b>Deployment Architecture:</b>
<pre>
<b>Initial State:</b>
Production Environment (app-prod)
    URL: myapp.elasticbeanstalk.com → Version 1.0
    Status: Serving live traffic

Staging Environment (app-staging)
    URL: myapp-staging.elasticbeanstalk.com → Empty
    Status: Idle

<b>Deployment Process:</b>
Step 1: Deploy new version to staging
    Production: myapp.elasticbeanstalk.com → v1.0 (live traffic)
    Staging: myapp-staging.elasticbeanstalk.com → v1.1 (deploying)

Step 2: Test staging environment
    Production: Still serving v1.0
    Staging: v1.1 ready, running tests

Step 3: Swap URLs (instant - DNS change)
    Production: myapp.elasticbeanstalk.com → v1.1 ✓
    Staging: myapp-staging.elasticbeanstalk.com → v1.0

<b>Rollback (if needed):</b>
    Swap URLs again → Back to v1.0 in seconds!

<b>Timeline:</b>
- Deploy to staging: 2-5 minutes
- Test staging: 1-2 minutes
- URL swap: 10-30 seconds ← Instant switch!
- Rollback: 10-30 seconds ← Instant recovery!
</pre><br>

<b>Implementation:</b>
<pre>
# 1. Create two Elastic Beanstalk environments
aws elasticbeanstalk create-environment \
  --application-name myapp \
  --environment-name myapp-prod \
  --solution-stack-name "64bit Amazon Linux 2023 v6.0.0 running Node.js 18"

aws elasticbeanstalk create-environment \
  --application-name myapp \
  --environment-name myapp-staging \
  --solution-stack-name "64bit Amazon Linux 2023 v6.0.0 running Node.js 18"

# 2. Deploy new version to staging
aws elasticbeanstalk update-environment \
  --environment-name myapp-staging \
  --version-label v1.1

# 3. Wait for deployment to complete
# 4. Run tests against staging

# 5. Swap environment URLs
aws elasticbeanstalk swap-environment-cnames \
  --source-environment-name myapp-prod \
  --destination-environment-name myapp-staging

# Result: Instant switch!

# 6. If issues detected, rollback (swap again)
aws elasticbeanstalk swap-environment-cnames \
  --source-environment-name myapp-prod \
  --destination-environment-name myapp-staging
</pre><br>

<b>CI/CD Pipeline Integration:</b>
<pre>
Code Commit → Build → Deploy to Staging → Test → Swap URLs

# Example with AWS CodePipeline
Stages:
1. Source (GitHub/CodeCommit)
2. Build (CodeBuild)
   - Compile application
   - Run unit tests
   - Create deployment package

3. Deploy to Staging (Elastic Beanstalk)
   - Deploy new version to staging environment
   - Wait for health checks

4. Integration Tests
   - Run automated tests against staging
   - Validate functionality

5. Approval (optional)
   - Manual approval gate
   - Or automated based on test results

6. Swap Environments (Lambda function)
   - Call swap-environment-cnames API
   - Monitor for 5 minutes

7. Rollback (if needed)
   - Automatic rollback on errors
   - Swap back to previous version
</pre><br>

<b>Multiple Deployments Per Hour:</b>
<pre>
Hour 1:
10:00 - Deploy v1.1 to staging
10:05 - Swap → v1.1 production
10:15 - Deploy v1.2 to staging (now has v1.0)
10:20 - Swap → v1.2 production
10:30 - Deploy v1.3 to staging (now has v1.1)
10:35 - Swap → v1.3 production

Each deployment: 5-10 minutes total
Frequency: Up to 6-12 deployments per hour possible!
</pre><br>

<b>Why Other Options Are Wrong:</b><br>

<b>Option A - AMI-Based Deployment:</b>
<ul>
<li>✗ <b>Slow AMI creation:</b> 10-30 minutes to create AMI</li>
<li>✗ <b>Slow instance replacement:</b> Launch new instances, terminate old</li>
<li>✗ <b>Cannot deploy "several times per hour":</b> Too slow</li>
<li>✗ <b>Rollback complexity:</b> Need to rebuild previous AMI</li>
<li>✗ <b>Not suitable for frequent deployments</b></li>
</ul>

Timeline:
<pre>
AMI Deployment Process:
1. Build application: 5 min
2. Create AMI: 15-30 min ← TOO SLOW!
3. Launch instances: 5-10 min
4. Register with LB: 2-5 min
5. Terminate old instances: 5 min

Total: 32-55 minutes per deployment
Can only deploy ~1-2 times per hour (not "several")
</pre><br>

<b>Option C - Systems Manager + User Data:</b>
<ul>
<li>✗ <b>Re-provision infrastructure:</b> Creates new resources (slow)</li>
<li>✗ <b>User data execution:</b> Runs on instance boot (minutes)</li>
<li>✗ <b>Route 53 DNS propagation:</b> 60+ seconds, up to minutes</li>
<li>✗ <b>Complex orchestration:</b> Many moving parts</li>
<li>✗ <b>Rollback requires full reprovision:</b> Not quick</li>
</ul>

Problems:
<pre>
- Reprovisioning: Create new VPC/subnets/etc? (minutes-hours)
- User data: Pulls code on boot (adds 2-5 minutes)
- Route 53 weighted routing: DNS caching issues
- Rollback: Must reprovision old environment
- Complexity: Error-prone, hard to maintain
</pre><br>

<b>Option D - Auto Scaling AMI Updates:</b>
<ul>
<li>✗ <b>Same AMI creation slowness as Option A</b></li>
<li>✗ <b>Instance replacement during Auto Scaling:</b> Gradual, not instant</li>
<li>✗ <b>Dependent on scaling events:</b> Must trigger scaling</li>
<li>✗ <b>Rollback requires new AMI:</b> Time-consuming</li>
<li>✗ <b>Not designed for frequent deployments</b></li>
</ul>

Auto Scaling deployment:
<pre>
1. Create new Launch Template with new AMI
2. Update Auto Scaling Group
3. Instance refresh (gradual replacement)
   - Replaces instances one by one
   - Takes 20-60 minutes for full refresh
4. Monitor health checks

Timeline: 30-90 minutes per deployment
Frequency: ~1 deployment per hour maximum
</pre><br>

<b>Comparison Table:</b>
<table border="1" cellpadding="5">
<tr><th>Option</th><th>Deploy Time</th><th>Rollback Time</th><th>Deployments/Hour</th><th>Complexity</th></tr>
<tr><td>A - AMI</td><td>30-60 min</td><td>30-60 min</td><td>1-2</td><td>High</td></tr>
<tr><td><b>B - Beanstalk</b></td><td><b>5-10 min</b></td><td><b>10-30 sec</b></td><td><b>6-12</b></td><td><b>Low</b></td></tr>
<tr><td>C - SSM</td><td>20-40 min</td><td>20-40 min</td><td>2-3</td><td>Very High</td></tr>
<tr><td>D - ASG</td><td>30-90 min</td><td>30-90 min</td><td>1-2</td><td>High</td></tr>
</table><br>

<b>Elastic Beanstalk Deployment Policies:</b>
<pre>
<b>All at Once:</b>
- Deploy to all instances simultaneously
- Downtime during deployment
- Fastest for small apps

<b>Rolling:</b>
- Deploy in batches
- Some instances unavailable
- No additional resources needed

<b>Rolling with Additional Batch:</b>
- Maintain full capacity
- Deploy in batches with extra instances
- Longer deployment time

<b>Immutable:</b>
- Deploy to new instances
- Swap when ready
- Quick rollback

<b>Blue/Green (URL Swap):</b> ← THIS QUESTION
- Two complete environments
- Instant switch
- Instant rollback
- Best for frequent deployments
</pre><br>

<b>Additional Benefits of Option B:</b>
<ul>
<li>Managed platform (updates, patching)</li>
<li>Auto Scaling built-in</li>
<li>Load balancing included</li>
<li>Monitoring with CloudWatch</li>
<li>Multiple platform versions (Node.js, Python, Java, etc.)</li>
<li>Environment cloning for easy staging setup</li>
<li>Built-in health monitoring</li>
</ul><br>

<b>Monitoring and Alarms:</b>
<pre>
# CloudWatch Alarms for auto-rollback
{
  "AlarmName": "HighErrorRate",
  "MetricName": "ApplicationErrors",
  "Threshold": 5,
  "EvaluationPeriods": 2,
  "ComparisonOperator": "GreaterThanThreshold",
  "AlarmActions": ["arn:aws:lambda:region:account:function:RollbackSwap"]
}

# Lambda function triggers rollback if errors spike
def lambda_handler(event, context):
    eb = boto3.client('elasticbeanstalk')
    eb.swap_environment_cnames(
        SourceEnvironmentName='myapp-prod',
        DestinationEnvironmentName='myapp-staging'
    )
</pre><br>

<b>Best Practices:</b>
<ul>
<li>Always test in staging before swapping</li>
<li>Set up health checks and alarms</li>
<li>Implement automated rollback on errors</li>
<li>Keep staging environment warm (always running)</li>
<li>Use saved configurations for consistency</li>
<li>Enable enhanced health reporting</li>
<li>Monitor both environments during swap</li>
<li>Document rollback procedures</li>
</ul><br>

<b>Cost Optimization:</b>
<pre>
<b>Two environments cost 2x, but:</b>
- Terminate staging after swap (save 50%)
- Recreate staging when needed
- Or use smaller instance type for staging

<b>Alternative for cost savings:</b>
- Keep prod running 24/7
- Spin up staging only during deployments
- Terminate staging after successful swap
- Recreate for next deployment

Balance: Speed vs Cost
</pre>
</div>
</div>

<!-- ================= Q6 ================= -->
<div class="question">
<pre>
87) A company has several Amazon EC2 instances set up in a private subnet for security reasons. These instances host applications that read and write large amounts of data to and from Amazon S3 regularly. Currently, subnet routing directs all the traffic destined for the internet through a NAT gateway. The company wants to optimize the overall cost without impacting the ability of the application to communicate with Amazon S3 or the outside internet.

What change should a solutions architect make to optimize costs?
</pre>
<div class="options">
<label>
<input type="checkbox" name="q6">
A. Create an additional NAT gateway and attach it to the private subnet. Update the route table.
</label>

<label>
<input type="checkbox" name="q6">
B. Create an S3 VPC endpoint and attach it to the VPC. Update the route table for the subnet.
</label>

<label>
<input type="checkbox" name="q6">
C. Create an S3 interface VPC endpoint. Update the subnet configuration to enable private connectivity.
</label>

<label>
<input type="checkbox" name="q6">
D. Create an S3 gateway endpoint and add it to the route table of the subnet.
</label>
</div>

<button onclick="checkAnswer(this,[1,3])">Check Answer</button>
<button onclick="showAnswer(this,[1,3])">Show Answer</button>

<div class="explanation">
<button class="close-explanation" onclick="closeExplanation(this)">✕ Close</button>
<b>Correct Answer: B, D</b><br><br>

This question is about <b>VPC endpoints for cost optimization with S3 access</b>. Both B and D are essentially the same answer worded differently.<br><br>

<b>Current Situation:</b>
<ul>
<li>EC2 instances in private subnet</li>
<li>Need to access S3 regularly (large data transfers)</li>
<li>Currently using NAT Gateway for all internet traffic</li>
<li>NAT Gateway costs: Data processing fees</li>
<li>Goal: Optimize costs without breaking functionality</li>
</ul><br>

<b>Cost Problem with NAT Gateway for S3:</b>
<pre>
<b>NAT Gateway Pricing:</b>
- Hourly charge: $0.045/hour = $32.40/month
- Data processing: $0.045/GB processed
- For S3 access: All data goes through NAT Gateway

<b>Example monthly costs:</b>
Application transfers 10 TB/month with S3

NAT Gateway costs:
- Hourly: $32.40
- Data processing: 10,000 GB × $0.045 = $450
- Total: $482.40/month

S3 data transfer out costs:
- First 10 TB: $0.09/GB = $900
- Total NAT + S3: $1,382.40/month
</pre><br>

<b>Why Options B/D (S3 Gateway VPC Endpoint) Are Correct:</b><br>

<b>S3 Gateway VPC Endpoint Benefits:</b>
<ul>
<li>✓ <b>FREE:</b> No hourly charges, no data processing fees</li>
<li>✓ <b>Private connectivity:</b> Traffic doesn't leave AWS network</li>
<li>✓ <b>No NAT Gateway needed for S3:</b> Direct VPC routing</li>
<li>✓ <b>Better performance:</b> Lower latency, higher bandwidth</li>
<li>✓ <b>Secure:</b> Traffic stays within AWS</li>
<li>✓ <b>Scalable:</b> No throughput limits</li>
</ul><br>

<b>Cost Savings Calculation:</b>
<pre>
<b>Before (with NAT Gateway for S3):</b>
- NAT hourly: $32.40/month
- NAT data processing: $450/month (10 TB)
- S3 transfer: $900/month
- Total: $1,382.40/month

<b>After (with S3 Gateway Endpoint):</b>
- S3 Gateway Endpoint: $0.00
- NAT hourly: $32.40/month (still needed for internet access)
- NAT data processing: $0 (S3 traffic bypasses NAT)
- S3 transfer: $900/month (same)
- Total: $932.40/month

<b>Monthly Savings: $450 (NAT data processing fees eliminated!)</b>
<b>Annual Savings: $5,400</b>

For 10 accounts: $54,000/year savings!
</pre><br>

<b>Architecture Change:</b>
<pre>
<b>Before (all traffic through NAT Gateway):</b>
EC2 in Private Subnet
    │
    ↓ All traffic
Private Route Table
    │
    ├─ 0.0.0.0/0 → NAT Gateway (in public subnet)
    └─ 10.0.0.0/16 → local
    │
    ↓ NAT Gateway
    ↓ Internet Gateway
    ↓
S3 (via internet path) + Other Internet Services

<b>Cost: NAT Gateway data processing fees apply!</b>

───────────────────────────────────────────────

<b>After (S3 via Gateway Endpoint):</b>
EC2 in Private Subnet
    │
    ├─ S3 API calls
    │   ↓
    │   S3 Gateway VPC Endpoint (in route table)
    │   ↓
    │   S3 (private AWS network) ✓ FREE!
    │
    └─ Internet traffic
        ↓
        NAT Gateway
        ↓
        Internet Gateway
        ↓
        Other Internet Services

<b>Cost: Only internet traffic uses NAT Gateway!</b>
<b>S3 traffic: FREE (no NAT charges)</b>
</pre><br>

<b>Implementation (B and D are the same):</b>
<pre>
# Create S3 Gateway VPC Endpoint
aws ec2 create-vpc-endpoint \
  --vpc-id vpc-12345678 \
  --service-name com.amazonaws.us-east-1.s3 \
  --route-table-ids rtb-12345678 \
  --vpc-endpoint-type Gateway

# Route table automatically updated with:
# Destination: pl-xxxxxxxx (S3 prefix list)
# Target: vpce-xxxxxxxx (VPC endpoint)

# Result:
# S3 traffic: Routes to VPC endpoint (FREE)
# Internet traffic: Still routes to NAT Gateway (for other services)
</pre><br>

<b>Route Table After VPC Endpoint Creation:</b>
<pre>
Private Subnet Route Table:
┌─────────────────────┬──────────────────────┐
│ Destination         │ Target               │
├─────────────────────┼──────────────────────┤
│ 10.0.0.0/16        │ local                │ ← VPC CIDR
│ pl-63a5400a        │ vpce-1a2b3c4d        │ ← S3 prefix list → endpoint
│ 0.0.0.0/0          │ nat-0a1b2c3d          │ ← Internet → NAT Gateway
└─────────────────────┴──────────────────────┘

Traffic flow:
- S3: matches pl-63a5400a → uses VPC endpoint (FREE)
- Internet: matches 0.0.0.0/0 → uses NAT Gateway (cost for other services)
</pre><br>

<b>Why B and D Are the Same:</b>
<pre>
<b>Option B:</b> "Create an S3 VPC endpoint and attach it to the VPC"
<b>Option D:</b> "Create an S3 gateway endpoint and add it to the route table"

Both describe creating an S3 Gateway VPC Endpoint:
- "S3 VPC endpoint" for S3 = Gateway type (default)
- "S3 gateway endpoint" = explicit naming
- Both attach to route table (how gateway endpoints work)
- Both are the correct solution

AWS has only 2 endpoint types for S3:
1. Gateway endpoint (free, route table-based)
2. Interface endpoint (charged, ENI-based)

For S3, Gateway is standard and implied when unspecified.
</pre><br>

<b>Why Other Options Are Wrong:</b><br>

<b>Option A - Additional NAT Gateway:</b>
<ul>
<li>✗ <b>Increases costs:</b> 2 NAT Gateways = 2× hourly fees</li>
<li>✗ <b>Still pays data processing:</b> Doesn't eliminate S3 charges</li>
<li>✗ <b>Wrong direction:</b> Question asks to optimize/reduce costs</li>
<li>✗ <b>No benefit for S3 access</b></li>
</ul>

Cost impact:
<pre>
Current: 1 NAT Gateway = $32.40/month
Option A: 2 NAT Gateways = $64.80/month
Additional cost: +$32.40/month (worse!)

Plus: Still paying data processing fees for S3 = $450/month
Total: Makes it MORE expensive!
</pre><br>

<b>Option C - S3 Interface VPC Endpoint:</b>
<ul>
<li>✗ <b>Interface endpoints are charged:</b> $0.01/hour per AZ</li>
<li>✗ <b>Data processing fees:</b> $0.01/GB processed</li>
<li>✗ <b>More expensive than Gateway endpoint:</b> Should use Gateway</li>
<li>✗ <b>Not cost-optimized</b></li>
</ul>

S3 Interface Endpoint costs:
<pre>
Hourly charge: $0.01/hour per AZ
- 2 AZs: $0.02/hour = $14.40/month

Data processing: $0.01/GB
- 10 TB: 10,000 GB × $0.01 = $100/month

Total: $114.40/month

Compare to Gateway Endpoint: $0 (FREE)

Why pay when Gateway endpoint is free and better?
</pre><br>

<b>When to Use Each Endpoint Type:</b>
<pre>
<b>S3 Gateway Endpoint:</b> ← Use for this question
- Free
- Route table-based
- Use for S3 from VPC
- Most common use case

<b>S3 Interface Endpoint:</b>
- Charged (hourly + data)
- ENI-based (private IP)
- Use when:
  * Need to access from on-premises via Direct Connect/VPN
  * Need private DNS resolution
  * Need to access from peered VPC
- More expensive but more flexible
</pre><br>

<b>Complete Architecture:</b>
<pre>
<b>VPC: 10.0.0.0/16</b>

Public Subnet (10.0.1.0/24):
├─ NAT Gateway
├─ Internet Gateway attached
└─ Elastic IP for NAT

Private Subnet (10.0.2.0/24):
├─ EC2 Instances (application servers)
├─ Route Table:
│   ├─ 10.0.0.0/16 → local
│   ├─ pl-63a5400a (S3) → vpce-xxx (Gateway Endpoint) ✓ FREE
│   └─ 0.0.0.0/0 → NAT Gateway ✓ For other internet services
└─ Security Group: Allow outbound to S3, internet

<b>Traffic Flow:</b>
1. S3 API calls (s3:GetObject, s3:PutObject)
   → Matches S3 prefix list
   → Routes to VPC Gateway Endpoint
   → Direct to S3 (private AWS network)
   → Cost: $0 for NAT (only S3 transfer costs)

2. Other internet traffic (apt-get, yum, external APIs)
   → Matches 0.0.0.0/0
   → Routes to NAT Gateway
   → Goes to Internet Gateway
   → Cost: NAT Gateway fees apply
</pre><br>

<b>Endpoint Policy (optional):</b>
<pre>
{
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": "*",
      "Action": [
        "s3:GetObject",
        "s3:PutObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::my-company-bucket/*",
        "arn:aws:s3:::my-company-bucket"
      ]
    }
  ]
}

# Restrict endpoint to specific buckets for security
</pre><br>

<b>Comparison: NAT Gateway vs VPC Endpoints:</b>
<table border="1" cellpadding="5">
<tr><th>Feature</th><th>NAT Gateway</th><th>S3 Gateway Endpoint</th><th>S3 Interface Endpoint</th></tr>
<tr><td>Hourly Cost</td><td>$0.045/hr</td><td>$0.00</td><td>$0.01/hr per AZ</td></tr>
<tr><td>Data Processing</td><td>$0.045/GB</td><td>$0.00</td><td>$0.01/GB</td></tr>
<tr><td>Performance</td><td>Good</td><td>Excellent</td><td>Excellent</td></tr>
<tr><td>Setup</td><td>Medium</td><td>Simple</td><td>Medium</td></tr>
<tr><td>Use Case</td><td>Internet access</td><td><b>S3 from VPC</b></td><td>S3 from on-prem</td></tr>
</table><br>

<b>Real-World Scenario:</b>
<pre>
<b>Company with 50 TB/month S3 transfers:</b>

Without VPC Endpoint:
- NAT hourly: $32.40/month
- NAT data: 50,000 GB × $0.045 = $2,250/month
- S3 transfer: 50,000 GB × $0.09 = $4,500/month
- Total: $6,782.40/month
- Annual: $81,388.80

With VPC Gateway Endpoint:
- NAT hourly: $32.40/month (still need for internet)
- NAT data: $0 (S3 bypasses NAT)
- S3 transfer: $4,500/month (same)
- Total: $4,532.40/month
- Annual: $54,388.80

<b>Annual Savings: $27,000 per account!</b>
</pre><br>

<b>Additional Supported Services (Gateway Endpoints):</b>
<pre>
Gateway VPC Endpoints (FREE) support:
- Amazon S3
- DynamoDB

Both are free and should always be used when accessing
these services from VPC resources!
</pre><br>

<b>Verification:</b>
<pre>
# Test S3 access from EC2
aws s3 ls s3://my-bucket/

# Check VPC endpoint is being used
# Traffic should not show in NAT Gateway metrics

# VPC Flow Logs will show:
# srcaddr: 10.0.2.x (EC2 private IP)
# dstaddr: S3 prefix list IP
# action: ACCEPT
# No NAT Gateway IP in the path
</pre><br>

<b>Best Practices:</b>
<ul>
<li>Always use S3 Gateway Endpoints for VPC to S3 traffic</li>
<li>Create endpoints in all route tables that need S3 access</li>
<li>Use endpoint policies to restrict access to specific buckets</li>
<li>Monitor NAT Gateway data processing metrics (should decrease)</li>
<li>Keep NAT Gateway for general internet access</li>
<li>Consider DynamoDB Gateway Endpoint similarly</li>
<li>Document cost savings for stakeholders</li>
</ul><br>

<b>Migration Steps:</b>
<ol>
<li>Create S3 Gateway VPC Endpoint</li>
<li>Associate with route tables</li>
<li>Test S3 access from EC2 (should work immediately)</li>
<li>Monitor NAT Gateway metrics (data processing should drop)</li>
<li>Verify S3 traffic via VPC Flow Logs</li>
<li>Calculate and report cost savings</li>
</ol>
</div>
</div>

<!-- ================= Q7 ================= -->
<div class="question">
<pre>
88) A company has a multi-account AWS setup in AWS Organizations. The company has three accounts: the management account, development account, and production account. The company needs to set up cost alerts and track spending across all accounts. All billing data should flow to the management account.

What should a solutions architect recommend?
</pre>
<div class="options">
<label>
<input type="radio" name="q7">
A. Create AWS Budgets in each account to track spending. Use AWS Lambda to aggregate the budget data and send it to the management account.
</label>

<label>
<input type="radio" name="q7">
B. Activate IAM user and role access to billing in each account. Create AWS Budgets in the management account for each individual account.
</label>

<label>
<input type="radio" name="q7">
C. Create AWS Budgets in each account to track spending. Configure Amazon EventBridge (Amazon CloudWatch Events) in each account to send budget notifications to the management account.
</label>

<label>
<input type="radio" name="q7">
D. Create an AWS Cost and Usage Report in the management account. Use Amazon QuickSight to visualize the spending data across all accounts.
</label>
</div>

<button onclick="checkAnswer(this,[1])">Check Answer</button>
<button onclick="showAnswer(this,[1])">Show Answer</button>

<div class="explanation">
<button class="close-explanation" onclick="closeExplanation(this)">✕ Close</button>
<b>Correct Answer: B</b><br><br>

This question tests <b>AWS Organizations consolidated billing and centralized budget management</b>.<br><br>

<b>Requirements:</b>
<ul>
<li>Multi-account setup (Management, Development, Production)</li>
<li>Need cost alerts across all accounts</li>
<li>Track spending per account</li>
<li>All billing data flows to management account</li>
<li>Centralized monitoring from management account</li>
</ul><br>

<b>Why Option B (Budgets in Management Account) is Correct:</b><br>

<b>AWS Organizations Consolidated Billing:</b>
<ul>
<li>✓ <b>Automatic consolidation:</b> All account charges roll up to management account</li>
<li>✓ <b>Single payer account:</b> Management account pays for all</li>
<li>✓ <b>Centralized visibility:</b> See all account costs in one place</li>
<li>✓ <b>Volume discounts:</b> Combined usage for better pricing</li>
<li>✓ <b>IAM access control:</b> Enable billing access as needed</li>
</ul><br>

<b>AWS Budgets in Management Account:</b>
<ul>
<li>✓ <b>Filter by linked account:</b> Create budget per member account</li>
<li>✓ <b>Centralized alerting:</b> All alerts in one account</li>
<li>✓ <b>No aggregation needed:</b> Data already consolidated</li>
<li>✓ <b>Single management point:</b> One place to configure budgets</li>
<li>✓ <b>Cost allocation tags:</b> Filter by account, service, tag</li>
</ul><br>

<b>Architecture:</b>
<pre>
AWS Organization:
├─ Management Account (111111111111)
│   │
│   ├─ Consolidated Billing (automatic)
│   │   └─ Receives all charges from member accounts
│   │
│   └─ AWS Budgets (centralized)
│       ├─ Budget 1: Development Account ($10,000/month)
│       │   - Filter: Linked Account = 222222222222
│       │   - Alert: 80%, 90%, 100%
│       │
│       ├─ Budget 2: Production Account ($50,000/month)
│       │   - Filter: Linked Account = 333333333333
│       │   - Alert: 80%, 90%, 100%
│       │
│       └─ Budget 3: Total Organization ($100,000/month)
│           - Filter: All accounts
│           - Alert: 90%, 100%
│
├─ Development Account (222222222222)
│   └─ Resources: EC2, RDS, S3, etc.
│       └─ Charges flow to Management Account
│
└─ Production Account (333333333333)
    └─ Resources: EC2, RDS, S3, Lambda, etc.
        └─ Charges flow to Management Account

<b>All billing data automatically in Management Account!</b>
</pre><br>

<b>Implementation:</b>
<pre>
# 1. Enable IAM access to billing in all accounts
# In each account (Management, Dev, Prod):
# AWS Console → Account Settings → IAM User and Role Access to Billing
# Check: "Activate IAM Access" ✓

# 2. Create Budgets in Management Account

# Budget for Development Account
aws budgets create-budget \
  --account-id 111111111111 \
  --budget file://dev-budget.json \
  --notifications-with-subscribers file://notifications.json

# dev-budget.json
{
  "BudgetName": "Development-Monthly-Budget",
  "BudgetLimit": {
    "Amount": "10000",
    "Unit": "USD"
  },
  "TimeUnit": "MONTHLY",
  "BudgetType": "COST",
  "CostFilters": {
    "LinkedAccount": ["222222222222"]  ← Filter by dev account
  }
}

# notifications.json
{
  "Notification": {
    "NotificationType": "ACTUAL",
    "ComparisonOperator": "GREATER_THAN",
    "Threshold": 80,
    "ThresholdType": "PERCENTAGE"
  },
  "Subscribers": [
    {
      "SubscriptionType": "EMAIL",
      "Address": "devteam@company.com"
    },
    {
      "SubscriptionType": "SNS",
      "Address": "arn:aws:sns:us-east-1:111111111111:billing-alerts"
    }
  ]
}

# 3. Repeat for Production Account
# Budget with LinkedAccount filter: 333333333333

# 4. Create organization-wide budget
{
  "BudgetName": "Organization-Total-Budget",
  "BudgetLimit": {
    "Amount": "100000",
    "Unit": "USD"
  },
  "TimeUnit": "MONTHLY",
  "BudgetType": "COST"
  # No LinkedAccount filter = all accounts included
}
</pre><br>

<b>IAM Access to Billing (Required Step):</b>
<pre>
<b>Why activate IAM access to billing?</b>
- By default, only root user can access billing
- Enable IAM users/roles to view/manage budgets
- Required for CloudFormation/Terraform automation
- Enables delegated budget management

<b>Steps for each account:</b>
1. Sign in as root user
2. Account Settings → IAM User and Role Access to Billing
3. Click "Edit" and activate
4. Apply IAM policies to users/roles:

{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "budgets:*",
        "ce:*",
        "aws-portal:ViewBilling"
      ],
      "Resource": "*"
    }
  ]
}
</pre><br>

<b>Budget Alert Flow:</b>
<pre>
Development Account generates $8,500 in charges
    ↓
Consolidated Billing aggregates to Management Account
    ↓
Management Account Budget evaluates ($8,500 / $10,000 = 85%)
    ↓
Threshold exceeded (85% > 80%)
    ↓
Budget Alert triggered
    ↓
Notifications sent to:
    - Email: devteam@company.com
    - SNS: arn:aws:sns:...:billing-alerts
    ↓
DevTeam receives alert: "Dev account at 85% of monthly budget"
</pre><br>

<b>Why Other Options Are Wrong:</b><br>

<b>Option A - Budgets in Each Account + Lambda Aggregation:</b>
<ul>
<li>✗ <b>Unnecessary complexity:</b> No need for Lambda aggregation</li>
<li>✗ <b>Data already consolidated:</b> Organizations does this automatically</li>
<li>✗ <b>Multiple budget configurations:</b> Harder to manage</li>
<li>✗ <b>Custom code to maintain:</b> Lambda function requires updates</li>
<li>✗ <b>Higher cost:</b> Lambda invocations + multiple budgets</li>
</ul>

Why it's overcomplicated:
<pre>
Option A approach:
1. Create budget in Dev account
2. Create budget in Prod account
3. Create budget in Mgmt account
4. Lambda function reads all 3 budgets
5. Lambda aggregates data
6. Lambda sends to Mgmt account
7. Maintain Lambda code

Option B approach:
1. Create budgets in Mgmt account with filters
2. Done! (Data already there via consolidated billing)
</pre><br>

<b>Option C - Budgets + EventBridge:</b>
<ul>
<li>✗ <b>Same problem as Option A:</b> Unnecessary distribution</li>
<li>✗ <b>Multiple budgets to configure:</b> Management overhead</li>
<li>✗ <b>EventBridge cross-account rules:</b> Added complexity</li>
<li>✗ <b>Data already in Management Account:</b> No need to send it there</li>
</ul>

Complexity comparison:
<pre>
Option C:
- Create budget in Dev account
- Create EventBridge rule in Dev account
- Create EventBridge bus in Mgmt account
- Grant cross-account permissions
- Repeat for each account
- Manage multiple notification channels

Option B:
- Create budgets in Mgmt account
- Done!
</pre><br>

<b>Option D - Cost and Usage Report + QuickSight:</b>
<ul>
<li>✓ CUR is good for detailed analysis</li>
<li>✓ QuickSight visualization is valuable</li>
<li>✗ <b>Doesn't provide cost alerts:</b> Missing key requirement</li>
<li>✗ <b>Passive analysis only:</b> No proactive notifications</li>
<li>✗ <b>More complex setup:</b> S3, Athena, QuickSight</li>
<li>✗ <b>No budget thresholds:</b> Just reporting, not alerting</li>
</ul>

What Option D provides:
<pre>
<b>Cost and Usage Report:</b>
- Detailed billing data → S3
- Query with Athena
- Visualize with QuickSight
- Great for analysis and chargebacks

<b>What it DOESN'T do:</b>
✗ Proactive alerts when budget threshold exceeded
✗ Budget forecasting
✗ Automatic notifications
✗ Simple threshold monitoring

<b>Question requires:</b> "cost alerts" ← Budget alerts, not just reports
</pre><br>

<b>Comparison Table:</b>
<table border="1" cellpadding="5">
<tr><th>Feature</th><th>Option A</th><th><b>Option B</b></th><th>Option C</th><th>Option D</th></tr>
<tr><td>Cost Alerts</td><td>Yes (complex)</td><td><b>Yes (simple)</b></td><td>Yes (complex)</td><td>No</td></tr>
<tr><td>Centralized</td><td>No (distributed)</td><td><b>Yes</b></td><td>No (distributed)</td><td>Yes (reporting only)</td></tr>
<tr><td>Setup Complexity</td><td>High</td><td><b>Low</b></td><td>High</td><td>Medium</td></tr>
<tr><td>Maintenance</td><td>High (Lambda)</td><td><b>Low</b></td><td>Medium (EventBridge)</td><td>Medium</td></tr>
<tr><td>Meets Requirements</td><td>Partially</td><td><b>Fully</b></td><td>Partially</td><td>No (no alerts)</td></tr>
</table><br>

<b>Budget Types You Can Create:</b>
<pre>
<b>1. Cost Budget:</b> ← This question
   Monitor spending (actual or forecasted)
   Alert when threshold exceeded

<b>2. Usage Budget:</b>
   Monitor service usage (e.g., EC2 hours)
   Alert on usage thresholds

<b>3. RI Utilization Budget:</b>
   Monitor Reserved Instance usage %
   Alert if RIs underutilized

<b>4. RI Coverage Budget:</b>
   Monitor % of usage covered by RIs
   Alert if coverage drops

<b>5. Savings Plans Budget:</b>
   Monitor Savings Plans utilization
   Ensure maximum savings
</pre><br>

<b>Budget Alert Types:</b>
<pre>
<b>Actual Alerts:</b>
- Trigger when actual spending crosses threshold
- "You've spent $8,000 of $10,000 budget (80%)"

<b>Forecasted Alerts:</b>
- Trigger when projected spending will exceed budget
- "Based on current usage, you'll exceed budget by month-end"

<b>Both can notify via:</b>
- Email (up to 10 addresses)
- SNS topic (for Slack, PagerDuty, etc.)
- ChatBot (AWS Chatbot for Slack/Chime)
</pre><br>

<b>Budget Notification Example:</b>
<pre>
Subject: AWS Budget Alert - Development Account

Budget Name: Development-Monthly-Budget
Account: 222222222222 (Development)
Current Spend: $8,534.21
Budget Amount: $10,000.00
Percentage: 85%

Alert Threshold: 80%
Status: EXCEEDED

Forecasted Month-End Spend: $10,750.00
Forecasted Overage: $750.00

Take action to control costs or adjust budget.

View details: https://console.aws.amazon.com/billing/home#/budgets
</pre><br>

<b>Advanced Budget Features:</b>
<pre>
<b>Cost Filters:</b>
- Linked Account: 222222222222
- Service: Amazon EC2, Amazon RDS
- Tag: Environment=Production, Team=Data
- Region: us-east-1, eu-west-1
- Purchase Option: On Demand, Reserved, Spot

<b>Example: Multi-dimensional budget</b>
{
  "BudgetName": "Prod-EC2-OnDemand-Budget",
  "CostFilters": {
    "LinkedAccount": ["333333333333"],  ← Prod account
    "Service": ["Amazon Elastic Compute Cloud - Compute"],
    "PurchaseOption": ["On Demand"]
  },
  "BudgetLimit": {
    "Amount": "20000",
    "Unit": "USD"
  }
}
</pre><br>

<b>Automated Actions (Bonus Feature):</b>
<pre>
AWS Budgets can trigger automatic actions:

1. Apply IAM Policy
   - Restrict resource creation when budget exceeded
   - Deny EC2 launches over threshold

2. Apply SCP
   - Organization-level restrictions
   - Stop account spending

3. Stop EC2 Instances
   - Automatically stop dev instances at 100% budget
   - Prevent overspending

Example:
{
  "ActionType": "APPLY_IAM_POLICY",
  "PolicyArn": "arn:aws:iam::111111111111:policy/DenyEC2Launch",
  "ExecutionRoleArn": "arn:aws:iam::111111111111:role/BudgetActionRole",
  "ApprovalModel": "AUTOMATIC",
  "Subscribers": [...]
}
</pre><br>

<b>Best Practices:</b>
<ul>
<li>Create account-specific budgets with LinkedAccount filter</li>
<li>Set multiple thresholds (50%, 80%, 90%, 100%)</li>
<li>Use forecasted alerts to prevent overspending</li>
<li>Send notifications to both email and SNS</li>
<li>Create service-specific budgets for major services</li>
<li>Review and adjust budgets quarterly</li>
<li>Use cost allocation tags for granular budgets</li>
<li>Enable IAM access to billing for automation</li>
<li>Document budget alert procedures</li>
</ul><br>

<b>Complete Solution Summary:</b>
<ol>
<li>Activate IAM access to billing in all accounts</li>
<li>Create budgets in Management Account</li>
<li>Use LinkedAccount filter for per-account budgets</li>
<li>Set alert thresholds (80%, 90%, 100%)</li>
<li>Configure SNS notifications</li>
<li>Monitor and adjust monthly</li>
</ol>

Simple, centralized, and leverages built-in AWS Organizations features!
</div>
</div>

<!-- ================= Q8 ================= -->
<div class="question">
<pre>
89) A solutions architect is designing a solution where users will be directed to a backup static error page if the primary website is unavailable. The primary website's DNS records are hosted in Amazon Route 53. The domain is pointing to an Application Load Balancer (ALB). The solutions architect needs a solution that minimizes changes and infrastructure overhead.

Which solution will meet these requirements?
</pre>
<div class="options">
<label>
<input type="radio" name="q8">
A. Deploy a CloudFormation stack with an ALB and EC2 instances in a separate AWS Region. Configure Route 53 failover routing with health checks to route traffic to the backup instances if the primary ALB fails.
</label>

<label>
<input type="radio" name="q8">
B. Update the Route 53 DNS records to use a multivalue answer routing policy. Add a second value for the backup page hosted on Amazon S3. Configure health checks for both the ALB and the S3 bucket.
</label>

<label>
<input type="radio" name="q8">
C. Configure Route 53 active-passive failover with the primary resource pointing to the ALB and the secondary resource pointing to the S3 bucket hosting the error page. Configure health checks for the ALB.
</label>

<label>
<input type="radio" name="q8">
D. Create a Route 53 health check that monitors the primary ALB. Create a Lambda function that changes the Route 53 DNS record to point to the S3 bucket if the health check fails.
</label>
</div>

<button onclick="checkAnswer(this,[2])">Check Answer</button>
<button onclick="showAnswer(this,[2])">Show Answer</button>

<div class="explanation">
<button class="close-explanation" onclick="closeExplanation(this)">✕ Close</button>
<b>Correct Answer: C</b><br><br>

This question tests <b>Route 53 failover routing for disaster recovery with minimal infrastructure</b>.<br><br>

<b>Requirements:</b>
<ul>
<li>Primary: Website on ALB (active)</li>
<li>Backup: Static error page (passive)</li>
<li>Automatic failover to backup if primary unavailable</li>
<li>Minimize changes to existing setup</li>
<li>Minimize infrastructure overhead</li>
</ul><br>

<b>Why Option C (Route 53 Active-Passive Failover) is Correct:</b><br>

<b>Route 53 Failover Routing:</b>
<ul>
<li>✓ <b>Built-in feature:</b> No custom code needed</li>
<li>✓ <b>Automatic failover:</b> Route 53 handles switching</li>
<li>✓ <b>Health check based:</b> Monitors primary ALB</li>
<li>✓ <b>Simple S3 backup:</b> Static error page in S3 (cheap)</li>
<li>✓ <b>Minimal infrastructure:</b> Just S3 bucket + health check</li>
<li>✓ <b>No additional regions needed:</b> S3 is global service</li>
</ul><br>

<b>Architecture:</b>
<pre>
<b>Normal Operation (Primary Healthy):</b>

User Request: www.example.com
    ↓
Route 53 DNS Query
    ↓
Health Check: ALB Status = Healthy ✓
    ↓
Route 53 returns: PRIMARY record (ALB IP)
    ↓
User → Application Load Balancer
    ↓
Website served normally


<b>Failover (Primary Unhealthy):</b>

User Request: www.example.com
    ↓
Route 53 DNS Query
    ↓
Health Check: ALB Status = Unhealthy ✗
    ↓
Route 53 returns: SECONDARY record (S3 endpoint)
    ↓
User → S3 Static Website (error page)
    ↓
"We're experiencing technical difficulties. Please try again later."


<b>Recovery (Primary Back Online):</b>

Health Check: ALB Status = Healthy ✓
    ↓
Route 53 automatically switches back to PRIMARY
    ↓
Users directed to ALB again
</pre><br>

<b>Implementation:</b>
<pre>
# 1. Create S3 bucket for static error page
aws s3 mb s3://www-example-com-error-page

# 2. Enable static website hosting
aws s3 website s3://www-example-com-error-page \
  --index-document error.html

# 3. Upload error page
aws s3 cp error.html s3://www-example-com-error-page/ \
  --acl public-read

# 4. Create Route 53 health check for ALB
aws route53 create-health-check \
  --health-check-config file://health-check.json

# health-check.json
{
  "Type": "HTTPS",
  "ResourcePath": "/health",
  "FullyQualifiedDomainName": "my-alb-123456789.us-east-1.elb.amazonaws.com",
  "Port": 443,
  "RequestInterval": 30,
  "FailureThreshold": 3
}

# 5. Create PRIMARY failover record (ALB)
aws route53 change-resource-record-sets \
  --hosted-zone-id Z1234567890ABC \
  --change-batch file://primary-record.json

# primary-record.json
{
  "Changes": [{
    "Action": "CREATE",
    "ResourceRecordSet": {
      "Name": "www.example.com",
      "Type": "A",
      "SetIdentifier": "Primary",
      "Failover": "PRIMARY",
      "AliasTarget": {
        "HostedZoneId": "Z35SXDOTRQ7X7K",  ← ALB hosted zone
        "DNSName": "my-alb-123456789.us-east-1.elb.amazonaws.com",
        "EvaluateTargetHealth": false
      },
      "HealthCheckId": "abc123-health-check"  ← Link to health check
    }
  }]
}

# 6. Create SECONDARY failover record (S3)
aws route53 change-resource-record-sets \
  --hosted-zone-id Z1234567890ABC \
  --change-batch file://secondary-record.json

# secondary-record.json
{
  "Changes": [{
    "Action": "CREATE",
    "ResourceRecordSet": {
      "Name": "www.example.com",
      "Type": "A",
      "SetIdentifier": "Secondary",
      "Failover": "SECONDARY",
      "AliasTarget": {
        "HostedZoneId": "Z3AQBSTGFYJSTF",  ← S3 website hosted zone
        "DNSName": "s3-website-us-east-1.amazonaws.com",
        "EvaluateTargetHealth": false
      }
      # No health check needed for secondary
    }
  }]
}
</pre><br>

<b>Route 53 Health Check Details:</b>
<pre>
<b>Health Check Configuration:</b>
- Endpoint: ALB DNS name
- Protocol: HTTPS
- Path: /health (or any valid endpoint)
- Port: 443
- Interval: 30 seconds (check every 30 sec)
- Failure Threshold: 3 (fail after 3 consecutive failures)
- String Matching: Optional (check response contains "OK")

<b>Health Check Logic:</b>
Every 30 seconds, Route 53 sends HTTPS request to:
  https://my-alb.us-east-1.elb.amazonaws.com/health

If 3 consecutive checks fail (90 seconds):
  → Health Check Status = Unhealthy
  → Route 53 stops returning PRIMARY record
  → Route 53 returns SECONDARY record (S3)

When health check succeeds again:
  → Health Check Status = Healthy
  → Route 53 resumes returning PRIMARY record (ALB)
  → Automatic recovery!
</pre><br>

<b>S3 Static Website Setup:</b>
<pre>
error.html:
<!DOCTYPE html>
<html>
<head>
  <title>Service Temporarily Unavailable</title>
  <style>
    body { font-family: Arial; text-align: center; padding: 50px; }
    h1 { color: #e74c3c; }
  </style>
</head>
<body>
  <h1>We're experiencing technical difficulties</h1>
  <p>Our team is working to resolve the issue.</p>
  <p>Please try again in a few minutes.</p>
  <p>If the problem persists, contact support at support@example.com</p>
</body>
</html>

S3 Bucket Policy:
{
  "Version": "2012-10-17",
  "Statement": [{
    "Sid": "PublicReadGetObject",
    "Effect": "Allow",
    "Principal": "*",
    "Action": "s3:GetObject",
    "Resource": "arn:aws:s3:::www-example-com-error-page/*"
  }]
}
</pre><br>

<b>Failover Timeline:</b>
<pre>
<b>Failure Detection:</b>
00:00 - ALB becomes unhealthy (EC2 instances down)
00:30 - First health check fails
01:00 - Second health check fails
01:30 - Third health check fails → Status: Unhealthy
01:31 - Route 53 removes PRIMARY from DNS responses
01:31 - Route 53 starts returning SECONDARY (S3)

<b>User Impact:</b>
- Users with cached DNS (TTL not expired): Still go to ALB (fail)
- New DNS queries: Get S3 endpoint (see error page)
- Within TTL period: All users redirected to S3

<b>DNS TTL Impact:</b>
If TTL = 60 seconds:
  - Within 60 seconds, all users see error page
If TTL = 300 seconds (5 min):
  - Takes up to 5 minutes for all users to failover

Best practice: Set TTL = 60 seconds for faster failover
</pre><br>

<b>Why Other Options Are Wrong:</b><br>

<b>Option A - CloudFormation Stack in Another Region:</b>
<ul>
<li>✗ <b>Massive infrastructure overhead:</b> Complete duplicate setup</li>
<li>✗ <b>Expensive:</b> ALB + EC2 instances running 24/7 in another region</li>
<li>✗ <b>Over-engineered:</b> Question asks for "backup static error page", not full DR</li>
<li>✗ <b>More changes required:</b> Deploy entire new stack</li>
<li>✗ <b>Against requirements:</b> "minimize infrastructure overhead"</li>
</ul>

Cost comparison:
<pre>
Option A (Multi-region ALB + EC2):
- ALB in region 2: $16/month + $0.008/LCU-hour = $60+/month
- EC2 instances (2x t3.medium): 2 × $30 = $60/month
- Data transfer: $50/month
- Total: ~$170/month

Option C (S3 static site):
- S3 storage (1 MB): $0.00
- S3 requests (minimal during failures): $0.01/month
- Total: ~$0.01/month

<b>Savings: $169.99/month for same functionality!</b>
</pre><br>

<b>Option B - Multivalue Answer Routing:</b>
<ul>
<li>✗ <b>Wrong routing policy:</b> Multivalue returns BOTH values randomly</li>
<li>✗ <b>Not failover:</b> Users might still get unhealthy ALB</li>
<li>✗ <b>Unreliable:</b> Doesn't guarantee traffic goes to healthy endpoint</li>
<li>✗ <b>Used for load distribution:</b> Not disaster recovery</li>
</ul>

How multivalue works (wrong for this):
<pre>
Route 53 Multivalue Answer:
- Returns multiple IP addresses (up to 8)
- Client randomly picks one
- Used for simple load distribution
- All values returned even if some unhealthy (with health checks)

Problem:
User query → Route 53 returns: [ALB IP, S3 endpoint]
User picks ALB → Still fails! (not a backup)

Failover policy (correct):
User query → If ALB healthy: return ALB
           → If ALB unhealthy: return S3
Guaranteed to work!
</pre><br>

<b>Option D - Lambda Function to Change DNS:</b>
<ul>
<li>✗ <b>Custom code required:</b> Must write and maintain Lambda</li>
<li>✗ <b>More infrastructure:</b> Lambda, IAM roles, EventBridge</li>
<li>✗ <b>Higher complexity:</b> Custom solution vs built-in Route 53 feature</li>
<li>✗ <b>Slower response:</b> Lambda invocation adds delay</li>
<li>✗ <b>Unnecessary:</b> Route 53 failover does this automatically</li>
</ul>

Why reinvent the wheel?
<pre>
Option D approach:
1. Create health check
2. Create EventBridge rule on health check alarm
3. Create Lambda function
4. Lambda updates Route 53 record
5. Handle failures, retries, logging
6. Maintain Lambda code
7. Monitor Lambda execution
8. Handle race conditions

Option C approach:
1. Create health check
2. Create failover records
3. Done! (Route 53 handles everything)

Route 53 already provides failover natively!
</pre><br>

<b>Comparison Table:</b>
<table border="1" cellpadding="5">
<tr><th>Criteria</th><th>Option A</th><th>Option B</th><th><b>Option C</b></th><th>Option D</th></tr>
<tr><td>Infrastructure</td><td>Very High (ALB+EC2)</td><td>Low</td><td><b>Low (S3 only)</b></td><td>Medium (Lambda)</td></tr>
<tr><td>Cost</td><td>$170+/month</td><td>$0.01/month</td><td><b>$0.01/month</b></td><td>$5/month</td></tr>
<tr><td>Complexity</td><td>High</td><td>Medium</td><td><b>Low</b></td><td>High</td></tr>
<tr><td>Automatic Failover</td><td>Yes</td><td>No (random)</td><td><b>Yes</b></td><td>Yes (delayed)</td></tr>
<tr><td>Maintenance</td><td>High</td><td>Low</td><td><b>Very Low</b></td><td>Medium</td></tr>
<tr><td>Meets Requirements</td><td>No (overhead)</td><td>No (not failover)</td><td><b>Yes</b></td><td>Partially</td></tr>
</table><br>

<b>Route 53 Failover Record Types:</b>
<pre>
<b>PRIMARY:</b>
- Active resource (ALB)
- Requires health check
- Returned when healthy
- If unhealthy → use SECONDARY

<b>SECONDARY:</b>
- Passive resource (S3)
- No health check needed (assumed always available)
- Returned only when PRIMARY unhealthy
- Automatic fallback

You must have exactly:
- 1 PRIMARY record (or more with different SetIdentifiers)
- 1 or more SECONDARY records
</pre><br>

<b>Health Check Best Practices:</b>
<pre>
1. <b>Monitor endpoint health, not just ping:</b>
   Path: /health (application-level check)
   Not just: TCP port 80 open

2. <b>Check frequently:</b>
   Interval: 30 seconds (balance speed vs cost)

3. <b>Set appropriate thresholds:</b>
   Failure threshold: 3 (avoid false positives)

4. <b>Use string matching:</b>
   Check response contains "healthy"
   More reliable than just 200 OK

5. <b>Health check from multiple regions:</b>
   Route 53 checks from multiple locations
   Must fail from ~18% of checkers to be unhealthy
</pre><br>

<b>Monitoring:</b>
<pre>
# CloudWatch metrics for health check
aws cloudwatch get-metric-statistics \
  --namespace AWS/Route53 \
  --metric-name HealthCheckStatus \
  --dimensions Name=HealthCheckId,Value=abc123 \
  --start-time 2024-01-01T00:00:00Z \
  --end-time 2024-01-02T00:00:00Z \
  --period 300 \
  --statistics Average

# Create alarm for health check failures
aws cloudwatch put-metric-alarm \
  --alarm-name primary-site-down \
  --alarm-description "Primary website health check failed" \
  --metric-name HealthCheckStatus \
  --namespace AWS/Route53 \
  --statistic Minimum \
  --period 60 \
  --threshold 1 \
  --comparison-operator LessThanThreshold \
  --evaluation-periods 1 \
  --alarm-actions arn:aws:sns:us-east-1:123456789012:ops-alerts
</pre><br>

<b>Testing Failover:</b>
<pre>
# 1. Simulate ALB failure
# Update ALB health check path to /nonexistent
# Or: Stop all target instances

# 2. Wait for health check to fail (90 seconds)

# 3. Test DNS resolution
dig www.example.com
# Should return S3 website endpoint

# 4. Browser test
curl http://www.example.com
# Should return error page from S3

# 5. Restore primary
# Fix ALB, wait for health check to pass

# 6. Verify automatic recovery
dig www.example.com
# Should return ALB endpoint again
</pre><br>

<b>Complete Solution Steps:</b>
<ol>
<li>Create S3 bucket, enable static website hosting</li>
<li>Upload error page HTML</li>
<li>Create Route 53 health check for ALB</li>
<li>Create PRIMARY failover record pointing to ALB (with health check)</li>
<li>Create SECONDARY failover record pointing to S3</li>
<li>Set DNS TTL to 60 seconds for faster failover</li>
<li>Test failover by stopping ALB targets</li>
<li>Monitor with CloudWatch alarms</li>
</ol>

Minimal infrastructure, automatic failover, cost-effective!
</div>
</div>

<!-- ================= Q9 ================= -->
<div class="question">
<pre>
90) A company is using Amazon VPC flow logs to troubleshoot network connectivity issues between two Amazon EC2 instances. The VPC flow logs are published to Amazon CloudWatch Logs. A solutions architect needs to analyze the flow logs to troubleshoot the connectivity issues.

Which solution will provide the analysis with the LEAST operational overhead?
</pre>
<div class="options">
<label>
<input type="radio" name="q9">
A. Use Amazon Athena to query the VPC flow logs from Amazon S3. Use SQL queries to analyze the flow logs.
</label>

<label>
<input type="radio" name="q9">
B. Use CloudWatch Logs Insights to query and analyze the VPC flow logs.
</label>

<label>
<input type="radio" name="q9">
C. Export the VPC flow logs to Amazon S3. Download the logs and use a text editor to analyze them.
</label>

<label>
<input type="radio" name="q9">
D. Use AWS Lambda to process the VPC flow logs from CloudWatch Logs and store the results in Amazon DynamoDB for analysis.
</label>
</div>

<button onclick="checkAnswer(this,[1])">Check Answer</button>
<button onclick="showAnswer(this,[1])">Show Answer</button>

<div class="explanation">
<button class="close-explanation" onclick="closeExplanation(this)">✕ Close</button>
<b>Correct Answer: B</b><br><br>

This question tests <b>analyzing VPC Flow Logs with minimal operational overhead using CloudWatch Logs Insights</b>.<br><br>

<b>Given Situation:</b>
<ul>
<li>VPC Flow Logs already published to CloudWatch Logs</li>
<li>Need to troubleshoot connectivity between 2 EC2 instances</li>
<li>Goal: Analyze logs with LEAST operational overhead</li>
</ul><br>

<b>Why Option B (CloudWatch Logs Insights) is Correct:</b><br>

<b>CloudWatch Logs Insights:</b>
<ul>
<li>✓ <b>Zero setup:</b> Logs already in CloudWatch</li>
<li>✓ <b>Built-in query language:</b> No configuration needed</li>
<li>✓ <b>Interactive analysis:</b> Results in seconds</li>
<li>✓ <b>No data movement:</b> Query logs where they are</li>
<li>✓ <b>Pay per query:</b> $0.005 per GB scanned</li>
</ul><br>

Since the logs are already in CloudWatch Logs, using CloudWatch Logs Insights requires zero setup and provides immediate query capability - the answer with the LEAST operational overhead.
</div>
</div>

<!-- ================= Navigation Bottom ================= -->
<div style="text-align:center; margin: 40px 0;">
  <a href="page8.html" style="
      display:inline-block;
      padding: 12px 28px;
      background:#6b7280;
      color:#fff;
      font-size:15px;
      font-weight:600;
      border-radius:8px;
      text-decoration:none;
      margin-right:10px;
  ">
    ← Previous Page
  </a>
  <a href="page10.html" style="
      display:inline-block;
      padding: 12px 28px;
      background:#6b7280;
      color:#fff;
      font-size:15px;
      font-weight:600;
      border-radius:8px;
      text-decoration:none;
  ">
    Next Page →
  </a>
</div>

</div> <!-- end container -->

<script>
function checkAnswer(btn, correct) {
  const q = btn.parentElement;
  const isMultiSelect = q.querySelector('input[type="checkbox"]') !== null;
  const inputs = q.querySelectorAll(isMultiSelect ? 'input[type="checkbox"]' : 'input[type="radio"]');
  const labels = q.querySelectorAll("label");
  const selected = [];

  inputs.forEach((inp, idx) => {
    if (inp.checked) selected.push(idx);
  });

  labels.forEach((label, idx) => {
    label.classList.remove("user-correct", "user-wrong", "correct");
    if (selected.includes(idx)) {
      if (correct.includes(idx)) {
        label.classList.add("user-correct");
      } else {
        label.classList.add("user-wrong");
      }
    }
  });

  let resultMsg = q.querySelector(".result-message");
  if (!resultMsg) {
    resultMsg = document.createElement("div");
    resultMsg.className = "result-message";
    q.appendChild(resultMsg);
  }

  const isCorrect = JSON.stringify(selected.sort()) === JSON.stringify(correct.sort());
  if (isCorrect) {
    resultMsg.textContent = "✔ Correct!";
    resultMsg.style.color = "#10b981";
    resultMsg.style.fontWeight = "600";
  } else {
    resultMsg.textContent = "✖ Incorrect. Try again or click 'Show Answer'.";
    resultMsg.style.color = "#ef4444";
    resultMsg.style.fontWeight = "600";
  }
  resultMsg.style.display = "block";
}

function showAnswer(btn, correct) {
  const q = btn.parentElement;
  const labels = q.querySelectorAll("label");
  
  labels.forEach(label => {
    label.classList.remove("user-correct", "user-wrong");
  });
  
  correct.forEach(i => labels[i].classList.add("correct"));
  const explanation = q.querySelector(".explanation");
  explanation.style.display = "block";
  
  const resultMsg = q.querySelector(".result-message");
  if (resultMsg) {
    resultMsg.style.display = "none";
  }
}

function closeExplanation(btn) {
  const explanation = btn.parentElement;
  explanation.style.display = "none";
}
</script>

</body>
</html>
