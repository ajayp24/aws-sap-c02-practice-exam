<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>AWS Solution Architect Practice Test – Page 1</title>
<link rel="stylesheet" href="style.css">

</head>

<body>
<div class="container">

<h1>AWS Solution Architect – Practice Test (Page 2)</h1>

<!-- ================= Q1 ================= -->
<div class="question">
<pre>
11) A company has many AWS accounts and uses AWS Organizations to manage all of them. A solutions architect must implement a solution that the company can use to share a common network across multiple accounts.
The company’s infrastructure team has a dedicated infrastructure account that has a VPC. The infrastructure team must use this account to manage the network. 
Individual accounts cannot have the ability to manage their own networks. However, individual accounts must be able to create AWS resources within subnets.
Which combination of actions should the solutions architect perform to meet these requirements? (Choose two.)
</pre>

<div class="options">
<label>
<input type="checkbox">
A. Create a transit gateway in the infrastructure account.
</label>

<label>
<input type="checkbox">
B. Enable resource sharing from the AWS Organizations management account. Most Voted
</label>

<label>
<input type="checkbox">
C. Create VPCs in each AWS account within the organization in AWS Organizations. Configure the VPCs to share the same CIDR range and subnets as the VPC in the infrastructure account. Peer the VPCs in each individual account with the VPC in the infrastructure account.
</label>

<label>
<input type="checkbox">
D. Create a resource share in AWS Resource Access Manager in the infrastructure account. Select the specific AWS Organizations OU that will use the shared network. Select each subnet to associate with the resource share. Most Voted
</label>

<label>
<input type="checkbox">
E. Create a resource share in AWS Resource Access Manager in the infrastructure account. Select the specific AWS Organizations OU that will use the shared network. Select each prefix list to associate with the resource share.
</label>
</div>

<button onclick="showAnswer(this,[1,3])">Show Answer</button>

<div class="explanation">
<b>Correct Answers: B and D</b><br><br>

This question is about <b>centralized network ownership with decentralized resource creation</b> in a multi-account AWS Organization.

<br><br>

The infrastructure team owns the VPC and must retain full control over networking components such as CIDR ranges, route tables, and gateways. Application teams in other accounts should not be able to modify the network, but they still need to launch resources (EC2, RDS, Lambda ENIs) inside shared subnets.

<br><br>

<b>AWS Resource Access Manager (RAM)</b> allows AWS resources to be shared across accounts in the same AWS Organization. However, RAM sharing only works if resource sharing is explicitly enabled from the AWS Organizations management account.

<br><br>

<b>Option B</b> enables resource sharing at the organization level, which is required before any shared resources can be used.

<br><br>

<b>Option D</b> shares specific subnets from the infrastructure account’s VPC with selected AWS Organization OUs. This allows member accounts to deploy resources into those subnets while preventing them from changing the VPC configuration.

<br><br>

This approach enforces centralized network governance while allowing application teams to operate independently, which is an AWS best practice.
</div>
</div>


<!-- ================= Q2 ================= -->
<div class="question">
<pre>
12) A company wants to use a third-party software-as-a-service (SaaS) application. The third-party SaaS application is consumed through several API calls. 
The third-party SaaS application also runs on AWS inside a VPC.
The company will consume the third-party SaaS application from inside a VPC. 
The company has internal security policies that mandate the use of private connectivity that does not traverse the internet. 
No resources that run in the company VPC are allowed to be accessed from outside the company’s VPC. 
All permissions must conform to the principles of least privilege.
Which solution meets these requirements?
</pre>

<div class="options">
<label>
<input type="radio" name="q12">
A. Create an AWS PrivateLink interface VPC endpoint. Connect this endpoint to the endpoint service that the third-party SaaS application provides. Create a security group to limit the access to the endpoint. Associate the security group with the endpoint
</label>

<label>
<input type="radio" name="q12">
B. Create an AWS Site-to-Site VPN connection between the third-party SaaS application and the company VPC. Configure network ACLs to limit access across the VPN tunnels.
</label>

<label>
<input type="radio" name="q12">
C. Create a VPC peering connection between the third-party SaaS application and the company VPUpdate route tables by adding the needed routes for the peering connection.
</label>

<label>
<input type="radio" name="q12">
D. Create an AWS PrivateLink endpoint service. Ask the third-party SaaS provider to create an interface VPC endpoint for this endpoint service. Grant permissions for the endpoint service to the specific account of the third-party SaaS provider.
</label>
</div>

<button onclick="showAnswer(this,[0])">Show Answer</button>

<div class="explanation">
<b>Correct Answer: A</b><br><br>

This question is testing <b>secure, private consumption of a third-party SaaS application</b> while strictly following the <b>principle of least privilege</b>.

<br><br>

The company requires that traffic:
- Does <b>not traverse the public internet</b>
- Does <b>not allow inbound access</b> into the company VPC
- Is <b>restricted only to the specific SaaS service</b>, not an entire network

<br><br>

<b>AWS PrivateLink</b> is specifically designed for this use case.

<br><br>

With an <b>interface VPC endpoint</b>, AWS creates elastic network interfaces inside the company’s VPC. These ENIs provide private connectivity to the SaaS provider’s endpoint service entirely within the AWS network.

<br><br>

Traffic flow:
Company VPC → Interface VPC Endpoint → AWS PrivateLink → SaaS provider VPC

<br><br>

Because the connection is service-level (not network-level), the SaaS provider cannot access any other resources inside the company VPC. This satisfies the <b>least privilege</b> requirement.

<br><br>

Other options are not correct:
- A Site-to-Site VPN or VPC peering exposes broad network access.
- Creating an endpoint service is something the <b>SaaS provider</b> does, not the customer.

<br><br>

Therefore, using an AWS PrivateLink interface VPC endpoint is the correct and most secure solution.
</div>
</div>



<!-- ================= Q3 ================= -->
<div class="question">
<pre>
13) A company needs to implement a patching process for its servers. The on-premises servers and Amazon EC2 instances use a variety of tools to perform patching. 
Management requires a single report showing the patch status of all the servers and instances.
Which set of actions should a solutions architect take to meet these requirements?
</pre>

<div class="options">
<label><input type="radio" name="q13">
A. Use AWS Systems Manager to manage patches on the on-premises servers and EC2 instances. Use Systems Manager to generate patch compliance reports.
</label>

<label><input type="radio" name="q13">
B. Use AWS OpsWorks to manage patches on the on-premises servers and EC2 instances. Use Amazon QuickSight integration with OpsWorks to generate patch compliance reports.
</label>

<label><input type="radio" name="q13">
C. Use an Amazon EventBridge rule to apply patches by scheduling an AWS Systems Manager patch remediation job. Use Amazon Inspector to generate patch compliance reports.
</label>

<label><input type="radio" name="q13">
D. Use AWS OpsWorks to manage patches on the on-premises servers and EC2 instances. Use AWS X-Ray to post the patch status to AWS Systems Manager OpsCenter to generate patch compliance reports.
</label>
</div>

<button onclick="showAnswer(this,[0])">Show Answer</button>

<div class="explanation">
<b>Correct Answer: A</b><br><br>

AWS Systems Manager is designed to provide a <b>single unified management plane</b> for both Amazon EC2 instances and on-premises servers.

<br><br>

Systems Manager Patch Manager allows organizations to define patch baselines, apply patches automatically, and track compliance status.

<br><br>

Patch compliance data is centrally collected and can be reported across all managed instances, regardless of location.

<br><br>

This meets the requirement for a <b>single consolidated patch status report</b> with minimal operational complexity.
</div>
</div>


<!-- ================= Q4 ================= -->
<div class="question">
<pre>
14) A company is running an application on several Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer. 
The load on the application varies throughout the day, and EC2 instances are scaled in and out on a regular basis. 
Log files from the EC2 instances are copied to a central Amazon S3 bucket every 15 minutes. The security team discovers that log files are missing from some of the terminated EC2 instances.
Which set of actions will ensure that log files are copied to the central S3 bucket from the terminated EC2 instances?
</pre>

<div class="options">
<label><input type="radio" name="q14">
A. Create a script to copy log files to Amazon S3, and store the script in a file on the EC2 instance. Create an Auto Scaling lifecycle hook and an Amazon EventBridge rule to detect lifecycle events from the Auto Scaling group. Invoke an AWS Lambda function on the autoscaling:EC2_INSTANCE_TERMINATING transition to send ABANDON to the Auto Scaling group to prevent termination, run the script to copy the log files, and terminate the instance using the AWS SDK.
</label>

<label><input type="radio" name="q14">
B. Create an AWS Systems Manager document with a script to copy log files to Amazon S3. Create an Auto Scaling lifecycle hook and an Amazon EventBridge rule to detect lifecycle events from the Auto Scaling group. Invoke an AWS Lambda function on the autoscaling:EC2_INSTANCE_TERMINATING transition to call the AWS Systems Manager API SendCommand operation to run the document to copy the log files and send CONTINUE to the Auto Scaling group to terminate the instance. Most Voted
</label>

<label><input type="radio" name="q14">
C. Change the log delivery rate to every 5 minutes. Create a script to copy log files to Amazon S3, and add the script to EC2 instance user data. Create an Amazon EventBridge rule to detect EC2 instance termination. Invoke an AWS Lambda function from the EventBridge rule that uses the AWS CLI to run the user-data script to copy the log files and terminate the instance.
</label>

<label><input type="radio" name="q14">
D. Create an AWS Systems Manager document with a script to copy log files to Amazon S3. Create an Auto Scaling lifecycle hook that publishes a message to an Amazon Simple Notification Service (Amazon SNS) topic. From the SNS notification, call the AWS Systems Manager API SendCommand operation to run the document to copy the log files and send ABANDON to the Auto Scaling group to terminate the instance.
</label>
</div>

<button onclick="showAnswer(this,[1])">Show Answer</button>

<div class="explanation">
<b>Correct Answer: B</b><br><br>

Auto Scaling lifecycle hooks allow an EC2 instance to pause during termination.

<br><br>

Using AWS Systems Manager ensures the log-copy operation runs reliably on the instance itself.

<br><br>

The lifecycle hook prevents termination until the log copy completes, after which the CONTINUE signal allows termination.

<br><br>

This guarantees that no logs are lost when instances scale in.
</div>
</div>


<!-- ================= Q5 ================= -->
<div class="question">
<pre>
15) A company is using multiple AWS accounts. The DNS records are stored in a private hosted zone for Amazon Route 53 in Account A. The company’s applications and databases are running in Account B.
A solutions architect will deploy a two-tier application in a new VPC. To simplify the configuration, the db.example.com CNAME record set for the Amazon RDS endpoint was created in a private hosted zone for Amazon Route 53.
During deployment, the application failed to start. Troubleshooting revealed that db.example.com is not resolvable on the Amazon EC2 instance. The solutions architect confirmed that the record set was created correctly in Route 53.
Which combination of steps should the solutions architect take to resolve this issue? (Choose two.)
</pre>

<div class="options">
<label><input type="checkbox">
A. Deploy the database on a separate EC2 instance in the new VPC. Create a record set for the instance’s private IP in the private hosted zone.
</label>

<label><input type="checkbox">
B. Use SSH to connect to the application tier EC2 instance. Add an RDS endpoint IP address to the /etc/resolv.conf file.
</label>

<label><input type="checkbox">
C. Create an authorization to associate the private hosted zone in Account A with the new VPC in Account B.
</label>

<label><input type="checkbox">
D. Create a private hosted zone for the example com domain in Account B. Configure Route 53 replication between AWS accounts.
</label>

<label><input type="checkbox">
E. Associate a new VPC in Account B with a hosted zone in Account A. Delete the association authorization in Account A.
</label>
</div>

<button onclick="showAnswer(this,[2,4])">Show Answer</button>

<div class="explanation">
<b>Correct Answers: C, E</b><br><br>

Private hosted zones only resolve DNS names for VPCs that are explicitly associated.

<br><br>

When the hosted zone and VPC are in different accounts, an association authorization must be created first.

<br><br>

After authorization, the VPC can be associated with the hosted zone, enabling DNS resolution.

<br><br>

This is the correct cross-account Route 53 configuration.
</div>
</div>


<!-- ================= Q6 ================= -->
<div class="question">
<pre>
16) A company used Amazon EC2 instances to deploy a web fleet to host a blog site. The EC2 instances are behind an Application Load Balancer (ALB) and are configured in an Auto Scaling group. The web application stores all blog content on an Amazon EFS volume.
The company recently added a feature for bloggers to add video to their posts, attracting 10 times the previous user traffic. 
At peak times of day, users report buffering and timeout issues while attempting to reach the site or watch videos.
Which is the MOST cost-efficient and scalable deployment that will resolve the issues for users?
</pre>

<div class="options">
<label><input type="radio" name="q16">
A. Reconfigure Amazon EFS to enable maximum I/O.
</label>

<label><input type="radio" name="q16">
B. Update the blog site to use instance store volumes for storage. Copy the site contents to the volumes at launch and to Amazon S3 at shutdown.
</label>

<label><input type="radio" name="q16">
C. Configure an Amazon CloudFront distribution. Point the distribution to an S3 bucket, and migrate the videos from EFS to Amazon S3.
</label>

<label><input type="radio" name="q16">
D. Set up an Amazon CloudFront distribution for all site contents, and point the distribution at the ALB.
</label>
</div>

<button onclick="showAnswer(this,[2])">Show Answer</button>

<div class="explanation">
<b>Correct Answer: C</b><br><br>

The performance issue is caused by a sudden increase in demand for large video files.

<br><br>

Amazon EFS is designed for shared file systems, not for large-scale public content delivery. Increasing EFS performance would significantly increase cost and still would not scale globally.

<br><br>

Amazon S3 is highly durable and massively scalable object storage, making it ideal for storing video files.

<br><br>

Amazon CloudFront caches video content at edge locations close to users, reducing latency, eliminating buffering, and removing load from the backend infrastructure.

<br><br>

Migrating videos to S3 and delivering them through CloudFront provides the most cost-efficient and scalable solution.
</div>
</div>




<!-- ================= Q7 ================= -->
<div class="question">
<pre>
17) A company with global offices has a single 1 Gbps AWS Direct Connect connection to a single AWS Region. 
The company’s on-premises network uses the connection to communicate with the company’s resources in the AWS Cloud. 
The connection has a single private virtual interface that connects to a single VPC.
A solutions architect must implement a solution that adds a redundant Direct Connect connection in the same Region. 
The solution also must provide connectivity to other Regions through the same pair of Direct Connect connections as the company expands into other Regions.
Which solution meets these requirements?
</pre>

<div class="options">
<label><input type="radio" name="q17">
A. Provision a Direct Connect gateway. Delete the existing private virtual interface from the existing connection. Create the second Direct Connect connection. Create a new private virtual interface on each connection, and connect both private virtual interfaces to the Direct Connect gateway. Connect the Direct Connect gateway to the single VPC.
</label>

<label><input type="radio" name="q17">
B. Keep the existing private virtual interface. Create the second Direct Connect connection. Create a new private virtual interface on the new connection, and connect the new private virtual interface to the single VPC.
</label>

<label><input type="radio" name="q17">
C. Keep the existing private virtual interface. Create the second Direct Connect connection. Create a new public virtual interface on the new connection, and connect the new public virtual interface to the single VPC.
</label>

<label><input type="radio" name="q17">
D. Provision a transit gateway. Delete the existing private virtual interface from the existing connection. Create the second Direct Connect connection. Create a new private virtual interface on each connection, and connect both private virtual interfaces to the transit gateway. Associate the transit gateway with the single VPC.
</label>
</div>

<button onclick="showAnswer(this,[0])">Show Answer</button>

<div class="explanation">
<b>Correct Answer: A</b><br><br>

This question tests knowledge of Direct Connect scalability and redundancy.

<br><br>

A Direct Connect gateway allows multiple Direct Connect connections to provide connectivity to multiple VPCs across multiple Regions.

<br><br>

By attaching private virtual interfaces from two separate Direct Connect connections to the Direct Connect gateway, the company achieves redundancy in the same Region.

<br><br>

As the company expands to additional Regions, the same Direct Connect gateway can be associated with new VPCs without creating additional Direct Connect connections.

<br><br>

This design provides redundancy, regional scalability, and long-term flexibility.
</div>
</div>




<!-- ================= Q8 ================= -->
<div class="question">
<pre>
18) A company has a web application that allows users to upload short videos. The videos are stored on Amazon EBS volumes and analyzed by custom recognition software for categorization.
The website contains static content that has variable traffic with peaks in certain months. 
The architecture consists of Amazon EC2 instances running in an Auto Scaling group for the web application and EC2 instances running in an Auto Scaling group to process an Amazon SQS queue. 
The company wants to re-architect the application to reduce operational overhead using AWS managed services where possible and remove dependencies on third-party software.
Which solution meets these requirements?
</pre>

<div class="options">
<label><input type="radio" name="q18">
A. Use Amazon ECS containers for the web application and Spot instances for the Auto Scaling group that processes the SQS queue. Replace the custom software with Amazon Rekognition to categorize the videos.
</label>

<label><input type="radio" name="q18">
B. Store the uploaded videos in Amazon EFS and mount the file system to the EC2 instances for the web application. Process the SQS queue with an AWS Lambda function that calls the Amazon Rekognition API to categorize the videos.
</label>

<label><input type="radio" name="q18">
C. Host the web application in Amazon S3. Store the uploaded videos in Amazon S3. Use S3 event notification to publish events to the SQS queue. Process the SQS queue with an AWS Lambda function that calls the Amazon Rekognition API to categorize the videos.
</label>

<label><input type="radio" name="q18">
D. Use AWS Elastic Beanstalk to launch EC2 instances in an Auto Scaling group for the web application and launch a worker environment to process the SQS queue. Replace the custom software with Amazon Rekognition to categorize the videos.
</label>
</div>

<button onclick="showAnswer(this,[2])">Show Answer</button>

<div class="explanation">
<b>Correct Answer: C</b><br><br>

The goal is to remove operational overhead and eliminate EC2-based processing.

<br><br>

Amazon S3 is ideal for hosting static web content and storing uploaded videos because it scales automatically and requires no server management.

<br><br>

S3 event notifications can publish messages to Amazon SQS whenever new videos are uploaded, removing the need for EC2 polling.

<br><br>

AWS Lambda can process SQS messages and invoke Amazon Rekognition, replacing custom recognition software and EC2 workers.

<br><br>

This architecture uses fully managed services end-to-end, achieving the lowest operational overhead.
</div>
</div>


<!-- ================= Q9 ================= -->
<div class="question">
<pre>
19) A company has a serverless application comprised of Amazon CloudFront, Amazon API Gateway, and AWS Lambda functions. 
The current deployment process of the application code is to create a new version number of the Lambda function and run an AWS CLI script to update. 
If the new function version has errors, another CLI script reverts by deploying the previous working version of the function. 
The company would like to decrease the time to deploy new versions of the application logic provided by the Lambda functions, and also reduce the time to detect and revert when errors are identified.
How can this be accomplished?
</pre>

<div class="options">
<label><input type="radio" name="q19">
A. Create and deploy nested AWS CloudFormation stacks with the parent stack consisting of the AWS CloudFront distribution and API Gateway, and the child stack containing the Lambda function. For changes to Lambda, create an AWS CloudFormation change set and deploy; if errors are triggered, revert the AWS CloudFormation change set to the previous version.
</label>

<label><input type="radio" name="q19">
B. Use AWS SAM and built-in AWS CodeDeploy to deploy the new Lambda version, gradually shift traffic to the new version, and use pre-traffic and post-traffic test functions to verify code. Rollback if Amazon CloudWatch alarms are triggered.
</label>

<label><input type="radio" name="q19">
C. Refactor the AWS CLI scripts into a single script that deploys the new Lambda version. When deployment is completed, the script tests execute. If errors are detected, revert to the previous Lambda version.
</label>

<label><input type="radio" name="q19">
D. Create and deploy an AWS CloudFormation stack that consists of a new API Gateway endpoint that references the new Lambda version. Change the CloudFront origin to the new API Gateway endpoint, monitor errors and if detected, change the AWS CloudFront origin to the previous API Gateway endpoint.
</label>
</div>

<button onclick="showAnswer(this,[1])">Show Answer</button>

<div class="explanation">
<b>Correct Answer: B</b><br><br>

AWS SAM integrates directly with AWS CodeDeploy to support safe and automated Lambda deployments.

<br><br>

Traffic can be shifted gradually using canary or linear deployment strategies.

<br><br>

Pre-traffic and post-traffic hooks allow automated validation, while CloudWatch alarms can trigger automatic rollback if errors are detected.

<br><br>

This approach significantly reduces deployment time and minimizes customer impact.
</div>
</div>


<!-- ================= Q10 ================= -->
<div class="question">
<pre>
20) A company is planning to store a large number of archived documents and make the documents available to employees through the corporate intranet. 
Employees will access the system by connecting through a client VPN service that is attached to a VPC. 
The data must not be accessible to the public.
The documents that the company is storing are copies of data that is held on physical media elsewhere. 
The number of requests will be low. Availability and speed of retrieval are not concerns of the company.
Which solution will meet these requirements at the LOWEST cost?
</pre>

<div class="options">
<label><input type="radio" name="q20">
A. Create an Amazon S3 bucket. Configure the S3 bucket to use the S3 One Zone-Infrequent Access (S3 One Zone-IA) storage class as default. Configure the S3 bucket for website hosting. Create an S3 interface endpoint. Configure the S3 bucket to allow access only through that endpoint.
</label>

<label><input type="radio" name="q20">
B. Launch an Amazon EC2 instance that runs a web server. Attach an Amazon Elastic File System (Amazon EFS) file system to store the archived data in the EFS One Zone-Infrequent Access (EFS One Zone-IA) storage class Configure the instance security groups to allow access only from private networks.
</label>

<label><input type="radio" name="q20">
C. Launch an Amazon EC2 instance that runs a web server Attach an Amazon Elastic Block Store (Amazon EBS) volume to store the archived data. Use the Cold HDD (sc1) volume type. Configure the instance security groups to allow access only from private networks.
</label>

<label><input type="radio" name="q20">
D. Create an Amazon S3 bucket. Configure the S3 bucket to use the S3 Glacier Deep Archive storage class as default. Configure the S3 bucket for website hosting. Create an S3 interface endpoint. Configure the S3 bucket to allow access only through that endpoint.
</label>
</div>

<button onclick="showAnswer(this,[0])">Show Answer</button>

<div class="explanation">
<b>Correct Answer: A</b><br><br>

The primary requirement is lowest cost with private access.

<br><br>

S3 One Zone-IA provides significantly lower storage cost compared to standard storage and is acceptable because durability already exists elsewhere.

<br><br>

Using an S3 interface endpoint ensures access remains private and does not traverse the public internet.

<br><br>

This solution meets security requirements while minimizing cost.
</div>
</div>


</div>

<script>
function showAnswer(btn, correct) {
  const q = btn.parentElement;
  const labels = q.querySelectorAll("label");
  correct.forEach(i => labels[i].classList.add("correct"));
  q.querySelector(".explanation").style.display = "block";
}
</script>

</body>
</html>
